{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ot6-5dG7X745"
      },
      "source": [
        "# Lenet-5\n",
        "\n",
        "LeNet은 CNN을 처음으로 개발한 얀 르쿤(Yann Lecun) 연구팀이 1998년에 개발한 CNN 알고리즘의 이름이다. original 논문 제목은 \"Gradient-based learning applied to document recognition\"이다.\n",
        "\n",
        "참고 : https://bskyvision.com/418"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fF_lGp2BX_-M"
      },
      "source": [
        "\n",
        "네트워크를 보면,\n",
        "\n",
        "<img src = 'https://t1.daumcdn.net/cfile/tistory/99170D4C5C7E21250E'>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wP1m13DGYHaa"
      },
      "source": [
        "그림1에서 볼 수 있듯이 LeNet-5는 인풋, 3개의 컨볼루션 레이어(C1, C3, C5), 2개의 서브샘플링 레이어(S2, S4), 1층의 full-connected 레이어(F6), 아웃풋 레이어로 구성되어 있다. 참고로 C1부터 F6까지 활성화 함수로 tanh을 사용한다.   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-TDfwdqYJtq"
      },
      "source": [
        "1)  C1 레이어: 입력 영상(여기서는 32 x 32 사이즈의 이미지)을 6개의 5 x 5 필터와 컨볼루션 연산을 해준다. 그 결과 6장의 28 x 28 특성 맵을 얻게 된다. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1sOxb8OYOjr"
      },
      "source": [
        "2) S2 레이어: 6장의 28 x 28 특성 맵에 대해 서브샘플링을 진행한다. 결과적으로 28 x 28 사이즈의 특성 맵이 14 x 14 사이즈의 특성맵으로 축소된다. 2 x 2 필터를 stride 2로 설정해서 서브샘플링해주기 때문이다. 사용하는 서브샘플링 방법은 평균 풀링(average pooling)이다. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cAFEmkuYTQ6"
      },
      "source": [
        "평균풀링인데 왜 훈련해야할 파라미터가 필요한지 의아할 수 있는데, original 논문에 의하면 평균을 낸 후에 한 개의 훈련가능한 가중치(trainable weight)를 곱해주고 또 한 개의 훈련가능한 바이어스(trainable bias)를 더해준다고 한다. 그 값이 시그모이드 함수를 통해 활성화된다. 참고로 그 가중치와 바이어스는 시그모이드의 비활성도를 조절해준다고 한다. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Deep learning for Vision System Book 참고)"
      ],
      "metadata": {
        "id": "fXdgS2WZJ9I6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Load MNIST database"
      ],
      "metadata": {
        "id": "scr3tNeEKJzz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist\n",
        "\n",
        "# use Keras to import pre-shuffled MNIST database\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "print(\"The MNIST database has a training set of %d examples.\" % len(X_train))\n",
        "print(\"The MNIST database has a test set of %d examples.\" % len(X_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AizkbPrKANw",
        "outputId": "af9c64e9-55f3-4c9a-a82e-0ca53a3bb306"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The MNIST database has a training set of 60000 examples.\n",
            "The MNIST database has a test set of 10000 examples.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Visualize the First six Training images"
      ],
      "metadata": {
        "id": "Qgvt_UoEKNeU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import matplotlib.cm as cm\n",
        "import numpy as np\n",
        "\n",
        "# plot first six training images\n",
        "fig = plt.figure(figsize=(20,20))\n",
        "for i in range(6):\n",
        "    ax = fig.add_subplot(1, 6, i+1, xticks=[], yticks=[])\n",
        "    ax.imshow(X_train[i], cmap='gray')\n",
        "    ax.set_title(str(y_train[i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "fIbO57xZKQon",
        "outputId": "e436cff4-59d1-4cc9-92f0-c4283bfcdcdd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1440x1440 with 6 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. View an Image in More Detail"
      ],
      "metadata": {
        "id": "5QqDWWUrKRmE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_input(img, ax):\n",
        "    ax.imshow(img, cmap='gray')\n",
        "    width, height = img.shape\n",
        "    thresh = img.max()/2.5\n",
        "    for x in range(width):\n",
        "        for y in range(height):\n",
        "            ax.annotate(str(round(img[x][y],2)), xy=(y,x),\n",
        "                        horizontalalignment='center',\n",
        "                        verticalalignment='center',\n",
        "                        color='white' if img[x][y]<thresh else 'black')\n",
        "\n",
        "fig = plt.figure(figsize = (12,12)) \n",
        "ax = fig.add_subplot(111)\n",
        "visualize_input(X_train[0], ax)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        },
        "id": "l-ek9PbCKXbG",
        "outputId": "4fe0fa5e-8c7d-43a1-dc68-3bcb62aef99e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x864 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Preprocess input images : Rescale the Images by Dividing Every Pixel in Every Image by 255"
      ],
      "metadata": {
        "id": "JEpc6lamKYaf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# normalize the data to accelerate learning\n",
        "mean = np.mean(X_train)\n",
        "std = np.std(X_train)\n",
        "X_train = (X_train-mean)/(std+1e-7)\n",
        "X_test = (X_test-mean)/(std+1e-7)\n",
        "\n",
        "print('X_train shape:', X_train.shape)\n",
        "print(X_train.shape[0], 'train samples')\n",
        "print(X_test.shape[0], 'test samples')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SccvlujhKhFa",
        "outputId": "79153911-6e4d-4eb7-c3a1-61d0bbac65ce"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (60000, 28, 28)\n",
            "60000 train samples\n",
            "10000 test samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Preprocess the lables : Encode Categorical Integer Labels Using a One-Hot Scheme"
      ],
      "metadata": {
        "id": "h-FDzrtLKh2s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import np_utils\n",
        "\n",
        "num_classes = 10 \n",
        "# print first ten (integer-valued) training labels\n",
        "print('Integer-valued labels:')\n",
        "print(y_train[:10])\n",
        "\n",
        "# one-hot encode the labels\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = np_utils.to_categorical(y_train, num_classes)\n",
        "y_test = np_utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "# print first ten (one-hot) training labels\n",
        "print('One-hot labels:')\n",
        "print(y_train[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfeIXiU7Koru",
        "outputId": "37a4bdea-740b-4486-cc52-03ffff1480af"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Integer-valued labels:\n",
            "[5 0 4 1 9 2 1 3 1 4]\n",
            "One-hot labels:\n",
            "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Reshape data to fit our CNN (and input_shape)"
      ],
      "metadata": {
        "id": "W9O5H_LPKq3b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# input image dimensions 28x28 pixel images. \n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
        "input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "print('image input shape: ', input_shape)\n",
        "print('x_train shape:', X_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0l0TaS4KscL",
        "outputId": "23bb8849-c38e-4e73-ab7a-a6d521213030"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "image input shape:  (28, 28, 1)\n",
            "x_train shape: (60000, 28, 28, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Define the Model Architecture\n",
        "\n",
        "<img src = 'https://raw.githubusercontent.com/moelgendy/deep_learning_for_vision_systems/2c9d077b43003657cd8f6d5ddfb6f83ee8bae1f3/chapter_05/images/lenet_architecture.png'>"
      ],
      "metadata": {
        "id": "653eL4jaKr6c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To implement LeNet-5 in Keras, read the original paper and extract the architecture information from pages 6, 7 and 8. Here are the main takeaways to build the LeNet-5 network:\n",
        "\n",
        "- Number of filters in each CONV layer: you can see from the diagram (and defined in the paper) that the depth (number of filters) of each convolutional layer is as follows: C1 = 6, C3 = 16, C5 = 120 layers.\n",
        "- Kernel size of each CONV layer: from the paper, the kernel_size is = 5 x 5\n",
        "\n",
        "- A subsampling layer (POOL) is added after each convolutional layer. The receptive field of each unit is a 2 x 2 area (i.e. pool_size = 2). Note that the LeNet-5 creators used average pooling which computes the average value of its inputs instead of the max pooling layer that we used in our earlier projects which passes the maximum value of its inputs. You can try both if you are interested to see the difference. For this experiment, we are going to follow the paper architecture.\n",
        "\n",
        "- Activation function: the creators of LeNet-5 used tanh activation function for the hidden layers because symmetric functions are believed to yield faster convergence compared to sigmoid functions. In general, you are strongly encouraged to add a ReLU activation function to every convolutional layer in your networks.\n",
        "\n",
        "- Things to remember\n",
        "\n",
        "- Always add a ReLU activation function to the Conv2D layers in your CNN. With the exception of the final layer in the network, Dense layers should also have a ReLU activation function.\n",
        "\n",
        "- When constructing a network for classification, the final layer in the network should be a Dense (FC) layer with a softmax activation function. The number of nodes in the final layer should equal the total number of classes in the dataset."
      ],
      "metadata": {
        "id": "tOKIHW-jK1sg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, AveragePooling2D, Flatten, Dense"
      ],
      "metadata": {
        "id": "zkGjWwjsK9gw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Instantiate an empty model\n",
        "model = Sequential()\n",
        "\n",
        "# C1 Convolutional Layer\n",
        "model.add(Conv2D(6, kernel_size=(5, 5), strides=(1, 1), activation='tanh', input_shape=input_shape, padding='same'))\n",
        "\n",
        "# S2 Pooling Layer\n",
        "model.add(AveragePooling2D(pool_size=(2, 2), strides=2, padding='valid'))\n",
        "\n",
        "# C3 Convolutional Layer\n",
        "model.add(Conv2D(16, kernel_size=(5, 5), strides=(1, 1), activation='tanh', padding='valid'))\n",
        "\n",
        "# S4 Pooling Layer\n",
        "model.add(AveragePooling2D(pool_size=(2, 2), strides=2, padding='valid'))\n",
        "\n",
        "# C5 Fully Connected Convolutional Layer\n",
        "model.add(Conv2D(120, kernel_size=(5, 5), strides=(1, 1), activation='tanh', padding='valid'))\n",
        "\n",
        "#Flatten the CNN output so that we can connect it with fully connected layers\n",
        "model.add(Flatten())\n",
        "\n",
        "# FC6 Fully Connected Layer\n",
        "model.add(Dense(84, activation='tanh'))\n",
        "\n",
        "# Output Layer with softmax activation\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# print the model summary\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9hS0qDyK-tY",
        "outputId": "89321e71-d5d1-4e3c-aa50-a13a60b35508"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 28, 28, 6)         156       \n",
            "                                                                 \n",
            " average_pooling2d (AverageP  (None, 14, 14, 6)        0         \n",
            " ooling2D)                                                       \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 10, 10, 16)        2416      \n",
            "                                                                 \n",
            " average_pooling2d_1 (Averag  (None, 5, 5, 16)         0         \n",
            " ePooling2D)                                                     \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 1, 1, 120)         48120     \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 120)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 84)                10164     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                850       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 61,706\n",
            "Trainable params: 61,706\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Compile the Model\n",
        "\n",
        "In this exercise, we will use Adam optimizer"
      ],
      "metadata": {
        "id": "Mw6TAHrPLAPS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the loss function is categorical cross entropy since we have multiple classes (10) \n",
        "\n",
        "\n",
        "# compile the model by defining the loss function, optimizer, and performance metric\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "uK7M8vm4LDYV"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Train the Model\n",
        "\n",
        "LeCun and his team used scheduled decay learning where the value of the learning rate was decreased using the following schedule: 0.0005 for the first two epochs, 0.0002 for the next three epochs, 0.00005 for the next four, and then 0.00001 thereafter. In the paper, the authors trained their network for 20 epochs."
      ],
      "metadata": {
        "id": "czvVFaJYLEvY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "\n",
        "# set the learning rate schedule as created in the original paper\n",
        "def lr_schedule(epoch):\n",
        "    if epoch <= 2:     \n",
        "        lr = 5e-4\n",
        "    elif epoch > 2 and epoch <= 5:\n",
        "        lr = 2e-4\n",
        "    elif epoch > 5 and epoch <= 9:\n",
        "        lr = 5e-5\n",
        "    else: \n",
        "        lr = 1e-5\n",
        "    return lr\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "# set the checkpointer\n",
        "checkpointer = ModelCheckpoint(filepath='model.weights.best.hdf5', verbose=1, \n",
        "                               save_best_only=True)\n",
        "\n",
        "# train the model\n",
        "hist = model.fit(X_train, y_train, batch_size=32, epochs=20,\n",
        "          validation_data=(X_test, y_test), callbacks=[checkpointer, lr_scheduler], \n",
        "          verbose=2, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vk_FwG8ZLGsE",
        "outputId": "250a81ce-853d-4fa6-ec33-5e1c5e83ca31"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\n",
            "Epoch 1: val_loss improved from inf to 0.08104, saving model to model.weights.best.hdf5\n",
            "1875/1875 - 15s - loss: 0.2260 - accuracy: 0.9337 - val_loss: 0.0810 - val_accuracy: 0.9752 - lr: 5.0000e-04 - 15s/epoch - 8ms/step\n",
            "Epoch 2/20\n",
            "\n",
            "Epoch 2: val_loss improved from 0.08104 to 0.05516, saving model to model.weights.best.hdf5\n",
            "1875/1875 - 5s - loss: 0.0767 - accuracy: 0.9769 - val_loss: 0.0552 - val_accuracy: 0.9824 - lr: 5.0000e-04 - 5s/epoch - 3ms/step\n",
            "Epoch 3/20\n",
            "\n",
            "Epoch 3: val_loss improved from 0.05516 to 0.04909, saving model to model.weights.best.hdf5\n",
            "1875/1875 - 5s - loss: 0.0560 - accuracy: 0.9828 - val_loss: 0.0491 - val_accuracy: 0.9843 - lr: 5.0000e-04 - 5s/epoch - 3ms/step\n",
            "Epoch 4/20\n",
            "\n",
            "Epoch 4: val_loss improved from 0.04909 to 0.03796, saving model to model.weights.best.hdf5\n",
            "1875/1875 - 5s - loss: 0.0345 - accuracy: 0.9897 - val_loss: 0.0380 - val_accuracy: 0.9870 - lr: 2.0000e-04 - 5s/epoch - 3ms/step\n",
            "Epoch 5/20\n",
            "\n",
            "Epoch 5: val_loss improved from 0.03796 to 0.03548, saving model to model.weights.best.hdf5\n",
            "1875/1875 - 5s - loss: 0.0296 - accuracy: 0.9916 - val_loss: 0.0355 - val_accuracy: 0.9881 - lr: 2.0000e-04 - 5s/epoch - 3ms/step\n",
            "Epoch 6/20\n",
            "\n",
            "Epoch 6: val_loss improved from 0.03548 to 0.03488, saving model to model.weights.best.hdf5\n",
            "1875/1875 - 5s - loss: 0.0262 - accuracy: 0.9921 - val_loss: 0.0349 - val_accuracy: 0.9893 - lr: 2.0000e-04 - 5s/epoch - 3ms/step\n",
            "Epoch 7/20\n",
            "\n",
            "Epoch 7: val_loss improved from 0.03488 to 0.03099, saving model to model.weights.best.hdf5\n",
            "1875/1875 - 5s - loss: 0.0188 - accuracy: 0.9951 - val_loss: 0.0310 - val_accuracy: 0.9898 - lr: 5.0000e-05 - 5s/epoch - 3ms/step\n",
            "Epoch 8/20\n",
            "\n",
            "Epoch 8: val_loss improved from 0.03099 to 0.02959, saving model to model.weights.best.hdf5\n",
            "1875/1875 - 5s - loss: 0.0175 - accuracy: 0.9956 - val_loss: 0.0296 - val_accuracy: 0.9902 - lr: 5.0000e-05 - 5s/epoch - 3ms/step\n",
            "Epoch 9/20\n",
            "\n",
            "Epoch 9: val_loss did not improve from 0.02959\n",
            "1875/1875 - 5s - loss: 0.0168 - accuracy: 0.9959 - val_loss: 0.0297 - val_accuracy: 0.9897 - lr: 5.0000e-05 - 5s/epoch - 3ms/step\n",
            "Epoch 10/20\n",
            "\n",
            "Epoch 10: val_loss did not improve from 0.02959\n",
            "1875/1875 - 5s - loss: 0.0159 - accuracy: 0.9960 - val_loss: 0.0300 - val_accuracy: 0.9893 - lr: 5.0000e-05 - 5s/epoch - 3ms/step\n",
            "Epoch 11/20\n",
            "\n",
            "Epoch 11: val_loss improved from 0.02959 to 0.02917, saving model to model.weights.best.hdf5\n",
            "1875/1875 - 5s - loss: 0.0140 - accuracy: 0.9968 - val_loss: 0.0292 - val_accuracy: 0.9897 - lr: 1.0000e-05 - 5s/epoch - 3ms/step\n",
            "Epoch 12/20\n",
            "\n",
            "Epoch 12: val_loss improved from 0.02917 to 0.02897, saving model to model.weights.best.hdf5\n",
            "1875/1875 - 5s - loss: 0.0138 - accuracy: 0.9970 - val_loss: 0.0290 - val_accuracy: 0.9897 - lr: 1.0000e-05 - 5s/epoch - 3ms/step\n",
            "Epoch 13/20\n",
            "\n",
            "Epoch 13: val_loss improved from 0.02897 to 0.02869, saving model to model.weights.best.hdf5\n",
            "1875/1875 - 5s - loss: 0.0136 - accuracy: 0.9970 - val_loss: 0.0287 - val_accuracy: 0.9899 - lr: 1.0000e-05 - 5s/epoch - 3ms/step\n",
            "Epoch 14/20\n",
            "\n",
            "Epoch 14: val_loss did not improve from 0.02869\n",
            "1875/1875 - 5s - loss: 0.0135 - accuracy: 0.9971 - val_loss: 0.0291 - val_accuracy: 0.9898 - lr: 1.0000e-05 - 5s/epoch - 3ms/step\n",
            "Epoch 15/20\n",
            "\n",
            "Epoch 15: val_loss did not improve from 0.02869\n",
            "1875/1875 - 6s - loss: 0.0133 - accuracy: 0.9972 - val_loss: 0.0290 - val_accuracy: 0.9901 - lr: 1.0000e-05 - 6s/epoch - 3ms/step\n",
            "Epoch 16/20\n",
            "\n",
            "Epoch 16: val_loss did not improve from 0.02869\n",
            "1875/1875 - 5s - loss: 0.0131 - accuracy: 0.9972 - val_loss: 0.0289 - val_accuracy: 0.9900 - lr: 1.0000e-05 - 5s/epoch - 3ms/step\n",
            "Epoch 17/20\n",
            "\n",
            "Epoch 17: val_loss did not improve from 0.02869\n",
            "1875/1875 - 5s - loss: 0.0130 - accuracy: 0.9971 - val_loss: 0.0288 - val_accuracy: 0.9898 - lr: 1.0000e-05 - 5s/epoch - 3ms/step\n",
            "Epoch 18/20\n",
            "\n",
            "Epoch 18: val_loss did not improve from 0.02869\n",
            "1875/1875 - 6s - loss: 0.0128 - accuracy: 0.9973 - val_loss: 0.0287 - val_accuracy: 0.9900 - lr: 1.0000e-05 - 6s/epoch - 3ms/step\n",
            "Epoch 19/20\n",
            "\n",
            "Epoch 19: val_loss did not improve from 0.02869\n",
            "1875/1875 - 5s - loss: 0.0127 - accuracy: 0.9973 - val_loss: 0.0288 - val_accuracy: 0.9899 - lr: 1.0000e-05 - 5s/epoch - 3ms/step\n",
            "Epoch 20/20\n",
            "\n",
            "Epoch 20: val_loss did not improve from 0.02869\n",
            "1875/1875 - 6s - loss: 0.0125 - accuracy: 0.9974 - val_loss: 0.0288 - val_accuracy: 0.9901 - lr: 1.0000e-05 - 6s/epoch - 3ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Load the Model with the Best Classification Accuracy on the Validation Set"
      ],
      "metadata": {
        "id": "FKUD3lHWLJhY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the weights that yielded the best validation accuracy\n",
        "model.load_weights('model.weights.best.hdf5')"
      ],
      "metadata": {
        "id": "pZigfBAWLK4-"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Calculate the Classification Accuracy on the Test Set"
      ],
      "metadata": {
        "id": "fg5X3qVSLOHb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate test accuracy\n",
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "accuracy = 100*score[1]\n",
        "\n",
        "# print test accuracy\n",
        "print('Test accuracy: %.4f%%' % accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kJiMlHJLK69",
        "outputId": "314e1731-8ab3-4c77-8b67-4b5ae23877e7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 98.9900%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. evaluate the model"
      ],
      "metadata": {
        "id": "pV_uQvv_LRDU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "f, ax = plt.subplots()\n",
        "ax.plot([None] + hist.history['accuracy'], 'o-')\n",
        "ax.plot([None] + hist.history['val_accuracy'], 'x-')\n",
        "# Plot legend and use the best location automatically: loc = 0.\n",
        "ax.legend(['Train acc', 'Validation acc'], loc = 0)\n",
        "ax.set_title('Training/Validation acc per Epoch')\n",
        "ax.set_xlabel('Epoch')\n",
        "ax.set_ylabel('acc')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "tCmO97vjLK89",
        "outputId": "fa84a41d-e8ab-4fa0-8d52-86d3db130db9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "f, ax = plt.subplots()\n",
        "ax.plot([None] + hist.history['loss'], 'o-')\n",
        "ax.plot([None] + hist.history['val_loss'], 'x-')\n",
        "\n",
        "# Plot legend and use the best location automatically: loc = 0.\n",
        "ax.legend(['Train loss', \"Val loss\"], loc = 0)\n",
        "ax.set_title('Training/Validation Loss per Epoch')\n",
        "ax.set_xlabel('Epoch')\n",
        "ax.set_ylabel('Loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "VbBXfXvxLK_L",
        "outputId": "d199127b-4c39-433e-8d24-b6622756ef32"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ck3mFUdQYcer"
      },
      "source": [
        "# AlexNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBPXEqbOYeuS"
      },
      "source": [
        "AlexNet의 기본구조는 LeNet-5와 크게 다르지 않다. 2개의 GPU로 병렬연산을 수행하기 위해서 병렬적인 구조로 설계되었다는 점이 가장 큰 변화이다. AlexNet의 구조도를 살펴보자"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66WzgAMyYhsq"
      },
      "source": [
        "<img src = 'https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Ft1.daumcdn.net%2Fcfile%2Ftistory%2F99FEB93C5C80B5192E'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y77HTG1KYk-S"
      },
      "source": [
        "AlexNet은 8개의 레이어로 구성되어 있다. 5개의 컨볼루션 레이어와 3개의 full-connected 레이어로 구성되어 있다. 두번째, 네번째, 다섯번째 컨볼루션 레이어들은 전 단계의 같은 채널의 특성맵들과만 연결되어 있는 반면, 세번째 컨볼루션 레이어는 전 단계의 두 채널의 특성맵들과 모두 연결되어 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mK6N1c7yYp5y"
      },
      "source": [
        "이제 각 레이어마다 어떤 작업이 수행되는지 살펴보자. 우선 AlexNet에 입력 되는 것은 227 x 227 x 3 이미지다. (227 x 227 사이즈의 RGB 컬러 이미지를 뜻한다.) 그림에는 224로 되어 있는데 잘못된 겁니다. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src = 'https://raw.githubusercontent.com/moelgendy/deep_learning_for_vision_systems/2c9d077b43003657cd8f6d5ddfb6f83ee8bae1f3/chapter_05/images/alexnet_architecture.png'>"
      ],
      "metadata": {
        "id": "-84Q4SF6LYGW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, AveragePooling2D, Flatten, Dense,Activation,MaxPool2D, BatchNormalization, Dropout\n",
        "from keras.regularizers import l2"
      ],
      "metadata": {
        "id": "a1EpDyGRLbHv"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate an empty sequential model\n",
        "model = Sequential(name=\"Alexnet\")\n",
        "# 1st layer (conv + pool + batchnorm)\n",
        "model.add(Conv2D(filters= 96, kernel_size= (11,11), strides=(4,4), padding='valid', kernel_regularizer=l2(0.0005),\n",
        "input_shape = (227,227,3)))\n",
        "model.add(Activation('relu'))  #<---- activation function can be added on its own layer or within the Conv2D function\n",
        "model.add(MaxPool2D(pool_size=(3,3), strides= (2,2), padding='valid'))\n",
        "model.add(BatchNormalization())\n",
        "    \n",
        "# 2nd layer (conv + pool + batchnorm)\n",
        "model.add(Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), padding='same', kernel_regularizer=l2(0.0005)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPool2D(pool_size=(3,3), strides=(2,2), padding='valid'))\n",
        "model.add(BatchNormalization())\n",
        "            \n",
        "# layer 3 (conv + batchnorm)      <--- note that the authors did not add a POOL layer here\n",
        "model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='same', kernel_regularizer=l2(0.0005)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "        \n",
        "# layer 4 (conv + batchnorm)      <--- similar to layer 3\n",
        "model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='same', kernel_regularizer=l2(0.0005)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "            \n",
        "# layer 5 (conv + batchnorm)  \n",
        "model.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='same', kernel_regularizer=l2(0.0005)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPool2D(pool_size=(3,3), strides=(2,2), padding='valid'))\n",
        "\n",
        "# Flatten the CNN output to feed it with fully connected layers\n",
        "model.add(Flatten())\n",
        "\n",
        "# layer 6 (Dense layer + dropout)  \n",
        "model.add(Dense(units = 4096, activation = 'relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# layer 7 (Dense layers) \n",
        "model.add(Dense(units = 4096, activation = 'relu'))\n",
        "model.add(Dropout(0.5))\n",
        "                           \n",
        "# layer 8 (softmax output layer) \n",
        "model.add(Dense(units = 1000, activation = 'softmax'))\n",
        "\n",
        "# print the model summary\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVOPRarPLbJp",
        "outputId": "fbcd5bff-86a1-4bd0-e761-e1cf6ac00def"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"Alexnet\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_3 (Conv2D)           (None, 55, 55, 96)        34944     \n",
            "                                                                 \n",
            " activation (Activation)     (None, 55, 55, 96)        0         \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 27, 27, 96)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 27, 27, 96)       384       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 27, 27, 256)       614656    \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 27, 27, 256)       0         \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 13, 13, 256)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 13, 13, 256)      1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 13, 13, 384)       885120    \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 13, 13, 384)       0         \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 13, 13, 384)      1536      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv2d_6 (Conv2D)           (None, 13, 13, 384)       1327488   \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 13, 13, 384)       0         \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 13, 13, 384)      1536      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 13, 13, 256)       884992    \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 13, 13, 256)       0         \n",
            "                                                                 \n",
            " batch_normalization_4 (Batc  (None, 13, 13, 256)      1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 6, 6, 256)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 9216)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 4096)              37752832  \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 4096)              16781312  \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 1000)              4097000   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 62,383,848\n",
            "Trainable params: 62,381,096\n",
            "Non-trainable params: 2,752\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3euAK6M-Y-WO"
      },
      "source": [
        "# VGG-16/19"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tq9aPQ7IZB_b"
      },
      "source": [
        "VGGNet은 16개 또는 19개의 층으로 구성된 모델을 의미한다(VGG16, VGG19로 불림). 이전에 포스팅한 VGG-F, VGG-M, VGG-S와는 차이가 있다. 그 모델들은 8개의 층을 가진 AlexNet과 유사한 모델들이다. 하지만 AlexNet와 같이 병렬적 구조로 이뤄지진 않았다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjfMcv1QZFAw"
      },
      "source": [
        " - 역사적으로 봤을 때 VGGNet 모델부터 시작해서 네트워크의 깊이가 확 깊어졌다. 아래 그림을 참고하자. \n",
        "\n",
        "<img src = 'https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FdrYEZ4%2FbtqwpSXYd8U%2FLF8RkIhWpAmsgR67nfnZsk%2Fimg.png'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFXTN1hLY-ae"
      },
      "source": [
        "VGGNet은 사용하기 쉬운 구조와 좋은 성능 덕분에 그 대회에서 우승을 거둔 조금 더 복잡한 형태의 GoogLeNet보다 더 인기를 얻었다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHsEbiTWZNt7"
      },
      "source": [
        "개인적으로 나는 모든 필터 커널의 사이즈를 3 x 3으로 설정했기 때문에 네트워크의 깊이를 깊게 만들 수 있다고 생각한다. 왜냐하면 필터커널의 사이즈가 크면 그만큼 이미지의 사이즈가 금방 축소되기 때문에 네트워크의 깊이를 충분히 깊게 만들기 불가능하기 때문이다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofZWyUnbZSvC"
      },
      "source": [
        "<img src= 'https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fb1Vk5P%2FbtqwqjujKsa%2FTL2HyQ4kj6pNPz4TsirknK%2Fimg.png'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzMb3L9HZhTb"
      },
      "source": [
        "그것은 바로 3 x 3 필터로 두 차례 컨볼루션을 하는 것과 5 x 5 필터로 한 번 컨볼루션을 하는 것이 결과적으로 동일한 사이즈의 특성맵을 산출한다는 것이다(아래 그림 참고). 3 x 3 필터로 세 차례 컨볼루션 하는 것은 7 x 7 필터로 한 번 컨볼루션 하는 것과 대응된다.\n",
        "\n",
        "<img src ='https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FT5kb2%2FbtqwonLQrNm%2Fd56nskDXEYoCWRDpxxkeAK%2Fimg.png'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKx-HcexZn36"
      },
      "source": [
        "그러면 3 x 3 필터로 세 차례 컨볼루션을 하는 것이 7 x 7 필터로 한 번 컨볼루션하는 것보다 나은 점은 무엇일까? 일단 가중치 또는 파라미터의 갯수의 차이다. 3 x 3 필터가 3개면 총 27개의 가중치를 갖는다. 반면 7 x 7 필터는 49개의 가중치를 갖는다. CNN에서 가중치는 모두 훈련이 필요한 것들이므로, 가중치가 적다는 것은 그만큼 훈련시켜야할 것의 갯수가 작아진다. 따라서 학습의 속도가 빨라진다. 동시에 층의 갯수가 늘어나면서 특성에 비선형성을 더 증가시키기 때문에 특성이 점점 더 유용해진다. \n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiT2QB2OZqY7"
      },
      "source": [
        "## VGG 구조\n",
        "<img src = 'https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FK990l%2FbtqwDJ7C54R%2F664Ksm6gyTGBR1wK3YPDFk%2Fimg.png'>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://raw.githubusercontent.com/moelgendy/deep_learning_for_vision_systems/2c9d077b43003657cd8f6d5ddfb6f83ee8bae1f3/chapter_05/images/vggnet_architecture.png'>"
      ],
      "metadata": {
        "id": "esNtHTipLiFY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, AveragePooling2D, Flatten, Dense, Activation, MaxPool2D, BatchNormalization, Dropout, ZeroPadding2D"
      ],
      "metadata": {
        "id": "GC-StSy7Lkmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### VGG-16 (configuration D)"
      ],
      "metadata": {
        "id": "aNoPYzDjLnSq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "\n",
        "# first block\n",
        "model.add(Conv2D(filters=64, kernel_size=(3,3), strides=(1,1), activation='relu', padding='same',input_shape=(224,224, 3)))\n",
        "model.add(Conv2D(filters=64, kernel_size=(3,3), strides=(1,1), activation='relu', padding='same'))\n",
        "model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
        "\n",
        "# second block\n",
        "model.add(Conv2D(filters=128, kernel_size=(3,3), strides=(1,1), activation='relu', padding='same'))\n",
        "model.add(Conv2D(filters=128, kernel_size=(3,3), strides=(1,1), activation='relu', padding='same'))\n",
        "model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
        "\n",
        "# third block\n",
        "model.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding='same'))\n",
        "model.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding='same'))\n",
        "model.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding='same'))\n",
        "model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
        "\n",
        "# forth block\n",
        "model.add(Conv2D(filters=512, kernel_size=(3,3), strides=(1,1), activation='relu', padding='same'))\n",
        "model.add(Conv2D(filters=512, kernel_size=(3,3), strides=(1,1), activation='relu', padding='same'))\n",
        "model.add(Conv2D(filters=512, kernel_size=(3,3), strides=(1,1), activation='relu', padding='same'))\n",
        "model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
        "\n",
        "# fifth block\n",
        "model.add(Conv2D(filters=512, kernel_size=(3,3), strides=(1,1), activation='relu', padding='same'))\n",
        "model.add(Conv2D(filters=512, kernel_size=(3,3), strides=(1,1), activation='relu', padding='same'))\n",
        "model.add(Conv2D(filters=512, kernel_size=(3,3), strides=(1,1), activation='relu', padding='same'))\n",
        "model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
        "\n",
        "# sixth block (classifier)\n",
        "model.add(Flatten())\n",
        "model.add(Dense(4096, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(4096, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1000, activation='softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVyJ-jOiLkot",
        "outputId": "17d8a08b-e0ca-4a12-f1f7-c7254b6be769"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_8 (Conv2D)           (None, 224, 224, 64)      1792      \n",
            "                                                                 \n",
            " conv2d_9 (Conv2D)           (None, 224, 224, 64)      36928     \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 112, 112, 64)     0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_10 (Conv2D)          (None, 112, 112, 128)     73856     \n",
            "                                                                 \n",
            " conv2d_11 (Conv2D)          (None, 112, 112, 128)     147584    \n",
            "                                                                 \n",
            " max_pooling2d_4 (MaxPooling  (None, 56, 56, 128)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_12 (Conv2D)          (None, 56, 56, 256)       295168    \n",
            "                                                                 \n",
            " conv2d_13 (Conv2D)          (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " conv2d_14 (Conv2D)          (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " max_pooling2d_5 (MaxPooling  (None, 28, 28, 256)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_15 (Conv2D)          (None, 28, 28, 512)       1180160   \n",
            "                                                                 \n",
            " conv2d_16 (Conv2D)          (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " conv2d_17 (Conv2D)          (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " max_pooling2d_6 (MaxPooling  (None, 14, 14, 512)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_18 (Conv2D)          (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " conv2d_19 (Conv2D)          (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " conv2d_20 (Conv2D)          (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " max_pooling2d_7 (MaxPooling  (None, 7, 7, 512)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 25088)             0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 4096)              102764544 \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 4096)              16781312  \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 1000)              4097000   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 138,357,544\n",
            "Trainable params: 138,357,544\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### VGG-19 (configuration E)"
      ],
      "metadata": {
        "id": "8fmkRl_zLqLI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vgg_19 = Sequential()\n",
        "\n",
        "# first block\n",
        "vgg_19.add(Conv2D(filters=64, kernel_size=(3,3), strides=(1,1), activation='relu', padding='same',input_shape=(224,224, 3)))\n",
        "vgg_19.add(Conv2D(filters=64, kernel_size=(3,3), strides=(1,1), activation='relu', padding='same'))\n",
        "vgg_19.add(MaxPool2D((2,2), strides=(2,2)))\n",
        "\n",
        "# second block\n",
        "vgg_19.add(Conv2D(filters=128, kernel_size=(3,3), strides=(1,1), activation='relu', padding='same'))\n",
        "vgg_19.add(Conv2D(filters=128, kernel_size=(3,3), strides=(1,1), activation='relu', padding='same'))\n",
        "vgg_19.add(MaxPool2D((2,2), strides=(2,2)))\n",
        "\n",
        "# third block\n",
        "vgg_19.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding='same'))\n",
        "vgg_19.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding='same'))\n",
        "vgg_19.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding='same'))\n",
        "vgg_19.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding='same'))\n",
        "vgg_19.add(MaxPool2D((2,2), strides=(2,2)))\n",
        "\n",
        "# forth block\n",
        "vgg_19.add(Conv2D(filters=512, kernel_size=(3,3), strides=(1,1), activation='relu', padding='same'))\n",
        "vgg_19.add(Conv2D(filters=512, kernel_size=(3,3), strides=(1,1), activation='relu', padding='same'))\n",
        "vgg_19.add(Conv2D(filters=512, kernel_size=(3,3), strides=(1,1), activation='relu', padding='same'))\n",
        "vgg_19.add(Conv2D(filters=512, kernel_size=(3,3), strides=(1,1), activation='relu', padding='same'))\n",
        "vgg_19.add(MaxPool2D((2,2), strides=(2,2)))\n",
        "\n",
        "# fifth block\n",
        "vgg_19.add(Conv2D(filters=512, kernel_size=(3,3), strides=(1,1), activation='relu', padding='same'))\n",
        "vgg_19.add(Conv2D(filters=512, kernel_size=(3,3), strides=(1,1), activation='relu', padding='same'))\n",
        "vgg_19.add(Conv2D(filters=512, kernel_size=(3,3), strides=(1,1), activation='relu', padding='same'))\n",
        "vgg_19.add(Conv2D(filters=512, kernel_size=(3,3), strides=(1,1), activation='relu', padding='same'))\n",
        "vgg_19.add(MaxPool2D((2,2), strides=(2,2)))\n",
        "\n",
        "# seventh block (classifier)\n",
        "vgg_19.add(Flatten())\n",
        "vgg_19.add(Dense(4096, activation='relu'))\n",
        "vgg_19.add(Dropout(0.5))\n",
        "vgg_19.add(Dense(4096, activation='relu'))\n",
        "vgg_19.add(Dropout(0.5))\n",
        "vgg_19.add(Dense(1000, activation='softmax'))\n",
        "\n",
        "vgg_19.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_IIQtvgLkrB",
        "outputId": "ec1ed4e1-d0e1-45f7-b5b0-39666d6aea71"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_21 (Conv2D)          (None, 224, 224, 64)      1792      \n",
            "                                                                 \n",
            " conv2d_22 (Conv2D)          (None, 224, 224, 64)      36928     \n",
            "                                                                 \n",
            " max_pooling2d_8 (MaxPooling  (None, 112, 112, 64)     0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_23 (Conv2D)          (None, 112, 112, 128)     73856     \n",
            "                                                                 \n",
            " conv2d_24 (Conv2D)          (None, 112, 112, 128)     147584    \n",
            "                                                                 \n",
            " max_pooling2d_9 (MaxPooling  (None, 56, 56, 128)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_25 (Conv2D)          (None, 56, 56, 256)       295168    \n",
            "                                                                 \n",
            " conv2d_26 (Conv2D)          (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " conv2d_27 (Conv2D)          (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " conv2d_28 (Conv2D)          (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " max_pooling2d_10 (MaxPoolin  (None, 28, 28, 256)      0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_29 (Conv2D)          (None, 28, 28, 512)       1180160   \n",
            "                                                                 \n",
            " conv2d_30 (Conv2D)          (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " conv2d_31 (Conv2D)          (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " conv2d_32 (Conv2D)          (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " max_pooling2d_11 (MaxPoolin  (None, 14, 14, 512)      0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_33 (Conv2D)          (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " conv2d_34 (Conv2D)          (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " conv2d_35 (Conv2D)          (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " conv2d_36 (Conv2D)          (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " max_pooling2d_12 (MaxPoolin  (None, 7, 7, 512)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten_3 (Flatten)         (None, 25088)             0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 4096)              102764544 \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 4096)              16781312  \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 1000)              4097000   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 143,667,240\n",
            "Trainable params: 143,667,240\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inception(Going Deeper With Convolutions)\n",
        "\n",
        "\n",
        "참고 : https://kangbk0120.github.io/articles/2018-01/inception-googlenet-review\n",
        "\n",
        "참고 : https://jjuon.tistory.com/26\n",
        "\n",
        "[Going Deeper with Convolutions](https://arxiv.org/abs/1409.4842)\n",
        "\n",
        "## Introduction\n",
        "\n",
        "-  GoogLeNet은 본 논문에서 소개될 Inception 모듈의 한 형태이며, ILSVRC 2014에서 top-5 error 6.67%로 우승을 차지했습니다.\n",
        "\n",
        "- GoogLeNEt은 22개의 층으로 구성되었으나(VGGNet보다 층수는 많다.)\n",
        "\n",
        "- 최근 3년간(2012~2015) CNN 분야는 급속도로 발전해 왔습니다. 이러한 발전은 대개 하드웨어의 발전뿐만 아니라 주로 네트워크 구조에 대한 기발한 아이디어가 원인입니다. 본 논문에서 소개될 GoogLeNet은 AlexNet에 비해 1/12배 작은 파라미터를 가지면서 더 정확한 성능을 냈습니다.(VGGNet : 1억 3800만개, GoogLeNet:1,300만개)\n",
        "\n",
        " \n",
        "\n",
        "- GoogLeNet에서 주목할 만한 점은 본 논문에서는 전력과 메모리 사용을 효율적으로 설계하여 모바일이나 임베디드 환경에 적용시킬 수 있게 한 것입니다. GoogLeNet은 inference시에 합곱(multiply-adds) 연산 횟수를 15억 번 이하로 지정하여 단순히 학문적 호기심이 아닌 실제로 적용시킬 수 있게 하였습니다.\n",
        "\n",
        "\n",
        "- GoogLeNet을 이루는 Inception 모듈은 Lin et al.에서 유래했습니다. 본 논문의 제목에서 deep는 두 가지 의미를 가집니다. 첫번째론 **Inception 모듈**이라는 새로운 형태의 구성을 제안한 것이고, 두번째론 말 **그대로 매우 깊은 network**를 의미합니다. \n",
        "\n",
        "\n",
        "- 일반적으로 Convolution Neural Network(이하 CNN)에서 네트워크가 깊어지면 깊어질수록, 즉 더 많은 레이어를 가질수록 성능이 좋아지는 것은 분명함. 이러한 방식의 문제점은 네트워크가 깊어질수록 학습해야하는 파라미터의 수가 늘어난다는 것에 있습니다. 특히 이러한 특징이 두드러지는 것은 깊은 곳에 위치한 Convolution연산입니다.\n",
        "\n",
        "- 대부분의 CNN에서 이미지가 Convolution연산을 거치게 되면 channel은 커지고 height와 width는 줄어듭니다. 자 구체적인 예를 들어 자세히 알아봅시다. 192 x 28 x 28(C x H x W)의 크기를 가지는 데이터가 있다고 해볼까요? 필터의 크기는 5x5, 스트라이드는 1, 패딩은 2라고 합시다. 필터가 32개 있다고 하면 결과는 32 x 28 x 28이 되겠죠. 문제는 여기서 생겨납니다. 각 필터는 192 x 5 x 5의 크기를 갖고, 결과는 32x28x28이므로, 총 필요한 연산은 192x5x5x32x28x28이 됩니다. 이는 120만개에 이르는 엄청난 연산을 필요로 하죠. 이러한 연산들을 몇 층씩 쌓는다는 것은 너무 부담스러운 일입니다.\n",
        "\n",
        "- 만약 직접 모델을 설계해야하는 상황이라면? 필터의 크기와 같은 하이퍼파라미터를 설정하는 것에서부터 고민이 시작될 겁니다. 하지만 Inception이라는 것을 사용하면 이러한 문제를 어느정도 해결할 수 있습니다.\n",
        "\n",
        "- 커널 크기나 풀링층의 배치는 대체로 실험을 통해 시행착오를 거치며 최적의 결과를 내는 값을 찾아 결정함. 인셉션에서는 필터 크기나 풀링층의 배치를 직접 결정하는 대신 블록 전체에 똑같은 설정을 적용하고 이를 인셉션 모듈이라고 함"
      ],
      "metadata": {
        "id": "3PbPMxSTDiP1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Network in Network\n",
        "\n",
        "본격적으로 논문에 대해 설명하기 전에 잠깐 설명하고 넘어가고 싶은 것이 있습니다. Network in Network라는 논문입니다.\n",
        "\n",
        "우리가 일반적으로 생각하는 Convolution layer는 필터가 움직이면서 해당하는 입력 부분과 곱/합 연산을 수행하고, 활성화함수를 거쳐 결과를 만들어낸다고 생각할 수 있습니다. 근데 만약 데이터의 분포가 저러한 선형 관계로 표현될 수 없는 비선형적인 관계라면? 그래서 이 논문에서는 비선형적 관계를 표현할 수 있도록, 단순한 곱/합 연산이 아니라 Multi Layer Perceptron, 즉 MLP를 중간에 넣어버립니다. 그래서 이 논문의 제목이 Network in Network(NIN)인 겁니다."
      ],
      "metadata": {
        "id": "I8G2KCaQpuCp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://user-images.githubusercontent.com/25279765/34998212-cf38899a-fb21-11e7-97a2-beb346108f2c.jpg'>"
      ],
      "metadata": {
        "id": "AQzD7XQHpwo5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 왼쪽이 우리가 생각하는 일반적인 Conv 레이어, 오른쪽이 이 논문에서 제안하는 MLP Conv"
      ],
      "metadata": {
        "id": "SRxmDLK1pz1t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "논문에 따르면, MLP 또한 Convolution의 필터처럼 역전파를 통해 학습되고, 그 자체가 깊은 구조를 가질 수 있으므로 MLP를 사용했다고 합니다."
      ],
      "metadata": {
        "id": "KdT_6dPop2Uo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://user-images.githubusercontent.com/25279765/34998215-d150d458-fb21-11e7-961d-1f677b421529.jpg'>"
      ],
      "metadata": {
        "id": "cC-erjeNp3GC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "이런 식으로 MLP Conv를 세 개 쌓고, 마지막에 Fully-Connected Layer를 넣는 대신 Global Average Pooling을 넣었습니다. 이는 오버피팅을 방지하는 효과가 있다고 합니다. 그런데 여기서 한가지 중요한 점이 생겨납니다.\n",
        "\n",
        "자 여기서 MLP Conv의 연산을 식으로 나타내면(\n",
        "i\n",
        ",\n",
        "j\n",
        "는 feature map에서 픽셀의 위치, \n",
        "k\n",
        "는 feature map의 k번째 채널, \n",
        "n\n",
        "은 MLP Conv의 n번째 레이어)\n",
        "\n",
        "$f_{i,j,k_1}^{1} = max(w_{k_1}^{1 \\intercal}x_{i,j} + b_{k_1}, 0)$\n",
        "…\n",
        "\n",
        "$f_{i,j,k_n}^{n} = max(w_{k_n}^{n \\intercal}f_{i,j}^{n-1} + b_{k_n}, 0)$\n",
        "이 됩니다. 첫번째식은 일반적인 CNN 연산($f_{i,j,k} = max(w_{k}^T,0)$ $i,j$는 픽셀의  index, $x_{i,j}$는 그 점을 중심으로 하는 input patch이며,k는 채널의 index)과 동일하다고 볼 수 있습니다. 물론 bias가 더해져 있긴 하지만 CNN에서도 bias를 추가해줄 수 있으니까요. 중요한 것은 마지막 식입니다. 마지막 식은 cross channel parametri(c pooling(CCCP)을 일반적인 Conv 레이어에 적용한 것과 같습니다. 이는 첫번째 식이 일반적인 CNN과 동일하기 때문인데요, 무슨 소린지 모르겠죠?"
      ],
      "metadata": {
        "id": "CSthkh6hp6ao"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://user-images.githubusercontent.com/25279765/35000804-aacc58f0-fb28-11e7-9b27-f28c4a6568a2.jpg'>"
      ],
      "metadata": {
        "id": "yi-Ikwk9qfmp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "이렇게 일반적인 Convolution 연산을 거치고, 마지막에 하나의 값으로 매핑하는 과정을 CCCP라고 볼 수 있습니다. 즉\n",
        "\n"
      ],
      "metadata": {
        "id": "E4JNcbgVqiaA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://user-images.githubusercontent.com/25279765/35000899-eae138fc-fb28-11e7-9271-1faf59f7f3d6.jpg'>"
      ],
      "metadata": {
        "id": "jXVdCh0pqjbp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "요 사진의 연산을 수행한다고 볼 수 있습니다. 위의 두 사진에서 볼 수 있듯, 이 과정(n개의 레이어를 사용하는 MLP Conv)은 일반적인 Convolution 연산을 적용한 다음필터 크기가 1x1인 Convolution을 적용한 것과 동일합니다. 앞에서 말한 NIN의 목적이 무엇이었는지 기억하시나요? MLP의 도입을 통해 비선형적인 관계를 더 잘 표현하는 것이었습니다. 바로 위에서 보였듯, 결국 MLP를 통해 구하는 관계는 일반적인 CNN과 1x1 Conv의 결합으로도 표현할 수 있습니다. 즉 NIN의 의의는 이러한 1x1 Conv의 도입이라고 할 수 있죠. 결과적으로 1x1 Conv를 적절하게 사용하면 비선형적 함수를 더 잘 만들어낼 수 있게 되는 것입니다. 또한 1x1 Conv의 장점은 이것만이 아닙니다. 1x1 Conv는 채널 단위에서 Pooling을 해줍니다. 즉 1x1 Conv의 수를 입력의 채널보다 작게 하면 dimension reduction, 차원 축소가 가능한 것이죠."
      ],
      "metadata": {
        "id": "zI8LeT9Pqlma"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be_lbwWraBSV"
      },
      "source": [
        "# GoogleNet\n",
        "\n",
        "\n",
        "- GoogLeNet은 2014년 이미지넷 이미지 인식 대회(ILSVRC)에서 VGGNet(VGG19)을 이기고 우승을 차지한 알고리즘이다. GoogLeNet은 19층의 VGG19보다 좀 더 깊은 22층으로 구성되어 있다. GoogLeNet의 original 논문은 Christian Szegedy 등에 의해 2015 CVPR에 개제된 \"Going Deeper with Convolutions\"이다. GoogLeNet이란 이름에서 알 수 있듯이 구글이 이 알고리즘 개발에 참여했다. \n",
        "\n",
        "## GoogleNet의 구조\n",
        "\n",
        "<img src = 'https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FIq9NO%2FbtqyPWk5PBX%2FK2JicGjIjj5w0eFIbhx4bK%2Fimg.png'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOJFvfowbapz"
      },
      "source": [
        "GoogLeNet은 총 9개의 인셉션 모듈을 포함하고 있다. 인셉션 모듈을 하나 확대해서 자세히 살펴보자. \n",
        "\n",
        "<img src = 'https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2F14Um2%2FbtqyQ5nKlEA%2FhjSsZaYiBukseySytXWFCK%2Fimg.png'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8ECMPOGbuYL"
      },
      "source": [
        "GoogLeNet에 실제로 사용된 모듈은 1x1 컨볼루션이 포함된 (b) 모델이다. 아까 살펴봤듯이 1x1 컨볼루션은 특성맵의 장수를 줄여주는 역할을 한다. 노란색 블럭으로 표현된 1x1 컨볼루션을 제외한 나이브(naive) 버전을 살펴보면, 이전 층에서 생성된 특성맵을 1x1 컨볼루션, 3x3 컨볼루션, 5x5 컨볼루션, 3x3 최대풀링해준 결과 얻은 특성맵들을 모두 함께 쌓아준다. AlexNet, VGGNet 등의 이전 CNN 모델들은 한 층에서 동일한 사이즈의 필터커널을 이용해서 컨볼루션을 해줬던 것과 차이가 있다. 따라서 좀 더 다양한 종류의 특성이 도출된다. 여기에 1x1 컨볼루션이 포함되었으니 당연히 연산량은 많이 줄어들었을 것이다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yugsEQ1aNMz"
      },
      "source": [
        "### 1X1 컨볼루션\n",
        "\n",
        "<img src = 'https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FMzPze%2FbtqyQy5e3NM%2F5HPtmAwVQzKJTj6wgWautk%2Fimg.png'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TklGWijuaSNb"
      },
      "source": [
        "1 x 1 컨볼루션은 어떤 의미를 갖는 것일까? 왜 해주는 것일까? GoogLeNet에서 1 x 1 컨볼루션은 특성맵의 갯수를 줄이는 목적으로 사용된다. 특성맵의 갯수가 줄어들면 그만큼 연산량이 줄어든다.\n",
        "\n",
        " \n",
        "\n",
        "예를 들어, 480장의 14 x 14 사이즈의 특성맵(14 x 14 x 480)이 있다고 가정해보자. 이것을 48개의 5 x 5 x 480의 필터커널로 컨볼루션을 해주면 48장의 14 x 14의 특성맵(14 x 14 x 48)이 생성된다. (zero padding을 2로, 컨볼루션 보폭은 1로 설정했다고 가정했다.) 이때 필요한 연산횟수는 얼마나 될까? 바로 (14 x 14 x 48) x (5 x 5 x 480) = 약 112.9M이 된다. \n",
        "\n",
        "<img src = 'https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fbt4AxN%2FbtqyQ6NHO6u%2FezqfgBmWfkN5N2C49icbR1%2Fimg.png'>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 차원 축소층(1x1  합성곱층)\n",
        "    - 1x1 합성곱층을 사용해서 1억 6300만 회의 곱셈을 약 10분의 1로 줄일 수 있다. 이 층을 축소층(reduce layer)이라고 부르는 것도 이 때문이다.\n",
        "    - 3x3 또는 5x5처럼 크기가 큰 필터를 포함하는 합성곱층 앞에 1x1 합성곱층을 배치해서 입력의 깊이를 축소하고 필요한 연산 횟수를 그만큼 줄인다.\n",
        "    - Bottleneck layer이라고 하는데, 신경망 구조 내에서 차원을 축소하는 이 층의 역할이 마치 병에서 가장 좁은 병목을 연상케 하기 때문."
      ],
      "metadata": {
        "id": "D9lsSoVlGMAK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFABAHdabW2C"
      },
      "source": [
        "### Inception 모듈\n",
        "\n",
        "<img src = 'https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbHZHKC%2FbtqyQ5aekdF%2F3rkScmoIxS4P4fia96lQwk%2Fimg.png'>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAtY-yeJbwnK"
      },
      "source": [
        "### Global average Pooling\n",
        "\n",
        "AlexNet, VGGNet 등에서는 fully connected (FC) 층들이 망의 후반부에 연결되어 있다. 그러나 GoogLeNet은 FC 방식 대신에 global average pooling이란 방식을 사용한다. global average pooling은 전 층에서 산출된 특성맵들을 각각 평균낸 것을 이어서 1차원 벡터를 만들어주는 것이다. 1차원 벡터를 만들어줘야 최종적으로 이미지 분류를 위한 softmax 층을 연결해줄 수 있기 때문이다. 만약 전 층에서 1024장의 7 x 7의 특성맵이 생성되었다면, 1024장의 7 x 7 특성맵 각각 평균내주어 얻은 1024개의 값을 하나의 벡터로 연결해주는 것이다.\n",
        "\n",
        "<img src = 'https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbwTHh0%2FbtqB2uyArWI%2FqBr48Ik8bl4bK1oOEJa3bk%2Fimg.png'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkIepWGmb5xj"
      },
      "source": [
        "이렇게 해줌으로 얻을 수 있는 장점은 가중치의 갯수를 상당히 많이 없애준다는 것이다. 만약 FC 방식을 사용한다면 훈련이 필요한 가중치의 갯수가 7 x 7 x 1024 x 1024 = 51.3M이지만 global average pooling을 사용하면 가중치가 단 한개도 필요하지 않다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsW6eHFPb7k7"
      },
      "source": [
        "### auxiliary classifier\n",
        "\n",
        "네트워크의 깊이가 깊어지면 깊어질수록 vanishing gradient 문제를 피하기 어려워진다. 그러니까 가중치를 훈련하는 과정에 역전파(back propagation)를 주로 활용하는데, 역전파과정에서 가중치를 업데이트하는데 사용되는 gradient가 점점 작아져서 0이 되어버리는 것이다. 따라서 네트워크 내의 가중치들이 제대로 훈련되지 않는다. 이 문제를 극복하기 위해서 GoogLeNet에서는 네트워크 중간에 두 개의 보조 분류기(auxiliary classifier)를 달아주었다. \n",
        "\n",
        "<img src = 'https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbD5poT%2FbtqyQM98EkX%2FnbxasUSmCO1WnaIyIsvUD0%2Fimg.png'>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Deep learning for Vision systems book 참고"
      ],
      "metadata": {
        "id": "dMYs7ZyPJX8m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Import libraries"
      ],
      "metadata": {
        "id": "utVpaMKmHmIU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.layers import Layer\n",
        "import keras.backend as K\n",
        "import tensorflow as tf\n",
        "from keras.datasets import cifar10"
      ],
      "metadata": {
        "id": "P0nUJo3kHpAd"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Conv2D, MaxPool2D,  \\\n",
        "    Dropout, Dense, Input, concatenate,      \\\n",
        "    GlobalAveragePooling2D, AveragePooling2D,\\\n",
        "    Flatten\n",
        "\n",
        "import cv2 \n",
        "import numpy as np \n",
        "from keras.datasets import cifar10 \n",
        "from keras import backend as K \n",
        "from keras.utils import np_utils\n",
        "\n",
        "import math \n",
        "from keras.optimizers import SGD \n",
        "from keras.callbacks import LearningRateScheduler"
      ],
      "metadata": {
        "id": "q60f6pjTQTrO"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Prepare the dataset"
      ],
      "metadata": {
        "id": "e-8lQcwxHqYq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 10\n",
        "\n",
        "def load_cifar10_data(img_rows, img_cols):\n",
        "\n",
        "    # Load cifar10 training and validation sets\n",
        "    (X_train, Y_train), (X_valid, Y_valid) = cifar10.load_data()\n",
        "\n",
        "    # Resize training images\n",
        "    X_train = np.array([cv2.resize(img, (img_rows,img_cols)) for img in X_train[:,:,:,:]])\n",
        "    X_valid = np.array([cv2.resize(img, (img_rows,img_cols)) for img in X_valid[:,:,:,:]])\n",
        "\n",
        "    # Transform targets to keras compatible format\n",
        "    Y_train = np_utils.to_categorical(Y_train, num_classes)\n",
        "    Y_valid = np_utils.to_categorical(Y_valid, num_classes)\n",
        "    \n",
        "    X_train = X_train.astype('float32')\n",
        "    X_valid = X_valid.astype('float32')\n",
        "\n",
        "    # preprocess data\n",
        "    X_train = X_train / 255.0\n",
        "    X_valid = X_valid / 255.0\n",
        "\n",
        "    return X_train, Y_train, X_valid, Y_valid"
      ],
      "metadata": {
        "id": "UT8P_uV3HpCJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train, X_test, y_test = load_cifar10_data(224, 224)"
      ],
      "metadata": {
        "id": "znoERJZ0HpEA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Create the inception modeule"
      ],
      "metadata": {
        "id": "KyhZVQAnH5T-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://raw.githubusercontent.com/moelgendy/deep_learning_for_vision_systems/2c9d077b43003657cd8f6d5ddfb6f83ee8bae1f3/chapter_05/images/inception_module.png'>"
      ],
      "metadata": {
        "id": "_rZiavTvH8d2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def inception_module(x,\n",
        "                     filters_1x1,\n",
        "                     filters_3x3_reduce,\n",
        "                     filters_3x3,\n",
        "                     filters_5x5_reduce,\n",
        "                     filters_5x5,\n",
        "                     filters_pool_proj,\n",
        "                     name=None):\n",
        "    \n",
        "    conv_1x1 = Conv2D(filters_1x1, (1, 1), padding='same', activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init)(x)\n",
        "    \n",
        "    conv_3x3 = Conv2D(filters_3x3_reduce, (1, 1), padding='same', activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init)(x)\n",
        "    conv_3x3 = Conv2D(filters_3x3, (3, 3), padding='same', activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init)(conv_3x3)\n",
        "\n",
        "    conv_5x5 = Conv2D(filters_5x5_reduce, (1, 1), padding='same', activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init)(x)\n",
        "    conv_5x5 = Conv2D(filters_5x5, (5, 5), padding='same', activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init)(conv_5x5)\n",
        "\n",
        "    pool_proj = MaxPool2D((3, 3), strides=(1, 1), padding='same')(x)\n",
        "    pool_proj = Conv2D(filters_pool_proj, (1, 1), padding='same', activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init)(pool_proj)\n",
        "\n",
        "    output = concatenate([conv_1x1, conv_3x3, conv_5x5, pool_proj], axis=3, name=name)\n",
        "    \n",
        "    return output"
      ],
      "metadata": {
        "id": "6exfMlKkHpGJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Cretate the GoogLeNet architecture\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/moelgendy/deep_learning_for_vision_systems/2c9d077b43003657cd8f6d5ddfb6f83ee8bae1f3/chapter_05/images/inception_architecture.png'>"
      ],
      "metadata": {
        "id": "nA8zP7srIBJs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kernel_init = keras.initializers.glorot_uniform()\n",
        "bias_init = keras.initializers.Constant(value=0.2)"
      ],
      "metadata": {
        "id": "_ogjJ1pnHpIL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_layer = Input(shape=(224, 224, 3))\n",
        "\n",
        "x = Conv2D(64, (7, 7), padding='same', strides=(2, 2), activation='relu', name='conv_1_7x7/2', kernel_initializer=kernel_init, bias_initializer=bias_init)(input_layer)\n",
        "x = MaxPool2D((3, 3), padding='same', strides=(2, 2), name='max_pool_1_3x3/2')(x)\n",
        "x = Conv2D(64, (1, 1), padding='same', strides=(1, 1), activation='relu', name='conv_2a_3x3/1')(x)\n",
        "x = Conv2D(192, (3, 3), padding='same', strides=(1, 1), activation='relu', name='conv_2b_3x3/1')(x)\n",
        "x = MaxPool2D((3, 3), padding='same', strides=(2, 2), name='max_pool_2_3x3/2')(x)\n",
        "\n",
        "x = inception_module(x,\n",
        "                     filters_1x1=64,\n",
        "                     filters_3x3_reduce=96,\n",
        "                     filters_3x3=128,\n",
        "                     filters_5x5_reduce=16,\n",
        "                     filters_5x5=32,\n",
        "                     filters_pool_proj=32,\n",
        "                     name='inception_3a')\n",
        "\n",
        "x = inception_module(x,\n",
        "                     filters_1x1=128,\n",
        "                     filters_3x3_reduce=128,\n",
        "                     filters_3x3=192,\n",
        "                     filters_5x5_reduce=32,\n",
        "                     filters_5x5=96,\n",
        "                     filters_pool_proj=64,\n",
        "                     name='inception_3b')\n",
        "\n",
        "x = MaxPool2D((3, 3), padding='same', strides=(2, 2), name='max_pool_3_3x3/2')(x)\n",
        "\n",
        "x = inception_module(x,\n",
        "                     filters_1x1=192,\n",
        "                     filters_3x3_reduce=96,\n",
        "                     filters_3x3=208,\n",
        "                     filters_5x5_reduce=16,\n",
        "                     filters_5x5=48,\n",
        "                     filters_pool_proj=64,\n",
        "                     name='inception_4a')\n",
        "\n",
        "\n",
        "x1 = AveragePooling2D((5, 5), strides=3)(x)\n",
        "x1 = Conv2D(128, (1, 1), padding='same', activation='relu')(x1)\n",
        "x1 = Flatten()(x1)\n",
        "x1 = Dense(1024, activation='relu')(x1)\n",
        "x1 = Dropout(0.7)(x1)\n",
        "x1 = Dense(10, activation='softmax', name='auxilliary_output_1')(x1)\n",
        "\n",
        "x = inception_module(x,\n",
        "                     filters_1x1=160,\n",
        "                     filters_3x3_reduce=112,\n",
        "                     filters_3x3=224,\n",
        "                     filters_5x5_reduce=24,\n",
        "                     filters_5x5=64,\n",
        "                     filters_pool_proj=64,\n",
        "                     name='inception_4b')\n",
        "\n",
        "x = inception_module(x,\n",
        "                     filters_1x1=128,\n",
        "                     filters_3x3_reduce=128,\n",
        "                     filters_3x3=256,\n",
        "                     filters_5x5_reduce=24,\n",
        "                     filters_5x5=64,\n",
        "                     filters_pool_proj=64,\n",
        "                     name='inception_4c')\n",
        "\n",
        "x = inception_module(x,\n",
        "                     filters_1x1=112,\n",
        "                     filters_3x3_reduce=144,\n",
        "                     filters_3x3=288,\n",
        "                     filters_5x5_reduce=32,\n",
        "                     filters_5x5=64,\n",
        "                     filters_pool_proj=64,\n",
        "                     name='inception_4d')\n",
        "\n",
        "\n",
        "x2 = AveragePooling2D((5, 5), strides=3)(x)\n",
        "x2 = Conv2D(128, (1, 1), padding='same', activation='relu')(x2)\n",
        "x2 = Flatten()(x2)\n",
        "x2 = Dense(1024, activation='relu')(x2)\n",
        "x2 = Dropout(0.7)(x2)\n",
        "x2 = Dense(10, activation='softmax', name='auxilliary_output_2')(x2)\n",
        "\n",
        "x = inception_module(x,\n",
        "                     filters_1x1=256,\n",
        "                     filters_3x3_reduce=160,\n",
        "                     filters_3x3=320,\n",
        "                     filters_5x5_reduce=32,\n",
        "                     filters_5x5=128,\n",
        "                     filters_pool_proj=128,\n",
        "                     name='inception_4e')\n",
        "\n",
        "x = MaxPool2D((3, 3), padding='same', strides=(2, 2), name='max_pool_4_3x3/2')(x)\n",
        "\n",
        "x = inception_module(x,\n",
        "                     filters_1x1=256,\n",
        "                     filters_3x3_reduce=160,\n",
        "                     filters_3x3=320,\n",
        "                     filters_5x5_reduce=32,\n",
        "                     filters_5x5=128,\n",
        "                     filters_pool_proj=128,\n",
        "                     name='inception_5a')\n",
        "\n",
        "x = inception_module(x,\n",
        "                     filters_1x1=384,\n",
        "                     filters_3x3_reduce=192,\n",
        "                     filters_3x3=384,\n",
        "                     filters_5x5_reduce=48,\n",
        "                     filters_5x5=128,\n",
        "                     filters_pool_proj=128,\n",
        "                     name='inception_5b')\n",
        "\n",
        "x = GlobalAveragePooling2D(name='avg_pool_5_3x3/1')(x)\n",
        "\n",
        "x = Dropout(0.4)(x)\n",
        "\n",
        "x = Dense(10, activation='softmax', name='output')(x)"
      ],
      "metadata": {
        "id": "fx_cZkRpHpJ_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model(input_layer, [x, x1, x2], name='inception_v1')"
      ],
      "metadata": {
        "id": "I8H85tYMIZUw"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "3xmixnIFIaHA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9323ea5-0cb6-460b-e7f7-6a657aac24da"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"inception_v1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv_1_7x7/2 (Conv2D)          (None, 112, 112, 64  9472        ['input_1[0][0]']                \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " max_pool_1_3x3/2 (MaxPooling2D  (None, 56, 56, 64)  0           ['conv_1_7x7/2[0][0]']           \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv_2a_3x3/1 (Conv2D)         (None, 56, 56, 64)   4160        ['max_pool_1_3x3/2[0][0]']       \n",
            "                                                                                                  \n",
            " conv_2b_3x3/1 (Conv2D)         (None, 56, 56, 192)  110784      ['conv_2a_3x3/1[0][0]']          \n",
            "                                                                                                  \n",
            " max_pool_2_3x3/2 (MaxPooling2D  (None, 28, 28, 192)  0          ['conv_2b_3x3/1[0][0]']          \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 28, 28, 96)   18528       ['max_pool_2_3x3/2[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)              (None, 28, 28, 16)   3088        ['max_pool_2_3x3/2[0][0]']       \n",
            "                                                                                                  \n",
            " max_pooling2d (MaxPooling2D)   (None, 28, 28, 192)  0           ['max_pool_2_3x3/2[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 28, 28, 64)   12352       ['max_pool_2_3x3/2[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)              (None, 28, 28, 128)  110720      ['conv2d_1[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)              (None, 28, 28, 32)   12832       ['conv2d_3[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)              (None, 28, 28, 32)   6176        ['max_pooling2d[0][0]']          \n",
            "                                                                                                  \n",
            " inception_3a (Concatenate)     (None, 28, 28, 256)  0           ['conv2d[0][0]',                 \n",
            "                                                                  'conv2d_2[0][0]',               \n",
            "                                                                  'conv2d_4[0][0]',               \n",
            "                                                                  'conv2d_5[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)              (None, 28, 28, 128)  32896       ['inception_3a[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_9 (Conv2D)              (None, 28, 28, 32)   8224        ['inception_3a[0][0]']           \n",
            "                                                                                                  \n",
            " max_pooling2d_1 (MaxPooling2D)  (None, 28, 28, 256)  0          ['inception_3a[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)              (None, 28, 28, 128)  32896       ['inception_3a[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)              (None, 28, 28, 192)  221376      ['conv2d_7[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_10 (Conv2D)             (None, 28, 28, 96)   76896       ['conv2d_9[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_11 (Conv2D)             (None, 28, 28, 64)   16448       ['max_pooling2d_1[0][0]']        \n",
            "                                                                                                  \n",
            " inception_3b (Concatenate)     (None, 28, 28, 480)  0           ['conv2d_6[0][0]',               \n",
            "                                                                  'conv2d_8[0][0]',               \n",
            "                                                                  'conv2d_10[0][0]',              \n",
            "                                                                  'conv2d_11[0][0]']              \n",
            "                                                                                                  \n",
            " max_pool_3_3x3/2 (MaxPooling2D  (None, 14, 14, 480)  0          ['inception_3b[0][0]']           \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv2d_13 (Conv2D)             (None, 14, 14, 96)   46176       ['max_pool_3_3x3/2[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_15 (Conv2D)             (None, 14, 14, 16)   7696        ['max_pool_3_3x3/2[0][0]']       \n",
            "                                                                                                  \n",
            " max_pooling2d_2 (MaxPooling2D)  (None, 14, 14, 480)  0          ['max_pool_3_3x3/2[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_12 (Conv2D)             (None, 14, 14, 192)  92352       ['max_pool_3_3x3/2[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_14 (Conv2D)             (None, 14, 14, 208)  179920      ['conv2d_13[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_16 (Conv2D)             (None, 14, 14, 48)   19248       ['conv2d_15[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_17 (Conv2D)             (None, 14, 14, 64)   30784       ['max_pooling2d_2[0][0]']        \n",
            "                                                                                                  \n",
            " inception_4a (Concatenate)     (None, 14, 14, 512)  0           ['conv2d_12[0][0]',              \n",
            "                                                                  'conv2d_14[0][0]',              \n",
            "                                                                  'conv2d_16[0][0]',              \n",
            "                                                                  'conv2d_17[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_20 (Conv2D)             (None, 14, 14, 112)  57456       ['inception_4a[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_22 (Conv2D)             (None, 14, 14, 24)   12312       ['inception_4a[0][0]']           \n",
            "                                                                                                  \n",
            " max_pooling2d_3 (MaxPooling2D)  (None, 14, 14, 512)  0          ['inception_4a[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_19 (Conv2D)             (None, 14, 14, 160)  82080       ['inception_4a[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_21 (Conv2D)             (None, 14, 14, 224)  226016      ['conv2d_20[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_23 (Conv2D)             (None, 14, 14, 64)   38464       ['conv2d_22[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_24 (Conv2D)             (None, 14, 14, 64)   32832       ['max_pooling2d_3[0][0]']        \n",
            "                                                                                                  \n",
            " inception_4b (Concatenate)     (None, 14, 14, 512)  0           ['conv2d_19[0][0]',              \n",
            "                                                                  'conv2d_21[0][0]',              \n",
            "                                                                  'conv2d_23[0][0]',              \n",
            "                                                                  'conv2d_24[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_26 (Conv2D)             (None, 14, 14, 128)  65664       ['inception_4b[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_28 (Conv2D)             (None, 14, 14, 24)   12312       ['inception_4b[0][0]']           \n",
            "                                                                                                  \n",
            " max_pooling2d_4 (MaxPooling2D)  (None, 14, 14, 512)  0          ['inception_4b[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_25 (Conv2D)             (None, 14, 14, 128)  65664       ['inception_4b[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_27 (Conv2D)             (None, 14, 14, 256)  295168      ['conv2d_26[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_29 (Conv2D)             (None, 14, 14, 64)   38464       ['conv2d_28[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_30 (Conv2D)             (None, 14, 14, 64)   32832       ['max_pooling2d_4[0][0]']        \n",
            "                                                                                                  \n",
            " inception_4c (Concatenate)     (None, 14, 14, 512)  0           ['conv2d_25[0][0]',              \n",
            "                                                                  'conv2d_27[0][0]',              \n",
            "                                                                  'conv2d_29[0][0]',              \n",
            "                                                                  'conv2d_30[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_32 (Conv2D)             (None, 14, 14, 144)  73872       ['inception_4c[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_34 (Conv2D)             (None, 14, 14, 32)   16416       ['inception_4c[0][0]']           \n",
            "                                                                                                  \n",
            " max_pooling2d_5 (MaxPooling2D)  (None, 14, 14, 512)  0          ['inception_4c[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_31 (Conv2D)             (None, 14, 14, 112)  57456       ['inception_4c[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_33 (Conv2D)             (None, 14, 14, 288)  373536      ['conv2d_32[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_35 (Conv2D)             (None, 14, 14, 64)   51264       ['conv2d_34[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_36 (Conv2D)             (None, 14, 14, 64)   32832       ['max_pooling2d_5[0][0]']        \n",
            "                                                                                                  \n",
            " inception_4d (Concatenate)     (None, 14, 14, 528)  0           ['conv2d_31[0][0]',              \n",
            "                                                                  'conv2d_33[0][0]',              \n",
            "                                                                  'conv2d_35[0][0]',              \n",
            "                                                                  'conv2d_36[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_39 (Conv2D)             (None, 14, 14, 160)  84640       ['inception_4d[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_41 (Conv2D)             (None, 14, 14, 32)   16928       ['inception_4d[0][0]']           \n",
            "                                                                                                  \n",
            " max_pooling2d_6 (MaxPooling2D)  (None, 14, 14, 528)  0          ['inception_4d[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_38 (Conv2D)             (None, 14, 14, 256)  135424      ['inception_4d[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_40 (Conv2D)             (None, 14, 14, 320)  461120      ['conv2d_39[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_42 (Conv2D)             (None, 14, 14, 128)  102528      ['conv2d_41[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_43 (Conv2D)             (None, 14, 14, 128)  67712       ['max_pooling2d_6[0][0]']        \n",
            "                                                                                                  \n",
            " inception_4e (Concatenate)     (None, 14, 14, 832)  0           ['conv2d_38[0][0]',              \n",
            "                                                                  'conv2d_40[0][0]',              \n",
            "                                                                  'conv2d_42[0][0]',              \n",
            "                                                                  'conv2d_43[0][0]']              \n",
            "                                                                                                  \n",
            " max_pool_4_3x3/2 (MaxPooling2D  (None, 7, 7, 832)   0           ['inception_4e[0][0]']           \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv2d_45 (Conv2D)             (None, 7, 7, 160)    133280      ['max_pool_4_3x3/2[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_47 (Conv2D)             (None, 7, 7, 32)     26656       ['max_pool_4_3x3/2[0][0]']       \n",
            "                                                                                                  \n",
            " max_pooling2d_7 (MaxPooling2D)  (None, 7, 7, 832)   0           ['max_pool_4_3x3/2[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_44 (Conv2D)             (None, 7, 7, 256)    213248      ['max_pool_4_3x3/2[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_46 (Conv2D)             (None, 7, 7, 320)    461120      ['conv2d_45[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_48 (Conv2D)             (None, 7, 7, 128)    102528      ['conv2d_47[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_49 (Conv2D)             (None, 7, 7, 128)    106624      ['max_pooling2d_7[0][0]']        \n",
            "                                                                                                  \n",
            " inception_5a (Concatenate)     (None, 7, 7, 832)    0           ['conv2d_44[0][0]',              \n",
            "                                                                  'conv2d_46[0][0]',              \n",
            "                                                                  'conv2d_48[0][0]',              \n",
            "                                                                  'conv2d_49[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_51 (Conv2D)             (None, 7, 7, 192)    159936      ['inception_5a[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_53 (Conv2D)             (None, 7, 7, 48)     39984       ['inception_5a[0][0]']           \n",
            "                                                                                                  \n",
            " max_pooling2d_8 (MaxPooling2D)  (None, 7, 7, 832)   0           ['inception_5a[0][0]']           \n",
            "                                                                                                  \n",
            " average_pooling2d (AveragePool  (None, 4, 4, 512)   0           ['inception_4a[0][0]']           \n",
            " ing2D)                                                                                           \n",
            "                                                                                                  \n",
            " average_pooling2d_1 (AveragePo  (None, 4, 4, 528)   0           ['inception_4d[0][0]']           \n",
            " oling2D)                                                                                         \n",
            "                                                                                                  \n",
            " conv2d_50 (Conv2D)             (None, 7, 7, 384)    319872      ['inception_5a[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_52 (Conv2D)             (None, 7, 7, 384)    663936      ['conv2d_51[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_54 (Conv2D)             (None, 7, 7, 128)    153728      ['conv2d_53[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_55 (Conv2D)             (None, 7, 7, 128)    106624      ['max_pooling2d_8[0][0]']        \n",
            "                                                                                                  \n",
            " conv2d_18 (Conv2D)             (None, 4, 4, 128)    65664       ['average_pooling2d[0][0]']      \n",
            "                                                                                                  \n",
            " conv2d_37 (Conv2D)             (None, 4, 4, 128)    67712       ['average_pooling2d_1[0][0]']    \n",
            "                                                                                                  \n",
            " inception_5b (Concatenate)     (None, 7, 7, 1024)   0           ['conv2d_50[0][0]',              \n",
            "                                                                  'conv2d_52[0][0]',              \n",
            "                                                                  'conv2d_54[0][0]',              \n",
            "                                                                  'conv2d_55[0][0]']              \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 2048)         0           ['conv2d_18[0][0]']              \n",
            "                                                                                                  \n",
            " flatten_1 (Flatten)            (None, 2048)         0           ['conv2d_37[0][0]']              \n",
            "                                                                                                  \n",
            " avg_pool_5_3x3/1 (GlobalAverag  (None, 1024)        0           ['inception_5b[0][0]']           \n",
            " ePooling2D)                                                                                      \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 1024)         2098176     ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 1024)         2098176     ['flatten_1[0][0]']              \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)            (None, 1024)         0           ['avg_pool_5_3x3/1[0][0]']       \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 1024)         0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 1024)         0           ['dense_1[0][0]']                \n",
            "                                                                                                  \n",
            " output (Dense)                 (None, 10)           10250       ['dropout_2[0][0]']              \n",
            "                                                                                                  \n",
            " auxilliary_output_1 (Dense)    (None, 10)           10250       ['dropout[0][0]']                \n",
            "                                                                                                  \n",
            " auxilliary_output_2 (Dense)    (None, 10)           10250       ['dropout_1[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 10,334,030\n",
            "Trainable params: 10,334,030\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Compile a network"
      ],
      "metadata": {
        "id": "_nETLI-QIhlm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 25\n",
        "initial_lrate = 0.01\n",
        "\n",
        "def decay(epoch, steps=100):\n",
        "    initial_lrate = 0.01\n",
        "    drop = 0.96\n",
        "    epochs_drop = 8\n",
        "    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
        "    return lrate\n",
        "\n",
        "sgd = SGD(lr=initial_lrate, momentum=0.9, nesterov=False)\n",
        "\n",
        "lr_sc = LearningRateScheduler(decay, verbose=1)\n",
        "\n",
        "model.compile(loss=['categorical_crossentropy', 'categorical_crossentropy', 'categorical_crossentropy'], loss_weights=[1, 0.3, 0.3], optimizer=sgd, metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "KPeCAckOIlsB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "046ce91e-e09b-4ee9-8387-a15cb9899d67"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train, [y_train, y_train, y_train], validation_data=(X_test, [y_test, y_test, y_test]), epochs=epochs, batch_size=1, callbacks=[lr_sc])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        },
        "id": "QGFcj-OiQ9E8",
        "outputId": "84fa4c99-f566-43ae-e73b-7ce4cf0fa58e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "InternalError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-035f5b01ec0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlr_sc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    100\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYwvkulmcNrz"
      },
      "source": [
        "# Resnet(Deep Residual Learning for Image Recognition)\n",
        "\n",
        "- . 2014년의 GoogLeNet이 22개 층으로 구성된 것에 비해, ResNet은 152개 층을 갖는다. 약 7배나 깊어졌다! \n",
        "\n",
        "<img src = 'https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fcx1l7G%2FbtqzR2RurjQ%2FuRBKXJoxhDZdBjqI2BqWnK%2Fimg.png'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNFioAlWcV0b"
      },
      "source": [
        "## Resnet의 결과\n",
        "\n",
        "<img src ='https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FuiYxJ%2FbtqzU9uR8kC%2FzQi10LVKdbdcwAs1XOhqAk%2Fimg.png'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Gows20gcd0V"
      },
      "source": [
        "위 그래프들을 보면 오히려 더 깊은 구조를 갖는 56층의 네트워크가 20층의 네트워크보다 더 나쁜 성능을 보임을 알 수 있다. 기존의 방식으로는 망을 무조건 깊게 한다고 능사가 아니라는 것을 확인한 것이다. 뭔가 새로운 방법이 있어야 망을 깊게 만드는 효과를 볼 수 있다는 것을 ResNet의 저자들은 깨달았다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NG_GojcFcV2l"
      },
      "source": [
        "### Residual Block\n",
        "\n",
        "그것이 바로 ResNet의 핵심인 Residual Block의 출현을 가능케 했다. 아래 그림에서 오른쪽이 Residual Block을 나타낸다. 기존의 망과 차이가 있다면 입력값을 출력값에 더해줄 수 있도록 지름길(shortcut)을 하나 만들어준 것 뿐이다. \n",
        "\n",
        "<img src = 'https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbFPOry%2FbtqzR2En9ry%2F2DTETgT1BkCrW74hKQCsrk%2Fimg.png'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UN7rBJbFclWR"
      },
      "source": [
        "기존의 신경망은 입력값 x를 타겟값 y로 매핑하는 함수 H(x)를 얻는 것이 목적이었다. 그러나 ResNet은 F(x) + x를 최소화하는 것을 목적으로 한다. x는 현시점에서 변할 수 없는 값이므로 F(x)를 0에 가깝게 만드는 것이 목적이 된다. F(x)가 0이 되면 출력과 입력이 모두 x로 같아지게 된다. F(x) = H(x) - x이므로 F(x)를 최소로 해준다는 것은 H(x) - x를 최소로 해주는 것과 동일한 의미를 지닌다. 여기서 H(x) - x를 잔차(residual)라고 한다. 즉, 잔차를 최소로 해주는 것이므로 ResNet이란 이름이 붙게 된다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8I26yRGcV48"
      },
      "source": [
        "### Resnet의 구조\n",
        "ResNet은 기본적으로 VGG-19의 구조를 뼈대로 한다. 거기에 컨볼루션 층들을 추가해서 깊게 만든 후에, shortcut들을 추가하는 것이 사실상 전부다.\n",
        "\n",
        "<img src = 'https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbdQ7nn%2FbtqzVCKyKVV%2F5nkGhNvCqK9BcIgasYRxH0%2Fimg.jpg'>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DoALeYncxzr"
      },
      "source": [
        "#### Resnet의 구조\n",
        "\n",
        "<img src = 'https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FzwdXd%2FbtqzVoeJwoE%2F9oMcs2Qkj5m07pKPHRmeK0%2Fimg.png'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNlV69Ep455c"
      },
      "source": [
        "# 전이 학습 사용!\n",
        "\n",
        "### pretrained model\n",
        "\n",
        "<img src = 'https://www.researchgate.net/profile/Seunghyoung-Ryu/publication/329954455/figure/fig1/AS:725290594623488@1549934161033/The-structure-of-ResNet-12.png'>\n",
        "\n",
        "\n",
        "<img src = 'https://neurohive.io/wp-content/uploads/2019/01/resnet-e1548261477164.png'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCR5jZ5M62Gt"
      },
      "source": [
        "GAP 참고 :https://gaussian37.github.io/dl-concept-global_average_pooling/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pf3XSp6trVVd"
      },
      "source": [
        "<img src = 'https://paperswithcode.com/media/methods/Screen_Shot_2020-09-25_at_10.26.40_AM_SAB79fQ.png'>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxSDqZu5rIlG"
      },
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "###################keras model##############################################\n",
        "from keras.layers import Dense, Conv2D,BatchNormalization, Activation\n",
        "from keras.layers import MaxPool2D, AveragePooling2D, Input, Flatten\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory \n",
        "from keras.regularizers import l2\n",
        "from keras.models import Model\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6Ovy8lLrEs8"
      },
      "source": [
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "(x_train,y_train),(x_test,y_test) = fashion_mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hExBtPsEr_JP"
      },
      "source": [
        "x_train = x_train.reshape(x_train.shape[0],28,28,1)\n",
        "x_test = x_test.reshape(x_test.shape[0],28,28,1)\n",
        "\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1TQoyU19lDw"
      },
      "source": [
        "epochs = 10\n",
        "batch_size = 100\n",
        "data_augemenation = False\n",
        "img_size = 28\n",
        "\n",
        "num_classes = 10\n",
        "num_filters = 64\n",
        "num_blocks = 4\n",
        "num_sub_blocks = 2\n",
        "use_max_pool = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7PiK1bUsbSe"
      },
      "source": [
        "import tensorflow as tf\n",
        "#Converting labels to ont-hot vectors\n",
        "y_train = tf.keras.utils.to_categorical(y_train,num_classes)\n",
        "y_test = tf.keras.utils.to_categorical(y_test,num_classes )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBNbvFK-yMVm"
      },
      "source": [
        "#modeling\n",
        "\n",
        "inputs = Input(shape =(28,28,1))\n",
        "x = Conv2D(num_filters, padding = 'same', kernel_initializer = 'he_normal', kernel_size=7, strides = 2,\n",
        "           kernel_regularizer = l2(1e-4))(inputs)\n",
        "\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation('relu')(x)\n",
        "\n",
        "if use_max_pool:\n",
        "  x = MaxPool2D(pool_size=3, padding = 'same', stride = 2)(x)\n",
        "  num_blocks = 3\n",
        "\n",
        "# CNN Resnet\n",
        "for i in range(num_blocks):\n",
        "  for j in range(num_sub_blocks):\n",
        "    strides = 1\n",
        "    is_first_layer_but_not_first_block=j==0 and i>0\n",
        "    if is_first_layer_but_not_first_block:\n",
        "      strides = 2\n",
        "    # Residual block\n",
        "    y = Conv2D(num_filters,\n",
        "               kernel_size = 3, \n",
        "               padding = 'same', \n",
        "               strides = strides,\n",
        "               kernel_initializer = 'he_normal',\n",
        "              kernel_regularizer = l2(1e-4))(x)\n",
        "    y = BatchNormalization()(y)\n",
        "    y = Activation('relu')(y)\n",
        "    y = Conv2D(num_filters,\n",
        "               kernel_size = 3, \n",
        "               padding = 'same', \n",
        "               kernel_initializer = 'he_normal',\n",
        "               kernel_regularizer= l2(1e-4))(y)\n",
        "    y = BatchNormalization()(y)\n",
        "    if is_first_layer_but_not_first_block:\n",
        "      x =  Conv2D(num_filters,\n",
        "               kernel_size = 3, \n",
        "               padding = 'same', \n",
        "               strides = 2,\n",
        "               kernel_initializer = 'he_normal',\n",
        "               kernel_regularizer= l2(1e-4))(x)\n",
        "    \n",
        "    x = keras.layers.add([x,y])\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "  num_filter = 2 * num_filters\n",
        "\n",
        "#classifier \n",
        "x = AveragePooling2D()(x)\n",
        "y = Flatten()(x)\n",
        "\n",
        "outputs = Dense(num_classes, activation='softmax', kernel_initializer='he_normal')(y)\n",
        "\n",
        "model = Model(inputs = inputs, outputs = outputs)\n",
        "model.compile(loss = 'categorical_crossentropy',\n",
        "              optimizer = Adam(),\n",
        "              metrics = ['acc'])\n",
        "model.summary()\n",
        "\n",
        "      \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dg1dWst08rr_"
      },
      "source": [
        "import os\n",
        "save_dir = os.path.join(os.getcwd(),'save_model')\n",
        "model_name = 'fmnist_resnt_model.h5'\n",
        "if not os.path.join(save_dir):\n",
        "  os.makedirs(save_dir)\n",
        "filepath = os.path.join(save_dir, model_name)\n",
        "print(filepath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zsbp3c6G-kJZ"
      },
      "source": [
        "checkpoint = ModelCheckpoint(filepath=filepath,\n",
        "                             verbose = 1,\n",
        "                             save_best_only = True)\n",
        "callbacks = [checkpoint]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoIoFEod-x8w"
      },
      "source": [
        "model.fit(x_train,y_train, \n",
        "          batch_size = batch_size,\n",
        "          epochs = epochs,\n",
        "          validation_data = (x_test,y_test),\n",
        "          shuffle = True,\n",
        "          callbacks = callbacks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSZ9geQSG2w8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLgMHxYdc6tD"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFlKQWmzDQWR"
      },
      "source": [
        "# Cats and dogs kaggle dataset(https://www.kaggle.com/c/dogs-vs-cats)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LphjSIF_DSH"
      },
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp /content/drive/MyDrive/kaggle.json ~/.kaggle/\n",
        "# Permission Warning 이 일어나지 않도록 \n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "# 본인이 참가한 모든 대회 보기 \n",
        "!kaggle competitions list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bngFa84De4d"
      },
      "source": [
        "!kaggle competitions download -c dogs-vs-cats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6ZTwUoIDtqk"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import zipfile\n",
        "import glob\n",
        "import os\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf  \n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.image  import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "from keras.applications.resnet50 import preprocess_input"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2iM08msFj1p"
      },
      "source": [
        "1. Extracting Zip file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9pxW-IOE-8A"
      },
      "source": [
        "zip_file = glob.glob('/content/*.zip')\n",
        "print(zip_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_eWY3V1TFuAw"
      },
      "source": [
        "def extract_zip(file):\n",
        "  with zipfile.ZipFile(file,'r') as zip_ref:\n",
        "    zip_ref.extractall('temp')\n",
        "\n",
        "#extract train and test\n",
        "for files in zip_file:\n",
        "  extract_zip(files)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLAvMfWFF_vn"
      },
      "source": [
        "batch_size = 16\n",
        "img_size = 224\n",
        "epochs = 30"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUBB9T5rGfVA"
      },
      "source": [
        "print(len(os.listdir('/content/temp/train')), 'training data') \n",
        "print(len(os.listdir('/content/temp/test1')), 'test data')\n",
        "os.listdir('/content/temp/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-B6wNaiHQaK"
      },
      "source": [
        "1-2. Data Formatting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MZ2dvAbHClI"
      },
      "source": [
        "def gen_label(directory):\n",
        "  label = []\n",
        "  for file in os.listdir(directory):\n",
        "    if (file.split('.')[0] == 'dog'):\n",
        "      label.append(str(1))\n",
        "    elif (file.split('.')[0] =='cat'):\n",
        "      label.append(str(0))\n",
        "  return label\n",
        "def gen_path(directory):\n",
        "  path = []\n",
        "  for files in os.listdir(directory):\n",
        "    path.append(files)\n",
        "  return path\n",
        "\n",
        "y_train = gen_label('/content/temp/train')\n",
        "X_train = gen_path('/content/temp/train')\n",
        "X_test = gen_path('/content/temp/test1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2ctn_nqIDnQ"
      },
      "source": [
        "df = pd.DataFrame({'filename' : X_train,'category':y_train})\n",
        "\n",
        "print(df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rliLsZHLI27H"
      },
      "source": [
        "sns.countplot(x = 'category', data = df).set_title('Data Distribution')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqOoKkaSJl0s"
      },
      "source": [
        "os.chdir('/content/temp/train')\n",
        "\n",
        "img = load_img(df['filename'].iloc[600])\n",
        "\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.imshow(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeHnBbg0J6Zc"
      },
      "source": [
        "df_train,df_valid = train_test_split(df, test_size = 0.25)\n",
        "print(df_train.shape)\n",
        "print(df_valid.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiXx_ixyLBSU"
      },
      "source": [
        "def generate_train_batch(model):\n",
        "  if model == 'resnet':\n",
        "    print('resnet data')  # Specific preprossing method\n",
        "    train_datagen = ImageDataGenerator(\n",
        "        rotation_range = 10,\n",
        "        zoom_range = 0.1,\n",
        "        horizontal_flip = True,\n",
        "        fill_mode = 'nearest',\n",
        "        width_shift_range = 0.1,\n",
        "        height_shift_range = 0.1,\n",
        "        preprocess_function = preprocess_input) #preprocess_input : 단일 이미지를 로드하면 하나의 이미지 모양인(size1,size2,channel)를 얻게됨.\n",
        "        # 이미지 배치를 생성하려면 추가 자원이 필요함-> (sample,size1,size2, channels) / preprcess_input 함수는 모델에 필요한 형식에\n",
        "        # 이미지를 적절하게 맞추기 위한 것.\n",
        "  else :  # standard Augmenatation.\n",
        "    train_datagen = ImageDataGenerator(\n",
        "        rotation_range = 10,\n",
        "        rescale = 1./255,\n",
        "        zoom_range = 0.1,\n",
        "        horizontal_flip = True,\n",
        "        fill_mode = 'nearest',\n",
        "        width_shift_range = 0.1,\n",
        "        height_shift_range = 0.1)\n",
        "  if model =='vgg':\n",
        "    print('vgg data') \n",
        "    train_gen = train_datagen.flow_from_dataframe(\n",
        "        df_train[['filename']],\n",
        "        x_col = 'filename',\n",
        "        y_col = None,\n",
        "        target_size = (img_size, img_size), # 224x224\n",
        "        class_mode = None,\n",
        "        shuffle = False\n",
        "    )\n",
        "  else : \n",
        "    train_gen = train_datagen.flow_from_dataframe(\n",
        "        df_train,\n",
        "        x_col = 'filename',\n",
        "        y_col = 'category',\n",
        "        target_size = (img_size, img_size), # 224x224\n",
        "        class_mode = 'binary'\n",
        "    )\n",
        "  return train_gen  \n",
        "\n",
        "def generate_valid_batch(model):\n",
        "  if model =='resnet':\n",
        "    print('resnet validation set')\n",
        "    valid_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "  else:\n",
        "    valid_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "\n",
        "  valid_gen = valid_datagen.flow_from_dataframe(\n",
        "      df_valid,\n",
        "      x_col = 'filename',\n",
        "      y_col = 'category',\n",
        "      target_size = (img_size, img_size),\n",
        "      batch_size = batch_size,\n",
        "      class_mode = 'binary'\n",
        "  )\n",
        "\n",
        "  return valid_gen  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v41OtFr_QwFa"
      },
      "source": [
        "train_gen = generate_train_batch('others')\n",
        "valid_gen = generate_valid_batch('others')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_cL1tMkSBgr"
      },
      "source": [
        "## Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOrh5eLTRxjV"
      },
      "source": [
        "#standard\n",
        "visual_datagen = ImageDataGenerator(\n",
        "    rotation_range = 10,\n",
        "    rescale = 1./255,\n",
        "    zoom_range = 0.1,\n",
        "    horizontal_flip = True,\n",
        "    fill_mode = 'nearest',\n",
        "    width_shift_range = 0.1,\n",
        "    height_shift_range = 0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLkSoaz_SbBg"
      },
      "source": [
        "visualise_df = df_train.sample(n=1).reset_index(drop = True)\n",
        "\n",
        "visualisation_generator = visual_datagen.flow_from_dataframe(\n",
        "    visualise_df,\n",
        "    x_col = 'filename',\n",
        "    y_col = 'category' \n",
        ")\n",
        "plt.figure(figsize = (8,8))\n",
        "for i in range(0,9):\n",
        "  plt.subplot(3,3, i+1)\n",
        "  for X_batch, y_batch in visualisation_generator:\n",
        "    image = X_batch[0]\n",
        "    plt.imshow(image)\n",
        "    break\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSY02eeOGhO2"
      },
      "source": [
        "# CNN Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbLqqgcjTFA8"
      },
      "source": [
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from keras.layers import *\n",
        "from keras.models import Model, Sequential\n",
        "from keras import optimizers,regularizers\n",
        "from keras import backend as K\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2yjfJndG-qk"
      },
      "source": [
        "modelcnn = Sequential()\n",
        "modelcnn.add(Conv2D(16,(3,3), activation= 'relu', input_shape = (img_size,img_size,3)))\n",
        "modelcnn.add(Conv2D(16,(3,3),activation = 'relu'))\n",
        "modelcnn.add(MaxPool2D(2,2))\n",
        "\n",
        "\n",
        "modelcnn.add(Conv2D(32,(3,3), activation= 'relu'))\n",
        "modelcnn.add(Conv2D(32,(3,3),activation = 'relu'))\n",
        "modelcnn.add(MaxPool2D(2,2))\n",
        "\n",
        "\n",
        "modelcnn.add(Conv2D(64,(3,3), activation= 'relu'))\n",
        "modelcnn.add(Conv2D(64,(3,3),activation = 'relu'))\n",
        "modelcnn.add(MaxPool2D(2,2))\n",
        "modelcnn.add(Dropout(0.2))\n",
        "\n",
        "modelcnn.add(Conv2D(64,(3,3), activation= 'relu'))\n",
        "modelcnn.add(MaxPool2D(2,2))\n",
        "\n",
        "modelcnn.add(Flatten())\n",
        "modelcnn.add(Dense(512, activation='relu'))\n",
        "modelcnn.add(Dropout(0.5))\n",
        "modelcnn.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "\n",
        "modelcnn.compile(loss = tf.keras.losses.binary_crossentropy,\n",
        "                 optimizer = optimizers.Adam(),\n",
        "                 metrics = ['acc'])\n",
        "modelcnn.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPA7Z99tJe5_"
      },
      "source": [
        "modelcnn.fit_generator(train_gen,\n",
        "                       epochs = epochs,\n",
        "                       validation_data = valid_gen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4tHpi_aLEi7"
      },
      "source": [
        "# Transfer learning\n",
        "\n",
        "전이 학습-> pretrained model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmIlN9mOKGzO"
      },
      "source": [
        "vgg = VGG16(weights = 'imagenet',\n",
        "            include_top = False,\n",
        "            input_shape = (224,224,3))\n",
        "for layers in vgg.layers:\n",
        "  layers.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqesu7oNLean"
      },
      "source": [
        "h5 file ->pretrain model \n",
        "\n",
        "imagenet ->class 1000개 이미지를 학습한 weight ->.h5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xM2z0XCiLa23"
      },
      "source": [
        "feature_list = []\n",
        "for path in df_train['filename'].to_numpy():\n",
        "  x = load_img(path,target_size = (img_size,img_size))\n",
        "  img_array = img_to_array(x)\n",
        "  img_array = np.expand_dims(img_array,axis =0)\n",
        "  features = vgg.predict(img_array)\n",
        "  feature_list.append(features)\n",
        "\n",
        "feat_lst = np.reshape(feature_list, (-1,7*7*512))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fq2R3qjtO9x2"
      },
      "source": [
        "del feature_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0qrvyGeOjj9"
      },
      "source": [
        "## Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqg_ri0eOids"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "y = df_train['category'].to_numpy()\n",
        "\n",
        "X_train,X_test,y_train,y_test = train_test_split(feat_lst, y, test_size = 0.2, random_state = 42)\n",
        "\n",
        "glm = LogisticRegression(C=0.1)\n",
        "glm.fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otvcLzzxMTxd"
      },
      "source": [
        "# Fine Turning-> Resnet +fully Connected"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEDbg3wCyXLJ"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N21CsVlpMY_q"
      },
      "source": [
        "np.random.seed(2020)\n",
        "\n",
        "res = ResNet50(weights = 'imagenet',\n",
        "                      include_top = False,\n",
        "                      input_shape = (224,224,3))\n",
        "res_train_gen = generate_train_batch('resnet')\n",
        "res_valid_gen = generate_valid_batch('resnet')\n",
        "\n",
        "\n",
        "for layer in res.layer[:171]:\n",
        "  layer.trainable = False\n",
        "\n",
        "\n",
        "flat = Flatten()(res.output)\n",
        "dense = Dense91024,activation = 'relu')(falt)\n",
        "drop  = Dropout(0.5)(dense)\n",
        "classifier = Dense(1, activation = 'sigmoid')(drop)\n",
        "\n",
        "res_model = Model(res.input, classifier)\n",
        "res_model.compile(loss = tf.keras.losses.binary_crossentropy,\n",
        "                 optimizer = optimizers.Adam(),\n",
        "                 metrics = ['acc'])\n",
        "res_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbuCCXCQNRi7"
      },
      "source": [
        "modelcnn.fit_generator(res_train_gen,\n",
        "                       epochs = epochs,\n",
        "                       validation_data = res_valid_gen,\n",
        "                       validation_steps = res_train_gen.samples // batch_size,\n",
        "                       step_per_epoch = res_valid_gen.samples // batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lHxQlRiNf3e"
      },
      "source": [
        "testdf = pd.DataFrame({'filename':X_test})\n",
        "test_sample = testdf.sample(n=12, random_state = 2021)\n",
        "\n",
        "def test_img(model,name):\n",
        "  result_lst = []\n",
        "  for path in test_sample['fliename'].to_numpy():\n",
        "    full_path = '/content/temp/test1'+path\n",
        "    x = load_img(full_path, target_size = (224,224))\n",
        "    img_array = img_to_array(x)\n",
        "    img_array = np.exapan_dims(img_array,axis =0)\n",
        "\n",
        "    if name =='vgg':\n",
        "      features = model.predict(img_array)\n",
        "      features = np.reshape(features, (-1,7*7*512))\n",
        "      result = glm.predict(features)\n",
        "    else:\n",
        "      result = model.predict(img_array)\n",
        "    \n",
        "    result = 'dog' if float(result) > 0.5 else 'cat'\n",
        "\n",
        "    result_lst.append(result)\n",
        "\n",
        "  return result_lst"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBHdo7EYPabt"
      },
      "source": [
        "custom_cnn_result = test_img(modecnn,'cnn')\n",
        "vgg_result = test_img(vgg,'VGG')\n",
        "finetune_result = test_img(res_model, 'resnet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIvzv53StcDg"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsjt5hZbtcBM"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3R6r2FGtb-7"
      },
      "source": [
        "## Neural Style Transfer로 알려져있으며, Leon A. Gatys의 논문 A Neural Algorithm of Artistic Style에 있습니다.\n",
        "\n",
        "Neural style transfer은 콘텐츠 (content) 이미지와 (유명한 작가의 삽화와 같은) 스타일 참조 (style reference) 이미지를 이용하여, 콘텐츠 이미지의 콘텐츠는 유지하되 스타일 참조 이미지의 화풍으로 채색한 것 같은 새로운 이미지를 생성하는 최적화 기술입니다.\n",
        "\n",
        "이 과정은 출력 이미지를 콘텐츠 이미지의 콘텐츠 통계랑(statistic)과 스타일 참조 이미지의 스타일 통계량에 맞춰 최적화시킴으로써 구현됩니다. 통계량은 합성곱 신경망을 이용해 각각의 이미지에서 추출합니다.\n",
        "\n",
        "예시로, 아래에 주어진 강아지의 이미지와 바실리 칸딘스키의 7번 작품을 살펴봅시다:\n",
        "\n",
        "<img src = 'https://storage.googleapis.com/download.tensorflow.org/example_images/YellowLabradorLooking_new.jpg'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBzoAaG1uMFP"
      },
      "source": [
        "<img src = 'https://storage.googleapis.com/download.tensorflow.org/example_images/Vassily_Kandinsky%2C_1913_-_Composition_7.jpg'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "983SQgnaubCE"
      },
      "source": [
        "만약 칸딘스키가 7번 작품의 화풍으로 이 강아지를 그렸다면 어떤 작품이 탄생했을까요? 아마 이런 그림이 아니었을까요?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpiWd3vEuVvM"
      },
      "source": [
        "<img src = 'https://tensorflow.org/tutorials/generative/images/stylized-image.png?hl=ko'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAL73HWyuXvV"
      },
      "source": [
        "#import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6Vq0FAcuVEF"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdmkzABotDj_"
      },
      "source": [
        "import IPython.display as display\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "mpl.rcParams['figure.figsize'] = (12,12)\n",
        "mpl.rcParams['axes.grid'] = False\n",
        "\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "import time\n",
        "import functools"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDX8MntPudZJ"
      },
      "source": [
        "def tensor_to_image(tensor):\n",
        "  tensor = tensor*255\n",
        "  tensor = np.array(tensor, dtype=np.uint8)\n",
        "  if np.ndim(tensor)>3:\n",
        "    assert tensor.shape[0] == 1\n",
        "    tensor = tensor[0]\n",
        "  return PIL.Image.fromarray(tensor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2psbRt9HueRS"
      },
      "source": [
        "content_path = tf.keras.utils.get_file('YellowLabradorLooking_new.jpg', 'https://storage.googleapis.com/download.tensorflow.org/example_images/YellowLabradorLooking_new.jpg')\n",
        "\n",
        "# https://commons.wikimedia.org/wiki/File:Vassily_Kandinsky,_1913_-_Composition_7.jpg\n",
        "style_path = tf.keras.utils.get_file('kandinsky5.jpg','https://storage.googleapis.com/download.tensorflow.org/example_images/Vassily_Kandinsky%2C_1913_-_Composition_7.jpg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqTZ9n8ouia7"
      },
      "source": [
        "입력 시각화\n",
        "\n",
        "\n",
        "이미지를 불러오는 함수를 정의하고, 최대 이미지 크기를 512개의 픽셀로 제한합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcSOCPYTugip"
      },
      "source": [
        "def load_img(path_to_img):\n",
        "  max_dim = 512\n",
        "  img = tf.io.read_file(path_to_img)\n",
        "  img = tf.image.decode_image(img, channels=3)\n",
        "  img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "\n",
        "  shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n",
        "  long_dim = max(shape)\n",
        "  scale = max_dim / long_dim\n",
        "\n",
        "  new_shape = tf.cast(shape * scale, tf.int32)\n",
        "\n",
        "  img = tf.image.resize(img, new_shape)\n",
        "  img = img[tf.newaxis, :]\n",
        "  return img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXWUXDR4ulM-"
      },
      "source": [
        "이미지를 출력하기 위한 간단한 함수를 정의합니다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DG7JgPEfukQR"
      },
      "source": [
        "def imshow(image, title=None):\n",
        "  if len(image.shape) > 3:\n",
        "    image = tf.squeeze(image, axis=0)\n",
        "\n",
        "  plt.imshow(image)\n",
        "  if title:\n",
        "    plt.title(title)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQyCA6KNumXp"
      },
      "source": [
        "content_image = load_img(content_path)\n",
        "style_image = load_img(style_path)\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "imshow(content_image, 'Content Image')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "imshow(style_image, 'Style Image')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlLWFXnLuz0V"
      },
      "source": [
        "TF-Hub를 통한 빠른 스타일 전이\n",
        "\n",
        "\n",
        "앞서 언급했듯이, 본 튜토리얼은 이미지 콘텐츠를 특정 스타일에 맞춰 최적화시키는 기존의 스타일 전이 알고리즘을 소개합니다. 이에 대해 살펴보기 전에, 텐서플로 허브 모듈은 어떤 결과물을 생성하는지 시험해봅시다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKGNPNjvunzR"
      },
      "source": [
        "import tensorflow_hub as hub\n",
        "hub_module = hub.load('https://tfhub.dev/google/magenta/arbitrary-image-stylization-v1-256/1')\n",
        "stylized_image = hub_module(tf.constant(content_image), tf.constant(style_image))[0]\n",
        "tensor_to_image(stylized_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77piGs6wu2de"
      },
      "source": [
        "콘텐츠와 스타일 표현 정의하기\n",
        "\n",
        "\n",
        "\n",
        "이미지의 콘텐츠와 스타일 표현(representation)을 얻기 위해, 모델의 몇 가지 중간층들을 살펴볼 것입니다. 모델의 입력층부터 시작해서, 처음 몇 개의 층은 선분이나 질감과 같은 이미지 내의 저차원적 특성에 반응합니다. 반면, 네트워크가 깊어지면 최종 몇 개의 층은 바퀴나 눈과 같은 고차원적 특성들을 나타냅니다. 이번 경우, 우리는 사전학습된 이미지 분류 네트워크인 VGG19 네트워크의 구조를 사용할 것입니다. 이 중간층들은 이미지에서 콘텐츠와 스타일 표현을 정의하는 데 필요합니다. 입력 이미지가 주어졌을때, 스타일 전이 알고리즘은 이 중간층들에서 콘텐츠와 스타일에 해당하는 타깃 표현들을 일치시키려고 시도할 것입니다.\n",
        "\n",
        "VGG19 모델을 불러오고, 작동 여부를 확인하기 위해 이미지에 적용시켜봅시다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pO5s9NoWu1ap"
      },
      "source": [
        "x = tf.keras.applications.vgg19.preprocess_input(content_image*255)\n",
        "x = tf.image.resize(x, (224, 224))\n",
        "vgg = tf.keras.applications.VGG19(include_top=True, weights='imagenet')\n",
        "prediction_probabilities = vgg(x)\n",
        "prediction_probabilities.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbZ_7-ilu4MS"
      },
      "source": [
        "predicted_top_5 = tf.keras.applications.vgg19.decode_predictions(prediction_probabilities.numpy())[0]\n",
        "[(class_name, prob) for (number, class_name, prob) in predicted_top_5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qgzcww1Ou66t"
      },
      "source": [
        "이제 분류층을 제외한 VGG19 모델을 불러오고, 각 층의 이름을 출력해봅니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EY_C5D4Tu5Fx"
      },
      "source": [
        "vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n",
        "\n",
        "print()\n",
        "for layer in vgg.layers:\n",
        "  print(layer.name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kKQFNfQu84F"
      },
      "source": [
        "이미지의 스타일과 콘텐츠를 나타내기 위한 모델의 중간층들을 선택합니다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuQ_hILsu7PS"
      },
      "source": [
        "content_layers = ['block5_conv2'] \n",
        "\n",
        "style_layers = ['block1_conv1',\n",
        "                'block2_conv1',\n",
        "                'block3_conv1', \n",
        "                'block4_conv1', \n",
        "                'block5_conv1']\n",
        "\n",
        "num_content_layers = len(content_layers)\n",
        "num_style_layers = len(style_layers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_wIoglyvA3P"
      },
      "source": [
        "## 스타일과 콘텐츠를 위한 중간층\n",
        "\n",
        "그렇다면 사전훈련된 이미지 분류 네트워크 속에 있는 중간 출력으로 어떻게 스타일과 콘텐츠 표현을 정의할 수 있을까요?\n",
        "\n",
        "고수준에서 보면 (네트워크의 훈련 목적인) 이미지 분류를 수행하기 위해서는 네트워크가 반드시 이미지를 이해햐야 합니다. 이는 미가공 이미지를 입력으로 받아 픽셀값들을 이미지 내에 존재하는 특성(feature)들에 대한 복합적인 이해로 변환할 수 있는 내부 표현(internal representation)을 만드는 작업이 포함됩니다.\n",
        "\n",
        "또한 부분적으로 왜 합성곱(convolutional) 신경망의 일반화(generalize)가 쉽게 가능한지를 나타냅니다. 즉, 합성곱 신경망은 배경잡음(background noise)과 기타잡음(nuisances)에 상관없이 (고양이와 강아지와 같이)클래스 안에 있는 불변성(invariance)과 특징을 포착할 수 있습니다. 따라서 미가공 이미지의 입력과 분류 레이블(label)의 출력 중간 어딘가에서 모델은 복합 특성(complex feature) 추출기의 역할을 수행합니다. 그러므로, 모델의 중간층에 접근함으로써 입력 이미지의 콘텐츠와 스타일을 추출할 수 있습니다.\n",
        "\n",
        "## 모델 만들기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GW_B7KEsvE6N"
      },
      "source": [
        "tf.keras.applications에서 제공하는 모델들은 케라스 함수형 API을 통해 중간층에 쉽게 접근할 수 있도록 구성되어있습니다.\n",
        "\n",
        "함수형 API를 이용해 모델을 정의하기 위해서는 모델의 입력과 출력을 지정합니다:\n",
        "\n",
        "model = Model(inputs, outputs)\n",
        "\n",
        "아래의 함수는 중간층들의 결과물을 배열 형태로 출력하는 VGG19 모델을 반환합니다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8yGx82ou-Qz"
      },
      "source": [
        "def vgg_layers(layer_names):\n",
        "  \"\"\" 중간층의 출력값을 배열로 반환하는 vgg 모델을 만듭니다.\"\"\"\n",
        "  # 이미지넷 데이터셋에 사전학습된 VGG 모델을 불러옵니다\n",
        "  vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n",
        "  vgg.trainable = False\n",
        "\n",
        "  outputs = [vgg.get_layer(name).output for name in layer_names]\n",
        "\n",
        "  model = tf.keras.Model([vgg.input], outputs)\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9Kjd4zJvHTF"
      },
      "source": [
        "위 함수를 이용해 모델을 만들어봅시다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNYmjkMMvGfK"
      },
      "source": [
        "style_extractor = vgg_layers(style_layers)\n",
        "style_outputs = style_extractor(style_image*255)\n",
        "\n",
        "# 각 층의 출력에 대한 통계량을 살펴봅니다\n",
        "for name, output in zip(style_layers, style_outputs):\n",
        "  print(name)\n",
        "  print(\"  크기: \", output.numpy().shape)\n",
        "  print(\"  최솟값: \", output.numpy().min())\n",
        "  print(\"  최댓값: \", output.numpy().max())\n",
        "  print(\"  평균: \", output.numpy().mean())\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ak7zkk-vvbiz"
      },
      "source": [
        "##스타일 계산하기\n",
        "\n",
        "\n",
        "이미지의 콘텐츠는 중간층들의 특성 맵(feature map)의 값들로 표현됩니다.\n",
        "\n",
        "이미지의 스타일은 각 특성 맵의 평균과 피쳐맵들 사이의 상관관계로 설명할 수 있습니다. 이런 정보를 담고 있는 그람 행렬(Gram matrix)은 각 위치에서 특성 벡터(feature vector)끼리의 외적을 구한 후,평균값을 냄으로써 구할 수 있습니다. 주어진 층에 대한 그람 행렬은 다음과 같이 계산할 수 있습니다:\n",
        "\n",
        "![image.png](data:image/png;base64,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)\n",
        "\n",
        "이 식은 tf.linalg.einsum 함수를 통해 쉽게 계산할 수 있습니다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efF-ipdtvIUa"
      },
      "source": [
        "def gram_matrix(input_tensor):\n",
        "  result = tf.linalg.einsum('bijc,bijd->bcd', input_tensor, input_tensor)\n",
        "  input_shape = tf.shape(input_tensor)\n",
        "  num_locations = tf.cast(input_shape[1]*input_shape[2], tf.float32)\n",
        "  return result/(num_locations)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-iF7zdmvtxt"
      },
      "source": [
        "## 스타일과 콘텐츠 추출하기\n",
        "\n",
        "스타일과 콘텐츠 텐서를 반환하는 모델을 만듭시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asU8VAPxvsrJ"
      },
      "source": [
        "class StyleContentModel(tf.keras.models.Model):\n",
        "  def __init__(self, style_layers, content_layers):\n",
        "    super(StyleContentModel, self).__init__()\n",
        "    self.vgg =  vgg_layers(style_layers + content_layers)\n",
        "    self.style_layers = style_layers\n",
        "    self.content_layers = content_layers\n",
        "    self.num_style_layers = len(style_layers)\n",
        "    self.vgg.trainable = False\n",
        "\n",
        "  def call(self, inputs):\n",
        "    \"[0,1] 사이의 실수 값을 입력으로 받습니다\"\n",
        "    inputs = inputs*255.0\n",
        "    preprocessed_input = tf.keras.applications.vgg19.preprocess_input(inputs)\n",
        "    outputs = self.vgg(preprocessed_input)\n",
        "    style_outputs, content_outputs = (outputs[:self.num_style_layers], \n",
        "                                      outputs[self.num_style_layers:])\n",
        "\n",
        "    style_outputs = [gram_matrix(style_output)\n",
        "                     for style_output in style_outputs]\n",
        "\n",
        "    content_dict = {content_name:value \n",
        "                    for content_name, value \n",
        "                    in zip(self.content_layers, content_outputs)}\n",
        "\n",
        "    style_dict = {style_name:value\n",
        "                  for style_name, value\n",
        "                  in zip(self.style_layers, style_outputs)}\n",
        "\n",
        "    return {'content':content_dict, 'style':style_dict}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VVb0W_9vxac"
      },
      "source": [
        "이미지가 입력으로 주어졌을때, 이 모델은 style_layers의 스타일과 content_layers의 콘텐츠에 대한 그람 행렬을 출력합니다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRc9M8G0vv0h"
      },
      "source": [
        "extractor = StyleContentModel(style_layers, content_layers)\n",
        "\n",
        "results = extractor(tf.constant(content_image))\n",
        "\n",
        "print('스타일:')\n",
        "for name, output in sorted(results['style'].items()):\n",
        "  print(\"  \", name)\n",
        "  print(\"    크기: \", output.numpy().shape)\n",
        "  print(\"    최솟값: \", output.numpy().min())\n",
        "  print(\"    최댓값: \", output.numpy().max())\n",
        "  print(\"    평균: \", output.numpy().mean())\n",
        "  print()\n",
        "\n",
        "print(\"콘텐츠:\")\n",
        "for name, output in sorted(results['content'].items()):\n",
        "  print(\"  \", name)\n",
        "  print(\"    크기: \", output.numpy().shape)\n",
        "  print(\"    최솟값: \", output.numpy().min())\n",
        "  print(\"    최댓값: \", output.numpy().max())\n",
        "  print(\"    평균: \", output.numpy().mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jshQY0aPwAv9"
      },
      "source": [
        "최적화를 진행하기 위해, 전체 오차를 콘텐츠와 스타일 오차의 가중합으로 정의합니다:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPcrMM0-v1ps"
      },
      "source": [
        "## 경사하강법 실행\n",
        "\n",
        "이제 스타일과 콘텐츠 추출기를 사용해 스타일 전이 알고리즘을 구현할 차례입니다. 타깃에 대한 입력 이미지의 평균 제곱 오차를 계산한 후, 오차값들의 가중합을 구합니다.\n",
        "\n",
        "스타일과 콘텐츠의 타깃값을 지정합니다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kDz5KYDvzXp"
      },
      "source": [
        "style_targets = extractor(style_image)['style']\n",
        "content_targets = extractor(content_image)['content']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JfK4FwAv4zX"
      },
      "source": [
        "최적화시킬 이미지를 담을 tf.Variable을 정의하고 콘텐츠 이미지로 초기화합니다. (이때 tf.Variable는 콘텐츠 이미지와 크기가 같아야 합니다.):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76_MnWxUv4Ap"
      },
      "source": [
        "image = tf.Variable(content_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tiz09ETv6wt"
      },
      "source": [
        "픽셀 값이 실수이므로 0과 1 사이로 클리핑하는 함수를 정의합니다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7NhoQ8cv53B"
      },
      "source": [
        "def clip_0_1(image):\n",
        "  return tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7GGzLvNv-j_"
      },
      "source": [
        "옵티마이저를 생성합니다. 참조 연구에서는 LBFGS를 추천하지만, Adam도 충분히 적합합니다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_d9qlxcXv83R"
      },
      "source": [
        "opt = tf.optimizers.Adam(learning_rate=0.02, beta_1=0.99, epsilon=1e-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIQyI2v3wB_U"
      },
      "source": [
        "최적화를 진행하기 위해, 전체 오차를 콘텐츠와 스타일 오차의 가중합으로 정의합니다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPZPzeyzv_lR"
      },
      "source": [
        "style_weight=1e-2\n",
        "content_weight=1e4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lj80B9CawDK5"
      },
      "source": [
        "def style_content_loss(outputs):\n",
        "    style_outputs = outputs['style']\n",
        "    content_outputs = outputs['content']\n",
        "    style_loss = tf.add_n([tf.reduce_mean((style_outputs[name]-style_targets[name])**2) \n",
        "                           for name in style_outputs.keys()])\n",
        "    style_loss *= style_weight / num_style_layers\n",
        "\n",
        "    content_loss = tf.add_n([tf.reduce_mean((content_outputs[name]-content_targets[name])**2) \n",
        "                             for name in content_outputs.keys()])\n",
        "    content_loss *= content_weight / num_content_layers\n",
        "    loss = style_loss + content_loss\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Og50s65wbVP"
      },
      "source": [
        "tf.GradientTape를 사용해 이미지를 업데이트합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNpZvBI5wETB"
      },
      "source": [
        "@tf.function() \n",
        "def train_step(image):\n",
        "  with tf.GradientTape() as tape:\n",
        "    outputs = extractor(image)\n",
        "    loss = style_content_loss(outputs)\n",
        "\n",
        "  grad = tape.gradient(loss, image)\n",
        "  opt.apply_gradients([(grad, image)])\n",
        "  image.assign(clip_0_1(image))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6xjNBAVwdO9"
      },
      "source": [
        "구현한 알고리즘을 시험해보기 위해 몇 단계를 돌려봅시다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qNQFAQFwcRB"
      },
      "source": [
        "train_step(image)\n",
        "train_step(image)\n",
        "train_step(image)\n",
        "tensor_to_image(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYzLpt0iwfDh"
      },
      "source": [
        "잘 작동하는 것을 확인했으니, 더 오랫동안 최적화를 진행해봅니다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUzbTKuyweJp"
      },
      "source": [
        "import time\n",
        "start = time.time()\n",
        "\n",
        "epochs = 10\n",
        "steps_per_epoch = 100\n",
        "\n",
        "step = 0\n",
        "for n in range(epochs):\n",
        "  for m in range(steps_per_epoch):\n",
        "    step += 1\n",
        "    train_step(image)\n",
        "    print(\".\", end='')\n",
        "  display.clear_output(wait=True)\n",
        "  display.display(tensor_to_image(image))\n",
        "  print(\"훈련 스텝: {}\".format(step))\n",
        "\n",
        "end = time.time()\n",
        "print(\"전체 소요 시간: {:.1f}\".format(end-start))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFjeQsxSwj5V"
      },
      "source": [
        "## 총 변위 손실\n",
        "\n",
        "\n",
        "이 기본 구현 방식의 한 가지 단점은 많은 고주파 아티팩(high frequency artifact)가 생겨난다는 점 입니다. 아티팩 생성을 줄이기 위해서는 이미지의 고주파 구성 요소에 대한 레귤러리제이션(regularization) 항을 추가해야 합니다. 스타일 전이에서는 이 변형된 오차값을 총 변위 손실(total variation loss)라고 합니다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLosc66iwgOh"
      },
      "source": [
        "def high_pass_x_y(image):\n",
        "  x_var = image[:,:,1:,:] - image[:,:,:-1,:]\n",
        "  y_var = image[:,1:,:,:] - image[:,:-1,:,:]\n",
        "\n",
        "  return x_var, y_var"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymRjiOjawm5R"
      },
      "source": [
        "x_deltas, y_deltas = high_pass_x_y(content_image)\n",
        "\n",
        "plt.figure(figsize=(14,10))\n",
        "plt.subplot(2,2,1)\n",
        "imshow(clip_0_1(2*y_deltas+0.5), \"Horizontal Deltas: Original\")\n",
        "\n",
        "plt.subplot(2,2,2)\n",
        "imshow(clip_0_1(2*x_deltas+0.5), \"Vertical Deltas: Original\")\n",
        "\n",
        "x_deltas, y_deltas = high_pass_x_y(image)\n",
        "\n",
        "plt.subplot(2,2,3)\n",
        "imshow(clip_0_1(2*y_deltas+0.5), \"Horizontal Deltas: Styled\")\n",
        "\n",
        "plt.subplot(2,2,4)\n",
        "imshow(clip_0_1(2*x_deltas+0.5), \"Vertical Deltas: Styled\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCTGSSShwpC-"
      },
      "source": [
        "위 이미지들은 고주파 구성 요소가 늘어났다는 것을 보여줍니다.\n",
        "\n",
        "한 가지 흥미로운 사실은 고주파 구성 요소가 경계선 탐지기의 일종이라는 점입니다. 이를테면 소벨 경계선 탐지기(Sobel edge detector)를 사용하면 유사한 출력을 얻을 수 있습니다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNkNCGhswn0p"
      },
      "source": [
        "plt.figure(figsize=(14,10))\n",
        "\n",
        "sobel = tf.image.sobel_edges(content_image)\n",
        "plt.subplot(1,2,1)\n",
        "imshow(clip_0_1(sobel[...,0]/4+0.5), \"Horizontal Sobel-edges\")\n",
        "plt.subplot(1,2,2)\n",
        "imshow(clip_0_1(sobel[...,1]/4+0.5), \"Vertical Sobel-edges\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlWBqXvuwrWd"
      },
      "source": [
        "정규화 오차는 각 값의 절대값의 합으로 표현됩니다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UakxwIYwqNB"
      },
      "source": [
        "def total_variation_loss(image):\n",
        "  x_deltas, y_deltas = high_pass_x_y(image)\n",
        "  return tf.reduce_sum(tf.abs(x_deltas)) + tf.reduce_sum(tf.abs(y_deltas))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BjDpbEMwsg4"
      },
      "source": [
        "total_variation_loss(image).numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-M9hDGcDwuel"
      },
      "source": [
        "식이 잘 계산된다는 것을 확인할 수 있습니다. 하지만 다행히도 텐서플로에는 이미 표준 함수가 내장되어 있기 직접 오차식을 구현할 필요는 없습니다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ku_9_B14wtbY"
      },
      "source": [
        "tf.image.total_variation(image).numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63WwAkkMwwmt"
      },
      "source": [
        "다시 최적화하기\n",
        "total_variation_loss를 위한 가중치를 정의합니다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MY0_-BRzwvdJ"
      },
      "source": [
        "total_variation_weight=30"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLNpp1jFwx9-"
      },
      "source": [
        "이제 이 가중치를 train_step 함수에서 사용합니다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnvFwiuGwxbf"
      },
      "source": [
        "@tf.function()\n",
        "def train_step(image):\n",
        "  with tf.GradientTape() as tape:\n",
        "    outputs = extractor(image)\n",
        "    loss = style_content_loss(outputs)\n",
        "    loss += total_variation_weight*tf.image.total_variation(image)\n",
        "\n",
        "  grad = tape.gradient(loss, image)\n",
        "  opt.apply_gradients([(grad, image)])\n",
        "  image.assign(clip_0_1(image))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvymn1ijw0cO"
      },
      "source": [
        "최적화할 변수를 다시 초기화합니다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZKMu5p3wz0s"
      },
      "source": [
        "image = tf.Variable(content_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1y6WUPZvw18V"
      },
      "source": [
        "최적화를 수행합니다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Dk_n4Qnw1Yx"
      },
      "source": [
        "import time\n",
        "start = time.time()\n",
        "\n",
        "epochs = 10\n",
        "steps_per_epoch = 100\n",
        "\n",
        "step = 0\n",
        "for n in range(epochs):\n",
        "  for m in range(steps_per_epoch):\n",
        "    step += 1\n",
        "    train_step(image)\n",
        "    print(\".\", end='')\n",
        "  display.clear_output(wait=True)\n",
        "  display.display(tensor_to_image(image))\n",
        "  print(\"훈련 스텝: {}\".format(step))\n",
        "\n",
        "end = time.time()\n",
        "print(\"전체 소요 시간: {:.1f}\".format(end-start))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cliCUxjYw49Q"
      },
      "source": [
        "file_name = 'stylized-image.png'\n",
        "tensor_to_image(image).save(file_name)\n",
        "\n",
        "try:\n",
        "  from google.colab import files\n",
        "except ImportError:\n",
        "   pass\n",
        "else:\n",
        "  files.download(file_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzfszG2aFQ1O"
      },
      "source": [
        "## Neural Transfer 다른 예제"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHmybRFkFQAn"
      },
      "source": [
        "import functools\n",
        "import os\n",
        "\n",
        "from matplotlib import gridspec\n",
        "import matplotlib.pylab as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3xBgtAWFTcg"
      },
      "source": [
        "print(\"TF Version: \", tf.__version__)\n",
        "print(\"TF-Hub version: \", hub.__version__)\n",
        "print(\"Eager mode enabled: \", tf.executing_eagerly())\n",
        "print(\"GPU available: \", tf.test.is_gpu_available())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqPSCoI-FTe4"
      },
      "source": [
        "def crop_center(image):\n",
        "  \"\"\"Returns a cropped square image.\"\"\"\n",
        "  shape = image.shape\n",
        "  new_shape = min(shape[1], shape[2])\n",
        "  offset_y = max(shape[1] - shape[2], 0) // 2\n",
        "  offset_x = max(shape[2] - shape[1], 0) // 2\n",
        "  image = tf.image.crop_to_bounding_box(\n",
        "      image, offset_y, offset_x, new_shape, new_shape)\n",
        "  return image\n",
        "\n",
        "@functools.lru_cache(maxsize=None)\n",
        "def load_image(image_url, image_size=(256, 256), preserve_aspect_ratio=True):\n",
        "  \"\"\"Loads and preprocesses images.\"\"\"\n",
        "  # Cache image file locally.\n",
        "  image_path = tf.keras.utils.get_file(os.path.basename(image_url)[-128:], image_url)\n",
        "  # Load and convert to float32 numpy array, add batch dimension, and normalize to range [0, 1].\n",
        "  img = plt.imread(image_path).astype(np.float32)[np.newaxis, ...]\n",
        "  if img.max() > 1.0:\n",
        "    img = img / 255.\n",
        "  if len(img.shape) == 3:\n",
        "    img = tf.stack([img, img, img], axis=-1)\n",
        "  img = crop_center(img)\n",
        "  img = tf.image.resize(img, image_size, preserve_aspect_ratio=True)\n",
        "  return img\n",
        "\n",
        "def show_n(images, titles=('',)):\n",
        "  n = len(images)\n",
        "  image_sizes = [image.shape[1] for image in images]\n",
        "  w = (image_sizes[0] * 6) // 320\n",
        "  plt.figure(figsize=(w  * n, w))\n",
        "  gs = gridspec.GridSpec(1, n, width_ratios=image_sizes)\n",
        "  for i in range(n):\n",
        "    plt.subplot(gs[i])\n",
        "    plt.imshow(images[i][0], aspect='equal')\n",
        "    plt.axis('off')\n",
        "    plt.title(titles[i] if len(titles) > i else '')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5eNykXuFjOH"
      },
      "source": [
        "output_image_size = 384 \n",
        "content_img_size = (output_image_size, output_image_size)\n",
        "style_img_size = (256, 256)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4iVYle3FTg-"
      },
      "source": [
        "content_image_url = 'https://d16yj43vx3i1f6.cloudfront.net/uploads/2019/10/GettyImages-803849852.jpg'\n",
        "style_image_url = 'https://vertexpages.com/wp-content/uploads/2019/10/farm.jpg'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5xiH8ZRFTjI"
      },
      "source": [
        "content_image = load_image(content_image_url, content_img_size)\n",
        "style_image = load_image(style_image_url, style_img_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlwbRVC3Flfu"
      },
      "source": [
        "style_image = tf.nn.avg_pool(style_image, ksize=[3,3], strides=[1,1], padding='SAME')\n",
        "show_n([content_image, style_image], ['Content image', 'Style image'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ga8mNiSzFTlZ"
      },
      "source": [
        "import time\n",
        "start_time = time.time()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BG3HxbKfFnsw"
      },
      "source": [
        "hub_handle = 'https://tfhub.dev/google/magenta/arbitrary-image-stylization-v1-256/2'\n",
        "hub_module = hub.load(hub_handle)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtIE2zWHFnu7"
      },
      "source": [
        "outputs = hub_module(content_image, style_image)\n",
        "stylized_image = outputs[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KR5ZH5pFnw9"
      },
      "source": [
        "# Stylize content image with given style image.\n",
        "# This is pretty fast within a few milliseconds on a GPU.\n",
        "\n",
        "outputs = hub_module(tf.constant(content_image), tf.constant(style_image))\n",
        "stylized_image = outputs[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwmmESGmFqZ2"
      },
      "source": [
        "# Visualize input images and the generated stylized image.\n",
        "\n",
        "show_n([content_image, style_image, stylized_image], titles=['Original content image', 'Style image', 'Stylized image'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwbG01BWFqcE"
      },
      "source": [
        "end_time = time.time()\n",
        "print('Time Taken = ', end_time-start_time)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyXvQ2GTFqeM"
      },
      "source": [
        "content_image_url = 'https://upload.wikimedia.org/wikipedia/commons/thumb/1/1d/Taj_Mahal_%28Edited%29.jpeg/1920px-Taj_Mahal_%28Edited%29.jpeg'\n",
        "style_image_url = 'https://joeburciaga.files.wordpress.com/2013/02/tsunami-2698.jpg'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbm8Ov2WFqgZ"
      },
      "source": [
        "content_image = load_image(content_image_url, content_img_size)\n",
        "style_image = load_image(style_image_url, style_img_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZC5XrhtFt_A"
      },
      "source": [
        "style_image = tf.nn.avg_pool(style_image, ksize=[3,3], strides=[1,1], padding='SAME')\n",
        "show_n([content_image, style_image], ['Content image', 'Style image'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tz4QTxOKFuA9"
      },
      "source": [
        "start_time = time.time()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvC-92XNFx6d"
      },
      "source": [
        "hub_handle = 'https://tfhub.dev/google/magenta/arbitrary-image-stylization-v1-256/2'\n",
        "hub_module = hub.load(hub_handle)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYJWBmFqFx8o"
      },
      "source": [
        "outputs = hub_module(content_image, style_image)\n",
        "stylized_image = outputs[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqWf1nLLFx-d"
      },
      "source": [
        "# Stylize content image with given style image.\n",
        "# This is pretty fast within a few milliseconds on a GPU.\n",
        "\n",
        "outputs = hub_module(tf.constant(content_image), tf.constant(style_image))\n",
        "stylized_image = outputs[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aTQ9zzfFyAn"
      },
      "source": [
        "# Visualize input images and the generated stylized image.\n",
        "\n",
        "show_n([content_image, style_image, stylized_image], titles=['Original content image', 'Style image', 'Stylized image'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YZP54nkFuDR"
      },
      "source": [
        "end_time = time.time()\n",
        "print('Time Taken = ', end_time-start_time)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KcHaONNEuP-"
      },
      "source": [
        "## Generative Modeling\n",
        "\n",
        "생성모델링은 가지고있는 데이터 분포에서 sampling한 것 같은 새로운 데이터를 만드는 것입니다.\n",
        "생성모델링은 판별모델링과 비교하면 이해하기 쉽습니다. 판별모델링은 label y가 필요한 supervised learning이고, 생성모델링은 label y가 필요없는 unsupervised learning입니다. [label y가 있는 경우도 존재]\n",
        "\n",
        "- 판별 모델링 : Sample x가 주어졌을 때, label y의 확률 $P(y|x)$를 추정\n",
        "- 생성 모델링 : Sample x의 P(x)를 추정\n",
        "\n",
        "생성 모델링의 목적 : Want to learn $P_{model}(x)$ similart to $P_{data(x)}$\n",
        "\n",
        "<img src = 'https://user-images.githubusercontent.com/41895063/92600717-e533d280-f2e6-11ea-929c-6de6c090150e.png'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R89Dr5hqJzS_"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "<img src = 'https://user-images.githubusercontent.com/41895063/92600802-00064700-f2e7-11ea-8588-25ccb610f63a.png'>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Generative Adversarial Network(GAN)\n",
        "\n",
        "- 생성자와 식별자가 서로 경쟁(Adversarial)하며 데이터를 생성(Generative)하는 모델(network)을 뜻함.\n",
        "\n",
        "만약, GAN으로 인물 사진을 생성해 낸다면\n",
        "\n",
        "인물 사진을 만들어내는 것을 Generator(생성자)라고 하며\n",
        "\n",
        "만들어진 인물 사진을 평가하는 것을 Discriminator(구분자)라고 합니다.\n",
        "\n",
        "생성자와 구분자가 서로 대립하며(Adversarial:대립하는) 서로의 성능을 점차 개선해 나가는 쪽으로 학습이 진행되는 것이 주요 개념.\n",
        "\n",
        "- 머신러닝은 크게 3가지 개념 지도학습/강화학습/비지도학습 으로 분류되는데, GAN은 '비지도 학습'에 해당합니다.\n",
        "\n",
        "<img src = 'https://media.vlpt.us/images/tobigs-gm1/post/b6751877-1293-4be7-b2b1-c31e4d013000/image.png'>\n",
        "\n",
        "\n",
        "출처 : https://velog.io/@tobigs-gm1/basicofgan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMN9iGs8lamc"
      },
      "source": [
        "<img src = 'https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FdsLkY2%2FbtqDx6ibwSp%2FFKgJojS3w2gsh9EP1XYVuK%2Fimg.png'>\n",
        "\n",
        "- Generator(생성자) : 생성된 z를 받아, 실제 데이터와 비슷한 데이터를 만들어내는 학습\n",
        "\n",
        "- Discriminator(구분자) : 실제 데이터와 생성자가 생성한 가짜 데이터를 구별하도록 학습.\n",
        "\n",
        "\n",
        "Generator는 입력 데이터의 분포(distribution)를 알아내도록 학습합니다. 이 분포를 재현하여 원 데이터의 분포와 차이가 없도록 하고 Discriminator는 실데이터인지 가짜 데이터인지 구별해서 각각에 대한 확률을 추정합니다.\n",
        "\n",
        "<img src = 'https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbzJmhS%2FbtqDx5jj8a2%2FaWL1vbWkBBXyun7Ll9lpW0%2Fimg.png'>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVTVPaoVpuK2"
      },
      "source": [
        "만약 Generator가 정확히 입력 데이터의 분포를 표현할 수 있으면 거기서 뽑은 샘플은 실제 데이터와 구별이 불가능 할 것입니다. Discriminator는 현재 데이터의 샘플이 진짜 데이터(입력)인지, 아니면 Generator로부터 만들어진 것인지 구별해서 각각의 경우에 대한 확률을 평가합니다.\n",
        "\n",
        " \n",
        "\n",
        "실제 데이터의 분포에 가까운 데이터를 생성하는 것이 GAN이 가진 궁극적인 목표이며, 생성자(Generator)는 구분자(Discriminator)가 거짓으로 판별하지 못하도록 가짜 데이터를 진짜 데이터와 가깝게 생성하도록 노력합니다. 이 과정을 통해 생성자(Generator)와 구분자(Discriminator)의 성능이 점차 개선되고 궁극적으로는 구분자(Discriminator)가 실제 데이터와 가짜 데이터를 구분하지 못하게 만드는 것이 목표입니다.\n",
        "\n",
        "\n",
        "## GAN의 학습\n",
        "\n",
        "<img src = 'https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbRwvnH%2FbtqDzkNAl8w%2FxRJmkKhyZLCATOxU6nMk2K%2Fimg.png'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_LHJljSrxZp"
      },
      "source": [
        "GAN의 구성요소인 두 모델 Generator와 Discriminator의 학습 진행 방법은 이러합니다.\n",
        "\n",
        "처음 학습이 진행되기 이전에 Real데이터의 확률분포, Generator의 확률분포, Discriminator의 확률분포의 그림이 (a)입니다. Discriminator는 Generator와 기존 확률 분포가 얼마나 다른지 판단합니다. 그리고 Generator는 Real 확률분포에 맞춰 Discriminator를 속이기 위한 쪽으로 생성모델을 수정해 나가고 궁극적으로 Generator의 확률분포가 Real데이터의 확률분포와 차이를 줄여나가는 과정을 가지게 됩니다. (D(x)=0.5파란선)\n",
        "\n",
        "\n",
        "\n",
        "- GAN은 결국 주어진 데이터의 확률 분포를 예측하는 모델이다.\n",
        "\n",
        "여기서 확률 분포간 차이를 계산하기위해 'JSD'를 사용합니다. JSD는 두개의 'KLD'를 통해서 이루어지며 공식은 다음과 같습니다.\n",
        "\n",
        " \n",
        "### KLD(Kullback-Leibler Divergence)\n",
        ": 같은 확률변수 x에 대한 2개의 확률분포 P(x)와 Q(x)가 있을 때, 이 두 분포사이의 차이를 의미\n",
        "\n",
        "<img src = 'https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2F9jVnW%2FbtqDAB89MVQ%2FlkQlw4rIO5BZiAe2A5MJEK%2Fimg.png'>\n",
        "\n",
        "### JSD(Jensen-Shannon Divergence)\n",
        " : : KLD의 문제는 asymmetric하다는 것. KL(P|Q)KL(P|Q)와 KL(Q|P)KL(Q|P)의 값이 서로 다르기 때문에 이를 “거리”라는 척도로 사용하기에는 애매한 부분이 존재하며 JSD는 이러한 문제를 해결할 수 있는 방법.\n",
        "\n",
        "<img src = 'https://www.oreilly.com/library/view/generative-adversarial-networks/9781789136678/assets/ce0afe99-4137-4673-985a-b073ab66c347.png'>\n",
        "\n",
        "\n",
        "참고 : https://hyunw.kim/blog/2017/10/27/KL_divergence.html\n",
        "\n",
        "https://hyeongminlee.github.io/post/prob002_kld_jsd/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUTSSQPgtcuQ"
      },
      "source": [
        "### DCGAN(Deep Convolutional GAN,2015)\n",
        "\n",
        "CNN구조로 판별자 D와 생성자 G를 구성한 GAN입니다. 판별자 D는 이미지(예 28x28x3)를 입력으로 받아 binary classification을 수행하므로 CNN구조를, 생성자 G는 random vector z(예 (100,1))를 입력으로 받아 이미지(28x28x3)을 생성해야므로 deconvolutional network구조를 갖게됩니다.\n",
        "\n",
        "\n",
        "또한 pooling layer를 사용하지 않고 stride 2이상인 convolution,deconvolution을 사용하였습니다.\n",
        "\n",
        "**deconvolution 참고 : https://zzsza.github.io/data/2018/06/25/upsampling-with-transposed-convolution/**\n",
        "\n",
        "\n",
        "<img src='https://media.vlpt.us/images/tobigs-gm1/post/c6e7c755-97c1-41d9-bcbb-df5b2667b422/image.png'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKoQLSFNtkY4"
      },
      "source": [
        "DCGAN의 논문에서, Z space의 연산이 가능하다는 내용을 언급합니다.\n",
        "예를 들어, \"안경 쓴 남자\"에 해당하는 z-vector에서 \"안경을 쓰지 않은 남자\"에 해당하는 z-vector를 빼고, \"안경을 쓰지 않은 여자\"에 해당하는 z-vector를 더한다면 그 z-vector로 생성한 이미지는 \"안경을 쓴 여자\"라는 겁니다. (vector space arithmetic)\n",
        "[하지만, \"안경 쓴 남자\"-\"안경 쓰지 않은 남자\"에 해당하는 z-vector로 이미지를 생성한다고 \"안경\"의 이미지가 나오는 것은 아닙니다. ]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOOBcHz-todC"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fg4ELXF1tqMx"
      },
      "source": [
        "# GIF를 만들기위해 설치합니다.\n",
        "!pip install -q imageio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHbDUS1wtsD6"
      },
      "source": [
        "import glob\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "from tensorflow.keras import layers\n",
        "import time\n",
        "\n",
        "from IPython import display"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awoZvYxetuY2"
      },
      "source": [
        "data loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIcUqlIsttLR"
      },
      "source": [
        "(train_images, train_labels), (_, _) = tf.keras.datasets.mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BZKP7uOtvgh"
      },
      "source": [
        "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\n",
        "train_images = (train_images - 127.5) / 127.5 # 이미지를 [-1, 1]로 정규화합니다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcozsZQztwqp"
      },
      "source": [
        "BUFFER_SIZE = 60000\n",
        "BATCH_SIZE = 256"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-PPq9ULtxii"
      },
      "source": [
        "# 데이터 배치를 만들고 섞습니다.\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NeZN8yAt0WZ"
      },
      "source": [
        "### model 생성\n",
        "\n",
        "####생성자\n",
        "\n",
        "\n",
        "생성자는 시드값 (seed; 랜덤한 잡음)으로부터 이미지를 생성하기 위해, tf.keras.layers.Conv2DTranspose (업샘플링) 층을 이용합니다. 처음 Dense층은 이 시드값을 인풋으로 받습니다. 그 다음 원하는 사이즈 28x28x1의 이미지가 나오도록 업샘플링을 여러번 합니다. tanh를 사용하는 마지막 층을 제외한 나머지 각 층마다 활성함수로 tf.keras.layers.LeakyReLU을 사용하고 있음을 주목합시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2yu00L8tyda"
      },
      "source": [
        "def make_generator_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Dense(7*7*256, use_bias=False, input_shape=(100,)))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Reshape((7, 7, 256)))\n",
        "    assert model.output_shape == (None, 7, 7, 256) # 주목: 배치사이즈로 None이 주어집니다.\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
        "    assert model.output_shape == (None, 7, 7, 128)\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
        "    assert model.output_shape == (None, 14, 14, 64)\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
        "    assert model.output_shape == (None, 28, 28, 1)\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHmehhTct5tQ"
      },
      "source": [
        "(아직 훈련이 되지않은) 생성자를 이용해 이미지를 생성해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpox6MfQt34C"
      },
      "source": [
        "generator = make_generator_model()\n",
        "\n",
        "noise = tf.random.normal([1, 100])\n",
        "generated_image = generator(noise, training=False)\n",
        "\n",
        "plt.imshow(generated_image[0, :, :, 0], cmap='gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWdoFZ4Kt9DQ"
      },
      "source": [
        "#### 감별자\n",
        "\n",
        "\n",
        "감별자는 합성곱 신경망(Convolutional Neural Network, CNN) 기반의 이미지 분류기입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vm1R9POEt6nV"
      },
      "source": [
        "def make_discriminator_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',\n",
        "                                     input_shape=[28, 28, 1]))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(1))\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsPrjIdat_hm"
      },
      "source": [
        "(아직까지 훈련이 되지 않은) 감별자를 사용하여, 생성된 이미지가 진짜인지 가짜인지 판별합니다. 모델은 진짜 이미지에는 양수의 값 (positive values)을, 가짜 이미지에는 음수의 값 (negative values)을 출력하도록 훈련되어집니다.|"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRDcKYXnt_F5"
      },
      "source": [
        "discriminator = make_discriminator_model()\n",
        "decision = discriminator(generated_image)\n",
        "print (decision)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIuBsn-3uHzo"
      },
      "source": [
        "#### 손실함수와 옵티마이저 정의\n",
        "\n",
        "두 모델의 손실함수와 옵티마이저를 정의합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3oS63HduBYZ"
      },
      "source": [
        "# 이 메서드는 크로스 엔트로피 손실함수 (cross entropy loss)를 계산하기 위해 헬퍼 (helper) 함수를 반환합니다.\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rbx9YvSPuKsX"
      },
      "source": [
        "감별자 손실함수\n",
        "\n",
        "\n",
        "이 메서드는 감별자가 가짜 이미지에서 얼마나 진짜 이미지를 잘 판별하는지 수치화합니다. 진짜 이미지에 대한 감별자의 예측과 1로 이루어진 행렬을 비교하고, 가짜 (생성된) 이미지에 대한 감별자의 예측과 0으로 이루어진 행렬을 비교합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yCOZ-PquJyI"
      },
      "source": [
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLUNGiNXuNkh"
      },
      "source": [
        "#### 생성자 손실함수\n",
        "\n",
        "\n",
        "생성자의 손실함수는 감별자를 얼마나 잘 속였는지에 대해 수치화를 합니다. 직관적으로 생성자가 원활히 수행되고 있다면, 감별자는 가짜 이미지를 진짜 (또는 1)로 분류를 할 것입니다. 여기서 우리는 생성된 이미지에 대한 감별자의 결정을 1로 이루어진 행렬과 비교를 할 것입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfbIhM8iuMvx"
      },
      "source": [
        "def generator_loss(fake_output):\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DX1YKCjJuQog"
      },
      "source": [
        "감별자와 생성자는 따로 훈련되기 때문에, 감별자와 생성자의 옵티마이저는 다릅니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdA8Bee5uP8B"
      },
      "source": [
        "generator_optimizer = tf.keras.optimizers.Adam(1e-4) \n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "196KEuebuSAW"
      },
      "source": [
        "#### 체크 포인트 저장"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzbLJU9auRpi"
      },
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=generator,\n",
        "                                 discriminator=discriminator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n75x6QZSuVbx"
      },
      "source": [
        "#### 훈련하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnEVlCLNuUn4"
      },
      "source": [
        "EPOCHS = 50\n",
        "noise_dim = 100\n",
        "num_examples_to_generate = 16\n",
        "\n",
        "# 이 시드를 시간이 지나도 재활용하겠습니다. \n",
        "# (GIF 애니메이션에서 진전 내용을 시각화하는데 쉽기 때문입니다.) \n",
        "seed = tf.random.normal([num_examples_to_generate, noise_dim])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EsLEhH_uZPQ"
      },
      "source": [
        "훈련 루프는 생성자가 입력으로 랜덤시드를 받는 것으로부터 시작됩니다. 그 시드값을 사용하여 이미지를 생성합니다. 감별자를 사용하여 (훈련 세트에서 갖고온) 진짜 이미지와 (생성자가 생성해낸) 가짜이미지를 분류합니다. 각 모델의 손실을 계산하고, 그래디언트 (gradients)를 사용해 생성자와 감별자를 업데이트합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvqHdQRxuYOB"
      },
      "source": [
        "# `tf.function`이 어떻게 사용되는지 주목해 주세요.\n",
        "# 이 데코레이터는 함수를 \"컴파일\"합니다.\n",
        "@tf.function\n",
        "def train_step(images):\n",
        "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "      generated_images = generator(noise, training=True)\n",
        "\n",
        "      real_output = discriminator(images, training=True)\n",
        "      fake_output = discriminator(generated_images, training=True)\n",
        "\n",
        "      gen_loss = generator_loss(fake_output)\n",
        "      disc_loss = discriminator_loss(real_output, fake_output)\n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z377dhwxuaha"
      },
      "source": [
        "def train(dataset, epochs):\n",
        "  for epoch in range(epochs):\n",
        "    start = time.time()\n",
        "\n",
        "    for image_batch in dataset:\n",
        "      train_step(image_batch)\n",
        "\n",
        "    # GIF를 위한 이미지를 바로 생성합니다.\n",
        "    display.clear_output(wait=True)\n",
        "    generate_and_save_images(generator,\n",
        "                             epoch + 1,\n",
        "                             seed)\n",
        "\n",
        "    # 15 에포크가 지날 때마다 모델을 저장합니다.\n",
        "    if (epoch + 1) % 15 == 0:\n",
        "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "    # print (' 에포크 {} 에서 걸린 시간은 {} 초 입니다'.format(epoch +1, time.time()-start))\n",
        "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
        "\n",
        "  # 마지막 에포크가 끝난 후 생성합니다.\n",
        "  display.clear_output(wait=True)\n",
        "  generate_and_save_images(generator,\n",
        "                           epochs,\n",
        "                           seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hujNi45ucMw"
      },
      "source": [
        "#### 이미지 생성 및 저장"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8jIVaucub3J"
      },
      "source": [
        "def generate_and_save_images(model, epoch, test_input):\n",
        "  # `training`이 False로 맞춰진 것을 주목하세요.\n",
        "  # 이렇게 하면 (배치정규화를 포함하여) 모든 층들이 추론 모드로 실행됩니다. \n",
        "  predictions = model(test_input, training=False)\n",
        "\n",
        "  fig = plt.figure(figsize=(4,4))\n",
        "\n",
        "  for i in range(predictions.shape[0]):\n",
        "      plt.subplot(4, 4, i+1)\n",
        "      plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
        "      plt.axis('off')\n",
        "\n",
        "  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0khOodu9ufu5"
      },
      "source": [
        "#### 모델 훈련"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyyDz7LUuhrL"
      },
      "source": [
        "위에 정의된 train() 메서드를 생성자와 감별자를 동시에 훈련하기 위해 호출합니다. 생성적 적대 신경망을 학습하는 것은 매우 까다로울 수 있습니다. 생성자와 감별자가 서로를 제압하지 않는 것이 중요합니다. (예를 들어 학습률이 비슷하면 한쪽이 우세해집니다.) 훈련 초반부에는 생성된 이미지는 랜덤한 노이즈처럼 보입니다. 훈련이 진행될수록, 생성된 숫자는 점차 진짜처럼 보일 것입니다. 약 50 에포크가 지난 후, MNIST 숫자와 닮은 이미지가 생성됩니다. 코랩에서 기본 설정으로 실행하면, 에포크마다 1분정도 소요될 것입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEYBKgQ5ufXh"
      },
      "source": [
        "%%time\n",
        "train(train_dataset, EPOCHS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsKz0m2FuisB"
      },
      "source": [
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VuFgmQfuj9x"
      },
      "source": [
        "# 에포크 숫자를 사용하여 하나의 이미지를 보여줍니다.\n",
        "def display_image(epoch_no):\n",
        "  return PIL.Image.open('image_at_epoch_{:04d}.png'.format(epoch_no))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHGQoQzvuk9B"
      },
      "source": [
        "display_image(EPOCHS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldk8ftuwulqp"
      },
      "source": [
        "anim_file = 'dcgan.gif'\n",
        "\n",
        "with imageio.get_writer(anim_file, mode='I') as writer:\n",
        "  filenames = glob.glob('image*.png')\n",
        "  filenames = sorted(filenames)\n",
        "  last = -1\n",
        "  for i,filename in enumerate(filenames):\n",
        "    frame = 2*(i**0.5)\n",
        "    if round(frame) > round(last):\n",
        "      last = frame\n",
        "    else:\n",
        "      continue\n",
        "    image = imageio.imread(filename)\n",
        "    writer.append_data(image)\n",
        "  image = imageio.imread(filename)\n",
        "  writer.append_data(image)\n",
        "\n",
        "import IPython\n",
        "if IPython.version_info > (6,2,0,''):\n",
        "  display.Image(filename=anim_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kILCpDURupBp"
      },
      "source": [
        "코랩에서 작업하고 있다면, 아래의 코드에서 애니메이션을 다운로드 받을 수 있습니다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HyvkujpRunlJ"
      },
      "source": [
        "# try:\n",
        "#   from google.colab import files\n",
        "# except ImportError:\n",
        "#   pass\n",
        "# else:\n",
        "#   files.download(anim_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioYDfj7s-Tl-"
      },
      "source": [
        "# 발표순서(15:00~)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x59KWX1lAH1P"
      },
      "source": [
        "## 07/23    ['이남경', '다다익선', '배지선', '신재영', '이의광', '김현호', '김지은', '최연제']"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKLMzN1Q_4FX"
      },
      "source": [
        "import random\n",
        "#김무관 : 면접 (월요일 오후 발표 예정)\n",
        "#이승원 : 개인 사정(월요일 오후 발표 예정)\n",
        "members = ['이남경','최연제','다다익선','김현호','김지은','이의광','배지선','신재영']\n",
        "\n",
        "random.shuffle(members)\n",
        "\n",
        "members"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OlQonBd-S-3"
      },
      "source": [
        "발표순서 =  random.sample(members, 8)\n",
        "\n",
        "발표순서"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLq4n4th_aXZ"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}