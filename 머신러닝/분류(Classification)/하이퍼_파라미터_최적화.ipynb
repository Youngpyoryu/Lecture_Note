{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# iris_data를 xgboost로 적용을 해보자."
      ],
      "metadata": {
        "id": "-XgDYosabhoJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "#Loading iris dataset from sklearn\n",
        "iris = load_iris()\n",
        "\n",
        "#independent feautres\n",
        "X = iris.data\n",
        "\n",
        "# target features\n",
        "y = iris.target"
      ],
      "metadata": {
        "id": "APzk3M3Hbklu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import XGboost\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "#Defining XGB Classification model\n",
        "clf = XGBClassifier()"
      ],
      "metadata": {
        "id": "YH_KPRdnbklw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- xgboost의 단점은 복잡한 하이퍼 파라미터가 있다."
      ],
      "metadata": {
        "id": "v7wfVSY2bro4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.Grid SearchCV\n",
        "- 사용자가 하이퍼 파라미터마다 몇가지 값을 가진 리스트를 입력하면, 가능한 하이퍼 파라미터의 경우의 수마다 예측 성능을 측정하여 사용자가 일일이 하이퍼 파라미터를 설정하고, 예측 성능을 비교하여 최적의 파라미터를 찾는 수고를 줄이고 이 과정을 한꺼번에 진행한다."
      ],
      "metadata": {
        "id": "bjRTeDOlburH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing packages from sklearn\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn import model_selection\n",
        "from sklearn import metrics\n",
        "\n",
        "#defining a set of values as a dictionary for hyperparameters\n",
        "\n",
        "param_grid = {\n",
        "    \"n_estimators\":[100,200,300,400],\n",
        "    \"max_depth\":[1,3,5,7],\n",
        "    \"reg_lambda\":[.01,.1,.5]\n",
        "}\n",
        "\n",
        "#declaring GridSearchCV model\n",
        "\n",
        "model = model_selection.GridSearchCV(\n",
        "    estimator = clf,\n",
        "    param_grid = param_grid,\n",
        "    scoring = 'accuracy',\n",
        "    verbose = 10,\n",
        "    n_jobs = 1,\n",
        "    cv = 5\n",
        ")\n",
        "\n",
        "#fitting values to the gridsearchcv model\n",
        "\n",
        "model.fit(X,y)\n",
        "#printing the best possible values to enhance accuracy\n",
        "print(model.best_params_)\n",
        "print(model.best_estimator_)\n",
        "#printing the best score\n",
        "print(model.best_score_)\n",
        "#GridSearchCV model save\n",
        "# import joblib\n",
        "# joblib.dump(model, 'model_xgboost.pkl')\n",
        "# joblib.load('model_xgboost.pkl')"
      ],
      "metadata": {
        "id": "TsJr9ua1bwty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.RandomizedSearchCV\n",
        "\n",
        "- 그리드 서치에서는 grid_param과 같이 매개변수마다 특정 값을 지정해주었습니다. 만약에 변수 범위가 너무 다양하다면 하나하나 작성해주는게 너무 힘들다.\n",
        "\n",
        "- 하이퍼 파라미터 검색 반영이 너무 클때 사용하는 방식이 Randomized Search입니다.\n"
      ],
      "metadata": {
        "id": "5YOrV2urb0yw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#defining a set of values as a dictionary for hyperparameters\n",
        "\n",
        "param_grid = {\n",
        "    \"n_estimators\":[100,200,300,400],\n",
        "    \"max_depth\":[1,3,5,7],\n",
        "    \"reg_lambda\":[.01,.1,.5]\n",
        "}\n",
        "\n",
        "#declaring RandomizedSearchCV model\n",
        "\n",
        "model = model_selection.RandomizedSearchCV(\n",
        "    estimator = clf,\n",
        "    param_distributions = param_grid,\n",
        "    scoring = 'accuracy',\n",
        "    verbose = 10,\n",
        "    n_jobs = 1,\n",
        "    cv = 5,\n",
        "    n_iter=10\n",
        ")\n",
        "\n",
        "#fitting values to the RandomizedSearchCV model\n",
        "\n",
        "model.fit(X,y)\n",
        "\n",
        "#printing the best possible values to enhance accuracy\n",
        "\n",
        "print(model.best_params_)\n",
        "print(model.best_estimator_)\n",
        "#printing the best score\n",
        "print(model.best_score_)"
      ],
      "metadata": {
        "id": "xmuYSlArbwv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.Bayesian optimization\n",
        "\n",
        "- 참고 : https://wooono.tistory.com/102\n",
        "\n",
        "Bayesian Optimization 은 어느 입력값(x)를 받는 미지의 목적 함수 (f(x))를 판단하고 결정하여, 해당 함숫값 (f(x))을 최대로 만드는 최적해를 찾는 것을 목적으로 합니다.\n",
        "\n",
        "즉, 목적 함수(탐색대상함수)와 하이퍼파라미터 쌍(pair)을 대상으로 Surrogate Model(대체 모델) 을 만들고,\n",
        "순차적으로 하이퍼 파라미터를 업데이트해 가면서 평가를 통해 최적의 하이퍼파라미터 조합을 탐색합니다.\n",
        "이 때의 목적 함수를 black-box function 이라고 합니다.\n",
        "Bayesian Optimization 에는 두 가지 필수 요소가 존재합니다.\n",
        "\n",
        "먼저 Surrogate Model 은, 현재까지 조사된 입력값-함숫결과값 점들 $(x_1, f(x_1)),...,(x_t, f(x_t))$ 을 바탕으로, 미지의 목적 함수의 형태에 대한 확률적인 추정을 수행하는 모델을 지칭합니다. 그리고 Acquisition Function 은, 목적 함수에 대한 현재까지의 확률적 추정 결과를 바탕으로, ‘최적 입력값을 찾는 데 있어 가장 유용할 만한’ 다음 입력값 후보를 추천해 주는 함수를 지칭합니다."
      ],
      "metadata": {
        "id": "RaVcCEqfb3dL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fb33tsP%2FbtraMpvxJG0%2FSn7uQK7k910IQ7cP3ZM9vk%2Fimg.png'>"
      ],
      "metadata": {
        "id": "O40L9E6lb5EN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 대략적인 수행 과정\n",
        "\n",
        "    - 위의 파란색 선은, 우리가 찾으려고 하는 목적함수 f(x) 를 나타내고,\n",
        "    - 검정색 점선은, 지금까지 관측한 데이터를 바탕으로 우리가 예측한 estimated function 을 의미합니다.\n",
        "    - 검정색 점선 주변에 있는 파란 영역은, 목적함수 f(x) 가 존재할만한 confidence bound(function의 variance) 를 의미합니다.\n",
        "    - 밑에 있는 EI(x) 는, Acquisition function 을 의미하며, 다음 입력값 후보 추천 시 사용됩니다.\n",
        "    - Acquisition function 값이 컸던 지점을 확인하고, 해당 지점의 hyperparameter 를 다음 입력 값으로 사용합니다.\n",
        "    - hyperparamter 에 따라 estimated function 을 계속 update 하면, estimation function 과 목적 함수 f(x) 가 흡사해집니다.\n",
        "    - 관측한 지점 중 best point 을 argmax f(x) 로 선택합니다.\n",
        "- 자세한 수행 과정\n",
        "\n",
        "    - 입력값, 목적 함수 및 그 외 설정값들을 정의합니다.\n",
        "\n",
        "        - 입력값 x : 여러가지 hyperparameter\n",
        "        - 목적 함수 f(x) : 설정한 입력값을 적용해 학습한, 딥러닝 모델의 성능 결과 수치(e.g. 정확도)\n",
        "        - 입력값 x 의 탐색 대상 구간 : (a,b)\n",
        "        - 입력값-함숫결과값 점들의 갯수 : n\n",
        "        - 조사할 입력값-함숫결과값 점들의 갯수 : N\n",
        "    - 설정한 탐색 대상 구간 (a,b) 내에서 처음 n 개의 입력값들을 랜덤하게 샘플링하여 선택합니다.\n",
        "\n",
        "    - 선택한 n 개의 입력값 x1, x2, ..., xn 을 각각 모델의 hyperparameter 로 설정하여 딥러닝 모델을 학습한 뒤, 학습이 완료된 모델의 성능 결과 수치를 계산합니다.\n",
        "\n",
        "        - 이들을 각각 함숫결과값 f(x1), f(x2), ..., f(xn) 으로 간주합니다.\n",
        "입력값-함숫결과값 점들의 모음 (x1, f(x1)), (x2, f(x2)), ..., (xn, f(xn)) 에 대하여 Surrogate Model 로 확률적 추정을 수행합니다.\n",
        "\n",
        "    - 조사된 입력값-함숫결과값 점들이 총 N 개에 도달할 때까지, 아래의 과정을 반복적으로 수행합니다.\n",
        "\n",
        "        - 기존 입력값-함숫결과값 점들의 모음 (x1, f(x1)),(x2, f(x2)), ..., (xt, f(xt)) 에 대한 Surrogate Model 의 확률적 추정 결과를 바탕으로, 입력값 구간 (a,b) 내에서의 EI 의 값을 계산하고, 그 값이 가장 큰 점을 다음 입력값 후보 x1 로 선정합니다.\n",
        "        - 다음 입력값 후보 x1 를 hyperparameter 로 설정하여 딥러닝 모델을 학습한 뒤, 학습이 완료된 모델의 성능 결과 수치를 계산하고, 이를 f(x1) 값으로 간주합니다.\n",
        "        - 새로운 점 (x2, f(x2)) 을 기존 입력값-함숫결과값 점들의 모음에 추가하고, 갱신된 점들의 모음에 대하여 Surrogate Model 로 확률적 추정을 다시 수행합니다.\n",
        "        - 총 N 개의 입력값-함숫결과값 점들에 대하여 확률적으로 추정된 목적 함수 결과물을 바탕으로, 평균 함수 μ(x) 을 최대로 만드는 최적해를 최종 선택합니다. 추후 해당값을 hyperparameter 로 사용하여 딥러닝 모델을 학습하면, 일반화 성능이 극대화된 모델을 얻을 수 있습니다."
      ],
      "metadata": {
        "id": "FZ_RPKfO0Qq8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def black_box_function(x, y):\n",
        "    \"\"\"Function with unknown internals we wish to maximize.\n",
        "\n",
        "    This is just serving as an example, for all intents and\n",
        "    purposes think of the internals of this function, i.e.: the process\n",
        "    which generates its output values, as unknown.\n",
        "    \"\"\"\n",
        "    return -x ** 2 - (y - 1) ** 2 + 1"
      ],
      "metadata": {
        "id": "nyr_1tlmbwyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bayesian-optimization"
      ],
      "metadata": {
        "id": "tjXwJ9Xmb6-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bayes_opt import BayesianOptimization\n",
        "\n",
        "# Bounded region of parameter space\n",
        "pbounds = {'x': (2, 4), 'y': (-3, 3)}\n",
        "\n",
        "optimizer = BayesianOptimization(\n",
        "    f=black_box_function,\n",
        "    pbounds=pbounds,\n",
        "    random_state=1,\n",
        ")"
      ],
      "metadata": {
        "id": "WDguW3mcb7Ab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- n_iter: How many steps of bayesian optimization you want to perform. The more steps the more likely to find a good maximum you are.\n",
        "\n",
        "- init_points: How many steps of random exploration you want to perform. Random exploration can help by diversifying the exploration space."
      ],
      "metadata": {
        "id": "T5TAgbkzb9iL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer.maximize(\n",
        "    init_points=2,\n",
        "    n_iter=3,\n",
        ")"
      ],
      "metadata": {
        "id": "hUs3G-ccb7Cr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(optimizer.max)"
      ],
      "metadata": {
        "id": "jOnAhNQVb_aV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, res in enumerate(optimizer.res):\n",
        "    print(\"Iteration {}: \\n\\t{}\".format(i, res))"
      ],
      "metadata": {
        "id": "Th9v5FM1cAOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer.set_bounds(new_bounds={\"x\": (-2, 3)})\n",
        "\n",
        "optimizer.maximize(\n",
        "    init_points=0,\n",
        "    n_iter=5,\n",
        ")\n"
      ],
      "metadata": {
        "id": "LBZZaoFNcBMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- xgboost 하이퍼 파라미터 최적화"
      ],
      "metadata": {
        "id": "VnAbSHdbze4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "#Loading iris dataset from sklearn\n",
        "iris = load_iris()\n",
        "\n",
        "#independent feautres\n",
        "X = iris.data\n",
        "\n",
        "# target features\n",
        "y = iris.target"
      ],
      "metadata": {
        "id": "LT41dfDjztml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2, stratify=y)"
      ],
      "metadata": {
        "id": "6_2WiA3szymE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "import xgboost as xgb\n",
        "\n",
        "# MAPE Metric\n",
        "def mean_absolute_percentage_error(y_test, y_pred):\n",
        "    y_test, y_pred = np.array(y_test), np.array(y_pred)\n",
        "    return np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
        "\n",
        "# 탐색 대상 함수 (XGBRegressor)\n",
        "def XGB_cv(max_depth,learning_rate, n_estimators, gamma\n",
        "            ,min_child_weight, subsample\n",
        "            ,colsample_bytree, silent=True, nthread=-1):\n",
        "\n",
        "    # 모델 정의\n",
        "    model = xgb.XGBRegressor(max_depth=int(max_depth),\n",
        "                            learning_rate=learning_rate,\n",
        "                            n_estimators=int(n_estimators),\n",
        "                            gamma=gamma,\n",
        "                            min_child_weight=min_child_weight,\n",
        "                            subsample=subsample,\n",
        "                            colsample_bytree=colsample_bytree,\n",
        "                            nthread=nthread\n",
        "                            )\n",
        "    # 모델 훈련\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # 예측값 출력\n",
        "    y_pred= model.predict(X_test)\n",
        "\n",
        "    # 각종 metric 계산\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    mape = mean_absolute_percentage_error(y_test, y_pred)\n",
        "\n",
        "    # 오차 최적화로 사용할 metric 반환\n",
        "    return r2"
      ],
      "metadata": {
        "id": "Xn6GBBVVzdRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  bayesian-optimization 라이브러리의 BayesianOptimization 클래스 import\n",
        "from bayes_opt import BayesianOptimization\n",
        "import numpy as np\n",
        "\n",
        "# 실험해보고자하는 hyperparameter 집합\n",
        "pbounds = {'max_depth': (3, 7),\n",
        "            'learning_rate': (0.01, 0.2),\n",
        "            'n_estimators': (5000, 10000),\n",
        "            'gamma': (0, 100),\n",
        "            'min_child_weight': (0, 3),\n",
        "            'subsample': (0.5, 1),\n",
        "            'colsample_bytree' :(0.2, 1)\n",
        "            }\n",
        "\n",
        "# Bayesian optimization 객체 생성\n",
        "# f : 탐색 대상 함수, pbounds : hyperparameter 집합\n",
        "# verbose = 2 항상 출력, verbose = 1 최댓값일 때 출력, verbose = 0 출력 안함\n",
        "# random_state : Bayesian Optimization 상의 랜덤성이 존재하는 부분을 통제\n",
        "bo=BayesianOptimization(f=XGB_cv, pbounds=pbounds, verbose=2, random_state=1 )\n",
        "\n",
        "# 메소드를 이용해 최대화 과정 수행\n",
        "# init_points :  초기 Random Search 갯수\n",
        "# n_iter : 반복 횟수 (몇개의 입력값-함숫값 점들을 확인할지! 많을 수록 정확한 값을 얻을 수 있다.)\n",
        "# acq : Acquisition Function들 중 Expected Improvement(EI) 를 사용 -> 사라짐(2024.04.12)\n",
        "# xi : exploration 강도 (기본값은 0.0) -> 사라짐(2024.04.12)\n",
        "bo.maximize(init_points=2, n_iter=10)\n",
        "\n",
        "# ‘iter’는 반복 회차, ‘target’은 목적 함수의 값, 나머지는 입력값을 나타냅니다.\n",
        "# 현재 회차 이전까지 조사된 함숫값들과 비교하여, 현재 회차에 최댓값이 얻어진 경우,\n",
        "# bayesian-optimization 라이브러리는 이를 자동으로 다른 색 글자로 표시하는 것을 확인할 수 있습니다\n",
        "\n",
        "# 찾은 파라미터 값 확인\n",
        "print(bo.max)"
      ],
      "metadata": {
        "id": "qnvOOx3Tzjzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.Hyperopt\n",
        "\n",
        "- HyperOpt는 자동화된 하이퍼파라미터 튜닝 프레임워크로서, fmin()이라는 함수 안에는 3가지의 파라미터가 있다:\n",
        "\n",
        "    - Objective Function: 최소화할 손실 함수\n",
        "    - Domain Space: 탐색 범위. 베이지안 최적화에서는 이 범위가 각  하이퍼파라미터에 대해 통계 분포를 만들어낸다.\n",
        "    - Optimization Algorithm : 최적의 조합을 찾기 위한 알고리즘\n",
        "\n",
        "참고 : https://velog.io/@emseoyk/%ED%95%98%EC%9D%B4%ED%8D%BC%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0-%ED%8A%9C%EB%8B%9D"
      ],
      "metadata": {
        "id": "oNOGx7UrcCXN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-optimize"
      ],
      "metadata": {
        "id": "K-k_fxgjcDeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#importing packages\n",
        "\n",
        "from hyperopt import hp,fmin, tpe, Trials\n",
        "\n",
        "from hyperopt.pyll.base import scope\n",
        "\n",
        "from functools import partial\n",
        "\n",
        "from skopt import space\n",
        "\n",
        "from skopt import gp_minimize\n",
        "\n",
        "#defining a method that will perfrom a 5 split cross validation over\n",
        "\n",
        "#dataset and and will produce the optimum value of the accuracy\n",
        "\n",
        "def optimize(params, x,y):\n",
        "\n",
        "    clf = XGBClassifier(**params)\n",
        "\n",
        "    kf = model_selection.StratifiedKFold(n_splits=5)\n",
        "\n",
        "    accuracies = []\n",
        "\n",
        "    for idx in kf.split(X=x,y=y):\n",
        "\n",
        "        train_idx,test_idx = idx[0],idx[1]\n",
        "\n",
        "        xtrain = x[train_idx]\n",
        "\n",
        "        ytrain = y[train_idx]\n",
        "\n",
        "        xtest = x[test_idx]\n",
        "\n",
        "        ytest = y[test_idx]\n",
        "\n",
        "        clf.fit(xtrain,ytrain)\n",
        "\n",
        "        preds =  clf.predict(xtest)\n",
        "\n",
        "        fold_acc = metrics.accuracy_score(ytest,preds)\n",
        "\n",
        "        accuracies.append(fold_acc)\n",
        "\n",
        "    return -1.0 * np.mean(accuracies)\n",
        "\n",
        "#defining a set of values as hp for hyperparameters\n",
        "\n",
        "param_space = {\n",
        "\n",
        "    \"max_depth\" : scope.int(hp.quniform(\"max_depth\",3,20, 1)) ,\n",
        "\n",
        "    \"min_child_weight\" : scope.int(hp.quniform(\"min_child_weight\",1,8, 1)),\n",
        "\n",
        "    \"n_estimators\": scope.int(hp.quniform(\"n_estimators\",100,1500,1)),\n",
        "\n",
        "    'learning_rate': hp.uniform(\"learning_rate\",0.01,1),\n",
        "\n",
        "    'reg_lambda': hp.uniform(\"reg_lambda\",0.01,1),\n",
        "\n",
        "    'gamma': hp.uniform(\"gamma\",0.01,1),\n",
        "\n",
        "    'subsample': hp.uniform(\"subsample\",0.01,1)\n",
        "\n",
        "    }\n",
        "\n",
        "#defiing optimization_fuction as partial and calling optimize within it\n",
        "\n",
        "optimization_fuction = partial(optimize,x = X, y = y)\n",
        "\n",
        "trials = Trials()\n",
        "\n",
        "#Getting the optimum values for hyperparameters\n",
        "\n",
        "result = fmin(\n",
        "\n",
        "    fn = optimization_fuction,\n",
        "\n",
        "    space = param_space,\n",
        "\n",
        "    algo = tpe.suggest,\n",
        "\n",
        "    max_evals = 15,\n",
        "\n",
        "    trials = trials\n",
        "\n",
        ")\n",
        "\n",
        "#Printing the best hyperparemeter set\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "id": "iM-SA_qFcEQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.Optuna\n",
        "\n",
        "- Optuna는 ML 알고리즘의 하이퍼파라미터 튜닝을 자동화해주는 오픈소스 툴입니다. 유사한 툴로 Hyperopt가 있지만 사용성과 문서화, 시각화 제공 여부 등에서 Optuna의 손을 들어주는 경우가 많음.\n",
        "\n",
        "- 하이퍼파라미터 튜닝에 쓰고 있는 최신 Automl 기법입니다.\n",
        "- 빠르게 튜닝이 가능하다는 장점이 있음.\n",
        "- 하이퍼파라미터 튜닝 방식을 지정할수 있다. -> 직관적인 api인 튜닝된 lightgbm도 제공해줍니다.\n",
        "\n",
        "- 다른 라이브러리들에 비해 직관적인 장점이 있어 코딩하기 용이합니다.\n"
      ],
      "metadata": {
        "id": "gaNtLF72cFZ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 거의 모든 ML/DL 프레임워크에서 사용 가능한 넓은 범용성을 가지고 있다.\n",
        "간단하고 빠르다.\n",
        "- 최신 동향의 다양한 최적화 알고리즘을 갖추고 있다.\n",
        "- 병렬 처리가 가능하다.\n",
        "- 간단한 메소드로 시각화가 가능하다."
      ],
      "metadata": {
        "id": "DgTDs7PVGC37"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Optuna를 이해하기 위해서는 다음의 용어에 익숙해져야 한다.\n",
        "\n",
        "    - Study: 목적 함수에 기반한 최적화\n",
        "    - Trial: 목적함수 시행\n",
        "- 쉽게 말해 study는 최적화를 하는 과정이고, trial은 다양한 조합으로 목적함수를 시행하는 횟수를 뜻한다.\n",
        "- Study의 목적은 여러 번의 trial을 거쳐 최적의 하이퍼파라미터 조합을 찾는 것이라고 할 수 있겠다."
      ],
      "metadata": {
        "id": "wO8cko_zGFGy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "id": "h3rOl_oqcHAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#importing all the required packages\n",
        "import optuna\n",
        "from sklearn.datasets import load_iris\n",
        "import sklearn.linear_model\n",
        "import sklearn.metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#Loading iris dataset from sklearn\n",
        "iris = load_iris()\n",
        "\n",
        "#independent feautres\n",
        "X = iris.data\n",
        "\n",
        "# target features\n",
        "y = iris.target"
      ],
      "metadata": {
        "id": "5fGjE4mrcH8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "print(optuna.__version__)"
      ],
      "metadata": {
        "id": "UQYpoUkQcJDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#importing packages\n",
        "import optuna\n",
        "from functools import partial\n",
        "\n",
        "#defining a method that will perfrom a 5 split cross validation over\n",
        "#dataset and and will produce the optimum value of the accuracy\n",
        "def optimize(trial, x,y):\n",
        "    #parameter set is declare within function\n",
        "    #suggest_uniform : 범위 내의 균일 분포 값을 선택.\n",
        "    #suggest_discrete_uniform : 이산 균등 분포를 값으로 선택.\n",
        "    #suggest_loguniform : 범위 내의 로그 함수 선상의 값을 선택.\n",
        "    reg_lambda = trial.suggest_uniform('reg_lambda',0.01,1)\n",
        "\n",
        "    n_estimators = trial.suggest_int('n_estimators',100,1500)\n",
        "\n",
        "    max_depth = trial.suggest_int('max_depth',3,15)\n",
        "\n",
        "    max_features = trial.suggest_uniform('max_features',0.01,1)\n",
        "\n",
        "    clf = XGBClassifier(\n",
        "\n",
        "    n_estimators= n_estimators,\n",
        "\n",
        "    reg_lambda=reg_lambda,\n",
        "\n",
        "    max_depth=max_depth,\n",
        "\n",
        "    max_features= max_features)\n",
        "\n",
        "    kf = model_selection.StratifiedKFold(n_splits=5)\n",
        "\n",
        "    accuracies = []\n",
        "\n",
        "    for idx in kf.split(X=x,y=y):\n",
        "\n",
        "        train_idx,test_idx = idx[0],idx[1]\n",
        "\n",
        "        xtrain = x[train_idx]\n",
        "\n",
        "        ytrain = y[train_idx]\n",
        "\n",
        "        xtest = x[test_idx]\n",
        "\n",
        "        ytest = y[test_idx]\n",
        "\n",
        "        clf.fit(xtrain,ytrain)\n",
        "\n",
        "        preds =  clf.predict(xtest)\n",
        "\n",
        "        fold_acc = metrics.accuracy_score(ytest,preds)\n",
        "\n",
        "        accuracies.append(fold_acc)\n",
        "\n",
        "    return -1.0 * np.mean(accuracies)\n",
        "\n",
        "#defiing optimization_fuction as partial and calling optimize within it\n",
        "\n",
        "optimization_fuction = partial(optimize,x = X, y = y)\n",
        "\n",
        "study = optuna.create_study(direction='minimize')\n",
        "\n",
        "#Printing the best hyperparemeter set\n",
        "\n",
        "study.optimize(optimization_fuction, n_trials=15)"
      ],
      "metadata": {
        "id": "1H0r9ygVcKDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- mimimize로 찾은 이유 : https://stackoverflow.com/questions/72193393/find-the-value-of-variables-to-maximize-return-of-function-in-python"
      ],
      "metadata": {
        "id": "WD46e556Omz6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing packages\n",
        "import optuna\n",
        "from functools import partial\n",
        "\n",
        "#defining a method that will perfrom a 5 split cross validation over\n",
        "#dataset and and will produce the optimum value of the accuracy\n",
        "def optimize(trial, x,y):\n",
        "    #parameter set is declare within function\n",
        "    #suggest_uniform : 범위 내의 균일 분포 값을 선택.\n",
        "    #suggest_discrete_uniform : 이산 균등 분포를 값으로 선택.\n",
        "    #suggest_loguniform : 범위 내의 로그 함수 선상의 값을 선택.\n",
        "    reg_lambda = trial.suggest_uniform('reg_lambda',0.01,1)\n",
        "\n",
        "    n_estimators = trial.suggest_int('n_estimators',100,1500)\n",
        "\n",
        "    max_depth = trial.suggest_int('max_depth',3,15)\n",
        "\n",
        "    max_features = trial.suggest_uniform('max_features',0.01,1)\n",
        "\n",
        "    clf = XGBClassifier(\n",
        "\n",
        "    n_estimators= n_estimators,\n",
        "\n",
        "    reg_lambda=reg_lambda,\n",
        "\n",
        "    max_depth=max_depth,\n",
        "\n",
        "    max_features= max_features)\n",
        "\n",
        "    kf = model_selection.StratifiedKFold(n_splits=5)\n",
        "\n",
        "    accuracies = []\n",
        "\n",
        "    for idx in kf.split(X=x,y=y):\n",
        "\n",
        "        train_idx,test_idx = idx[0],idx[1]\n",
        "\n",
        "        xtrain = x[train_idx]\n",
        "\n",
        "        ytrain = y[train_idx]\n",
        "\n",
        "        xtest = x[test_idx]\n",
        "\n",
        "        ytest = y[test_idx]\n",
        "\n",
        "        clf.fit(xtrain,ytrain)\n",
        "\n",
        "        preds =  clf.predict(xtest)\n",
        "\n",
        "        fold_acc = metrics.accuracy_score(ytest,preds)\n",
        "\n",
        "        accuracies.append(fold_acc)\n",
        "\n",
        "    return np.mean(accuracies)\n",
        "\n",
        "#defiing optimization_fuction as partial and calling optimize within it\n",
        "\n",
        "optimization_fuction = partial(optimize,x = X, y = y)\n",
        "\n",
        "study = optuna.create_study(direction='maximize')\n",
        "\n",
        "#Printing the best hyperparemeter set\n",
        "\n",
        "study.optimize(optimization_fuction, n_trials=15)"
      ],
      "metadata": {
        "id": "phw9SbcQJmD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create an objective function"
      ],
      "metadata": {
        "id": "9zw7Dt_UcOW_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## In optuna, A Trial represents a single call of the objective function\n",
        "## Study shows an optimization session which contains a set of trials\n",
        "## Study: optimization based on an objective function\n",
        "## Trial: a single execution of the objective function\n",
        "\n",
        "## In this demo, \"alpha\" is the hyperparameter which is need to be optimized\n",
        "def objective(trial):\n",
        "\n",
        "    # hyperparameter setting, trial.suggest_uniform will suggest uniform hyperparameter\n",
        "    #alpha between the range of 0.0 to 2.0, lowest value of interval is closed and\n",
        "    #when low=high, it will return low value\n",
        "    alpha = trial.suggest_uniform('alpha', 0.0, 2.0)\n",
        "\n",
        "    # data loading and train-test split\n",
        "    X, y = load_iris(return_X_y=True)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=0)\n",
        "\n",
        "    # model training and evaluation\n",
        "    model = sklearn.linear_model.Lasso(alpha=alpha)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_val)\n",
        "    error = sklearn.metrics.mean_squared_error(y_val, y_pred)\n",
        "\n",
        "    # output: evaluation score\n",
        "    return error"
      ],
      "metadata": {
        "id": "Pr8FbrtvcPdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create an study for that ML model and optimize it"
      ],
      "metadata": {
        "id": "J5_c2l7TcRKn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "# In Optuna, we use the study object to manage optimization.\n",
        "# Method :func:`~optuna.create_study` returns a study object.\n",
        "# A study object has useful properties for analyzing the optimization outcome.\n",
        "study = optuna.create_study(direction='minimize') #Set minimize for minimization and maximize for maximization.\n",
        "#To start the optimization, we create a study object and pass the objective function to method\n",
        "study.optimize(objective, n_trials=50)"
      ],
      "metadata": {
        "id": "MDjBVqYkcPfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualized the above hyperparameter optimization study"
      ],
      "metadata": {
        "id": "3vQMiNdmcTqW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing all the plot functions\n",
        "from optuna.visualization import plot_edf\n",
        "from optuna.visualization import plot_optimization_history\n",
        "from optuna.visualization import plot_parallel_coordinate\n",
        "from optuna.visualization import plot_param_importances\n",
        "from optuna.visualization import plot_slice"
      ],
      "metadata": {
        "id": "CFlHgvK9cPh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the optimization history. See :func:`~optuna.visualization.plot_optimization_history` for the details.\n",
        "plot_optimization_history(study)"
      ],
      "metadata": {
        "id": "E3dHCwofcVE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize high-dimensional parameter relationships. See :func:`~optuna.visualization.plot_parallel_coordinate` for the details.\n",
        "plot_parallel_coordinate(study)"
      ],
      "metadata": {
        "id": "oQ6qT2ZLcVHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize individual hyperparameters as slice plot. See :func:`~optuna.visualization.plot_slice` for the details.\n",
        "plot_slice(study)"
      ],
      "metadata": {
        "id": "HNAk09O6cVJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize parameter importances. See :func:`~optuna.visualization.plot_param_importances` for the details.\n",
        "#In this case, we have only one parameter.\n",
        "plot_param_importances(study)"
      ],
      "metadata": {
        "id": "nNK4p89hcY1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize empirical distribution function. See :func:`~optuna.visualization.plot_edf` for the details.\n",
        "plot_edf(study)"
      ],
      "metadata": {
        "id": "4UnJr023cY3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "import sklearn\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# 1. 최소화/최대화할 목적함수 정의\n",
        "def objective(trial):\n",
        "    iris = sklearn.datasets.load_iris()\n",
        "    x, y = iris.data, iris.target\n",
        "\n",
        "# 2. trial object로 하이퍼파라미터 값 추천\n",
        "# 다양한 분류모델을 설정해서 비교할 수 있다.\n",
        "    classifier_name = trial.suggest_categorical('classifier', ['SVC', 'RandomForest'])\n",
        "    #분류 모델이 SVC일 때\n",
        "    if classifier_name == 'SVC':\n",
        "        svc_c = trial.suggest_loguniform('svc_c', 1e-10, 1e10)\n",
        "        classifier_obj = sklearn.svm.SVC(C=svc_c, gamma='auto')\n",
        "\n",
        "    #분류모델이 랜덤포레스트일 때\n",
        "    else:\n",
        "        rf_max_depth = int(trial.suggest_loguniform('rf_max_depth', 2, 32))\n",
        "        classifier_obj = sklearn.ensemble.RandomForestClassifier(max_depth=rf_max_depth, n_estimators=10)\n",
        "\n",
        "    accuracy = cross_val_score(classifier_obj, x, y, cv = 4).mean()\n",
        "    return accuracy\n",
        "\n",
        "# 3. study 오브젝트 생성하고 목적함수 최적화하는 단계\n",
        "# 여기서는 목적함수를 정확도로 설정했기 때문에 최대화를 목표로 하고 있지만, 손실함수의 경우 direction='minimize'로 설정\n",
        "study = optuna.create_study(direction='maximize')\n",
        "# 반복 시행 횟수(trial)는 200번으로\n",
        "study.optimize(objective, n_trials=200)"
      ],
      "metadata": {
        "id": "8PahkZBmcY5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 시행된 trial 중 최적의 하이퍼파라미터 반환하는 메소드\n",
        "print(study.best_trial.params)\n",
        "\n",
        "# 시행된 trial 중 가장 높은 값 반환하는 메소드\n",
        "optuna_acc = study.best_trial.value\n",
        "print(optuna_acc)"
      ],
      "metadata": {
        "id": "8ELZ5u6Bcd-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualization"
      ],
      "metadata": {
        "id": "H2MWEFmgcgfk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optuna.visualization.plot_param_importances(study)\n",
        "\n",
        "optuna.visualization.plot_optimization_history(study)"
      ],
      "metadata": {
        "id": "E84NoqGTcd_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcXuNtGJ8XVh"
      },
      "source": [
        "# 6.Pycaret\n",
        "\n",
        "- AutoML을 하게 해주는 파이썬 라이브러리\n",
        "- scikit-learning 패키지를 기반으로 하고 있으며 Classification, Regression, Clustering, Anomaly Detection 등 다양한 모델을 지원함.\n",
        "\n",
        "- 공식문서에 설명이 매우 잘 되어 있고, 몇 줄의 코드로 쉽게 구현이 가능하기 때문에 유용하게 사용할 수 있음.\n",
        "\n",
        "- Pycaret을 활용하면 여러 모델의 성능 비교 뿐만 아니라 hyperparameter tunning,  여러 모델을 blending한 모델을 만들 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Pycaret 설치\n",
        "!pip install pycaret"
      ],
      "metadata": {
        "id": "Yh8ZZa_khlPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#모델 비교시 catboost가 없다면 다음과 같이 설치합니다.\n",
        "!pip install pycaret[full]=='3.0.0'"
      ],
      "metadata": {
        "id": "zpqIkD3ChlRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check installed version\n",
        "import pycaret\n",
        "pycaret.__version__"
      ],
      "metadata": {
        "id": "2uy410dqCXO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data load / pycaret에서 제공하는 'credit'데이터를 사용함.\n",
        "from pycaret.datasets import get_data\n",
        "dataset = get_data('credit')"
      ],
      "metadata": {
        "id": "J4JAVeY8h9zB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train test 분리\n",
        "train = dataset.sample(frac=0.95, random_state=786)\n",
        "test = dataset.drop(train.index)\n",
        "train.reset_index(inplace=True, drop=True)\n",
        "test.reset_index(inplace=True, drop=True)"
      ],
      "metadata": {
        "id": "Ffliqt1PhlTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 데이터 설정.\n",
        "    - Pycaret을 사용하기 전에 Pycaret에 맞게 데이터를 설정해줘야 함.\n",
        "    - set_up() 함수를 사용하며, 기본적으로 data와 target을 입력해줍니다.\n",
        "    - 입력 후 column에 대한 자료형이 출력되며 enter를 치면 data가 설정됩니다."
      ],
      "metadata": {
        "id": "oJkHjdg0h09e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- set_up(): pycaret을 사용하기 위한 data setting\n",
        "\n",
        "- session_id: random_state와 같은 개념으로 같은 결과가 나올 수 있게 seed를 고정합니다.\n",
        "- data: train 데이터를 입력합니다.\n",
        "- target = target 변수 이름을 입력합니다."
      ],
      "metadata": {
        "id": "4xcqgfWVia87"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Joblib==1.3 #joblib 1.4에서 사라짐.(함수)"
      ],
      "metadata": {
        "id": "oRGKm468Ipsr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pycaret.classification import *\n",
        "\n",
        "exp_clf = setup(data = train, target = 'default', session_id=123)"
      ],
      "metadata": {
        "id": "FcuFHz7phlVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**- 모델 비교**\n",
        "\n",
        "\n",
        "    - 여러 모델을 적합하여 성능을 비교하는 단계"
      ],
      "metadata": {
        "id": "96m0o2VhiVuN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = compare_models()"
      ],
      "metadata": {
        "id": "0LZKeUmYhlYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- compare_models(): 다양한 모델 적합 후 성능 비교\n",
        "    - fold: cross_validation의 fold를 지정 (default = 10)\n",
        "    - sort: 정렬기준 지표 설정\n",
        "    - n_select: 상위 n개의 모델 결과만 출력"
      ],
      "metadata": {
        "id": "RPWsFQxWin27"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 모델 적합\n",
        "    - 하나의 모델의 적합 결과는 보는 방법."
      ],
      "metadata": {
        "id": "x6B9GiT4itVR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf = create_model('rf')"
      ],
      "metadata": {
        "id": "k4Mxe5rRimuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- create_model(): 하나의 모델 적합\n",
        "\n",
        "    - fold: cross_validation의 fold 지정 (default = 10)"
      ],
      "metadata": {
        "id": "_QpYBwdujDvB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**- Tunning**\n",
        "\n",
        "\n",
        "    - tune_model() 함수를 사용해서 모델의 하이퍼파라미터 튜닝을 진행"
      ],
      "metadata": {
        "id": "j5fNHFFxjJ3Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tuned_rf = tune_model(rf)"
      ],
      "metadata": {
        "id": "yrY2kz5iimxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- tuned_rf를 호출하면 튜닝 결과를 확인할 수 있습니다.\n",
        "\n",
        "- tune_model(model): 모델의 하이퍼파라미터 튜닝\n",
        "\n",
        "- optimize: 평가 metric 지정"
      ],
      "metadata": {
        "id": "9dRdCMCfjYXd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Blending**\n",
        "\n",
        "- blend_models() 함수를 사용하면 여러 모델들을 혼합하여 새로운 모델을 생성합니다.\n",
        "모델을 하나씩 생성해서 blend해도 되고 compare_model을 사용하여 생성한 모델을 사용해서 blend할 수 있습니다."
      ],
      "metadata": {
        "id": "_HCTAPHIjzV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 방법 1\n",
        "dt = create_model('dt')\n",
        "rf = create_model('rf')\n",
        "\n",
        "blender_2 = blend_models(estimator_list = [dt, rf])\n"
      ],
      "metadata": {
        "id": "brbvUbqUimzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- blend_models(models): 여러 모델들을 혼합한 새로운 모델을 생성"
      ],
      "metadata": {
        "id": "H1T6A9EBj8M9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**- 예측**\n",
        "\n",
        "- finalize_model() 함수로 모델을 설정하면, cross_validation을 사용하여 적합한 모델을 전체 데이터로 마지막으로 학습을 합니다. 마지막 모델을 설정한 후에 predict_model()을 통해 예측을 합니다."
      ],
      "metadata": {
        "id": "moLhPog0lzUT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_model = finalize_model(blender_2)\n",
        "prediction = predict_model(final_model, data = test)"
      ],
      "metadata": {
        "id": "kN8MvE4Lim1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- finalize_model(): 최종 모델로 설정 후 마지막 학습 진행\n",
        "- predict_model(): 예측 결과를 'Label' 변수에 저장"
      ],
      "metadata": {
        "id": "fAI7i5nFl_gH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loading sample dataset from pycaret dataset module\n",
        "from pycaret.datasets import get_data\n",
        "data = get_data('diabetes')"
      ],
      "metadata": {
        "id": "lkSuCi1FjBC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c00f6a4a"
      },
      "source": [
        "- Setup\n",
        "This function initializes the training environment and creates the transformation pipeline. Setup function must be called before executing any other function in PyCaret. It only has two required parameters i.e. `data` and `target`. All the other parameters are optional."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97f2c6c6"
      },
      "outputs": [],
      "source": [
        "# import pycaret classification and init setup\n",
        "from pycaret.classification import *\n",
        "s = setup(data, target = 'Class variable', session_id = 123)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3c583864"
      },
      "source": [
        "Once the setup has been successfully executed it shows the information grid containing experiment level information.\n",
        "\n",
        "- **Session id:**  A pseudo-random number distributed as a seed in all functions for later reproducibility. If no `session_id` is passed, a random number is automatically generated that is distributed to all functions.<br/>\n",
        "<br/>\n",
        "- **Target type:**  Binary, Multiclass, or Regression. The Target type is automatically detected. <br/>\n",
        "<br/>\n",
        "- **Label Encoding:**  When the Target variable is of type string (i.e. 'Yes' or 'No') instead of 1 or 0, it automatically encodes the label into 1 and 0 and displays the mapping (0 : No, 1 : Yes) for reference. In this tutorial, no label encoding is required since the target variable is of numeric type. <br/>\n",
        "<br/>\n",
        "- **Original data shape:**  Shape of the original data prior to any transformations. <br/>\n",
        "<br/>\n",
        "- **Transformed train set shape :**  Shape of transformed train set <br/>\n",
        "<br/>\n",
        "- **Transformed test set shape :**  Shape of transformed test set <br/>\n",
        "<br/>\n",
        "- **Numeric features :**  The number of features considered as numerical. <br/>\n",
        "<br/>\n",
        "- **Categorical features :**  The number of features considered as categorical. <br/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ada19398"
      },
      "source": [
        "PyCaret has two set of API's that you can work with. (1) Functional (as seen above) and (2) Object Oriented API.\n",
        "\n",
        "With Object Oriented API instead of executing functions directly you will import a class and execute methods of class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32ee91c9"
      },
      "outputs": [],
      "source": [
        "# import ClassificationExperiment and init the class\n",
        "from pycaret.classification import ClassificationExperiment\n",
        "exp = ClassificationExperiment()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ead9fb5"
      },
      "outputs": [],
      "source": [
        "# check the type of exp\n",
        "type(exp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f05b8590"
      },
      "outputs": [],
      "source": [
        "# init setup on exp\n",
        "exp.setup(data, target = 'Class variable', session_id = 123)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77213120"
      },
      "source": [
        "You can use any of the two method i.e. Functional or OOP and even switch back and forth between two set of API's. The choice of method will not impact the results and has been tested for consistency."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f98dd435"
      },
      "source": [
        "- Compare Models\n",
        "\n",
        "This function trains and evaluates the performance of all the estimators available in the model library using cross-validation. The output of this function is a scoring grid with average cross-validated scores. Metrics evaluated during CV can be accessed using the `get_metrics` function. Custom metrics can be added or removed using `add_metric` and `remove_metric` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65a19df4"
      },
      "outputs": [],
      "source": [
        "# compare baseline models\n",
        "best = compare_models()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87477aa4"
      },
      "outputs": [],
      "source": [
        "# compare models using OOP\n",
        "exp.compare_models()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "340de1e2"
      },
      "source": [
        "Notice that the output between functional and OOP API is consistent. Rest of the functions in this notebook will only be shown using functional API only."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a77ec0c"
      },
      "source": [
        "- Analyze Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "595ea108"
      },
      "source": [
        "You can use the `plot_model` function to analyzes the performance of a trained model on the test set. It may require re-training the model in certain cases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ec7fad6"
      },
      "outputs": [],
      "source": [
        "# plot confusion matrix\n",
        "plot_model(best, plot = 'confusion_matrix')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fc4b9b1"
      },
      "outputs": [],
      "source": [
        "# plot AUC\n",
        "plot_model(best, plot = 'auc')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbc790e4"
      },
      "outputs": [],
      "source": [
        "# plot feature importance\n",
        "plot_model(best, plot = 'feature')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "da718984"
      },
      "outputs": [],
      "source": [
        "# check docstring to see available plots\n",
        "# help(plot_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bd66179"
      },
      "source": [
        "An alternate to `plot_model` function is `evaluate_model`. It can only be used in Notebook since it uses ipywidget."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c75f07a8"
      },
      "outputs": [],
      "source": [
        "evaluate_model(best)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab3d2f1e"
      },
      "source": [
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "954cbeff"
      },
      "source": [
        "-  Prediction\n",
        "The `predict_model` function returns `prediction_label` and `prediction_score` (probability of the predicted class) as new columns in dataframe. When data is `None` (default), it uses the test set (created during the setup function) for scoring."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87c1a007"
      },
      "outputs": [],
      "source": [
        "# predict on test set\n",
        "holdout_pred = predict_model(best)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5c01ac77"
      },
      "outputs": [],
      "source": [
        "# show predictions df\n",
        "holdout_pred.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4baf825"
      },
      "source": [
        "The same function works for predicting the labels on unseen dataset. Let's create a copy of original data and drop the `Class variable`. We can then use the new data frame without labels for scoring."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fb1cb86d"
      },
      "outputs": [],
      "source": [
        "# copy data and drop Class variable\n",
        "\n",
        "new_data = data.copy()\n",
        "new_data.drop('Class variable', axis=1, inplace=True)\n",
        "new_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5803df9"
      },
      "outputs": [],
      "source": [
        "# predict model on new_data\n",
        "predictions = predict_model(best, data = new_data)\n",
        "predictions.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4384735"
      },
      "source": [
        "- Save Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd63f053"
      },
      "source": [
        "Finally, you can save the entire pipeline on disk for later use, using pycaret's `save_model` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4181de41"
      },
      "outputs": [],
      "source": [
        "# save pipeline\n",
        "save_model(best, 'my_first_pipeline')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40ed5152"
      },
      "outputs": [],
      "source": [
        "# load pipeline\n",
        "loaded_best_pipeline = load_model('my_first_pipeline')\n",
        "loaded_best_pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2c7d62e"
      },
      "source": [
        "- Detailed function-by-function overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e05937f5"
      },
      "source": [
        "-  Setup\n",
        "This function initializes the experiment in PyCaret and creates the transformation pipeline based on all the parameters passed in the function. Setup function must be called before executing any other function. It takes two required parameters: `data` and `target`. All the other parameters are optional and are used for configuring data preprocessing pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24e503be"
      },
      "outputs": [],
      "source": [
        "# init setup function\n",
        "s = setup(data, target = 'Class variable', session_id = 123)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "924d198b"
      },
      "source": [
        "To access all the variables created by the setup function such as transformed dataset, random_state, etc. you can use `get_config` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76128b08"
      },
      "outputs": [],
      "source": [
        "# check all available config\n",
        "get_config()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbc43292"
      },
      "outputs": [],
      "source": [
        "# lets access X_train_transformed\n",
        "get_config('X_train_transformed')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ef9cd061"
      },
      "outputs": [],
      "source": [
        "# another example: let's access seed\n",
        "print(\"The current seed is: {}\".format(get_config('seed')))\n",
        "\n",
        "# now lets change it using set_config\n",
        "set_config('seed', 786)\n",
        "print(\"The new seed is: {}\".format(get_config('seed')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7afbe41d"
      },
      "source": [
        "All the preprocessing configurations and experiment settings/parameters are passed into the `setup` function. To see all available parameters, check the docstring:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2885a14f"
      },
      "outputs": [],
      "source": [
        "# help(setup)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34ae0fce"
      },
      "outputs": [],
      "source": [
        "# init setup with normalize = True\n",
        "\n",
        "s = setup(data, target = 'Class variable', session_id = 123,\n",
        "          normalize = True, normalize_method = 'minmax')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04204ae7"
      },
      "outputs": [],
      "source": [
        "# lets check the X_train_transformed to see effect of params passed\n",
        "get_config('X_train_transformed')['Number of times pregnant'].hist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d28a3e4e"
      },
      "source": [
        "Notice that all the values are between 0 and 1 - that is because we passed `normalize=True` in the `setup` function. If you don't remember how it compares to actual data, no problem - we can also access non-transformed values using `get_config` and then compare. See below and notice the range of values on x-axis and compare it with histogram above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68cc1c63"
      },
      "outputs": [],
      "source": [
        "get_config('X_train')['Number of times pregnant'].hist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3776fbf"
      },
      "source": [
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36b8b803"
      },
      "source": [
        "- Compare Models\n",
        "This function trains and evaluates the performance of all estimators available in the model library using cross-validation. The output of this function is a scoring grid with average cross-validated scores. Metrics evaluated during CV can be accessed using the `get_metrics` function. Custom metrics can be added or removed using `add_metric` and `remove_metric` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3350418"
      },
      "outputs": [],
      "source": [
        "best = compare_models()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd18dcf6"
      },
      "source": [
        "`compare_models` by default uses all the estimators in model library (all except models with `Turbo=False`) . To see all available models you can use the function `models()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "432e93d7"
      },
      "outputs": [],
      "source": [
        "# check available models\n",
        "models()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f588f54b"
      },
      "source": [
        "You can use the `include` and `exclude` parameter in the `compare_models` to train only select model or exclude specific models from training by passing the model id's in `exclude` parameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2a7e578"
      },
      "outputs": [],
      "source": [
        "compare_tree_models = compare_models(include = ['dt', 'rf', 'et', 'gbc', 'xgboost', 'lightgbm'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c15a467e"
      },
      "outputs": [],
      "source": [
        "compare_tree_models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af9ae6cd"
      },
      "source": [
        "The function above has return trained model object as an output. The scoring grid is only displayed and not returned. If you need access to the scoring grid you can use `pull` function to access the dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fc529e25"
      },
      "outputs": [],
      "source": [
        "compare_tree_models_results = pull()\n",
        "compare_tree_models_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05a72fc2"
      },
      "source": [
        "By default `compare_models` return the single best performing model based on the metric defined in the `sort` parameter. Let's change our code to return 3 top models based on `Recall`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1066dd07"
      },
      "outputs": [],
      "source": [
        "best_recall_models_top3 = compare_models(sort = 'Recall', n_select = 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aa259708"
      },
      "outputs": [],
      "source": [
        "# list of top 3 models by Recall\n",
        "best_recall_models_top3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d91d27e"
      },
      "source": [
        "Some other parameters that you might find very useful in `compare_models` are:\n",
        "\n",
        "- fold\n",
        "- cross_validation\n",
        "- budget_time\n",
        "- errors\n",
        "- probability_threshold\n",
        "- parallel\n",
        "\n",
        "You can check the docstring of the function for more info."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0c49e0b"
      },
      "outputs": [],
      "source": [
        "# help(compare_models)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4ec411d"
      },
      "source": [
        "- Set Custom Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bd9d6395"
      },
      "outputs": [],
      "source": [
        "# check available metrics used in CV\n",
        "get_metrics()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0d92512d"
      },
      "outputs": [],
      "source": [
        "# create a custom function\n",
        "import numpy as np\n",
        "\n",
        "def custom_metric(y, y_pred):\n",
        "    tp = np.where((y_pred==1) & (y==1), (100), 0)\n",
        "    fp = np.where((y_pred==1) & (y==0), -5, 0)\n",
        "    return np.sum([tp,fp])\n",
        "\n",
        "# add metric to PyCaret\n",
        "add_metric('custom_metric', 'Custom Metric', custom_metric)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4740319"
      },
      "outputs": [],
      "source": [
        "# now let's run compare_models again\n",
        "compare_models()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ee60c4e"
      },
      "outputs": [],
      "source": [
        "# remove custom metric\n",
        "remove_metric('custom_metric')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e6c47cb"
      },
      "source": [
        "-  Experiment Logging\n",
        "PyCaret integrates with many different type of experiment loggers (default = 'mlflow'). To turn on experiment tracking in PyCaret you can set `log_experiment` and `experiment_name` parameter. It will automatically track all the metrics, hyperparameters, and artifacts based on the defined logger."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1af63fd0"
      },
      "outputs": [],
      "source": [
        "# from pycaret.classification import *\n",
        "# s = setup(data, target = 'Class variable', log_experiment='mlflow', experiment_name='diabetes_experiment')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42cd6120"
      },
      "outputs": [],
      "source": [
        "# compare models\n",
        "# best = compare_models()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adc82ce5"
      },
      "outputs": [],
      "source": [
        "# start mlflow server on localhost:5000\n",
        "# !mlflow ui"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a06f9df7"
      },
      "source": [
        "By default PyCaret uses `MLFlow` logger that can be changed using `log_experiment` parameter. Following loggers are available:\n",
        "    \n",
        "    - mlflow\n",
        "    - wandb\n",
        "    - comet_ml\n",
        "    - dagshub\n",
        "    \n",
        "Other logging related parameters that you may find useful are:\n",
        "\n",
        "- experiment_custom_tags\n",
        "- log_plots\n",
        "- log_data\n",
        "- log_profile\n",
        "\n",
        "For more information check out the docstring of the `setup` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2f8b6aa1"
      },
      "outputs": [],
      "source": [
        "# help(setup)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ce0b555"
      },
      "source": [
        "-  Create Model\n",
        "This function trains and evaluates the performance of a given estimator using cross-validation. The output of this function is a scoring grid with CV scores by fold. Metrics evaluated during CV can be accessed using the `get_metrics` function. Custom metrics can be added or removed using `add_metric` and `remove_metric` function. All the available models can be accessed using the models function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "837cebfa"
      },
      "outputs": [],
      "source": [
        "# check all the available models\n",
        "models()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16641cab"
      },
      "outputs": [],
      "source": [
        "# train logistic regression with default fold=10\n",
        "lr = create_model('lr')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "074b4572"
      },
      "source": [
        "The function above has return trained model object as an output. The scoring grid is only displayed and not returned. If you need access to the scoring grid you can use `pull` function to access the dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fe277e1b"
      },
      "outputs": [],
      "source": [
        "lr_results = pull()\n",
        "print(type(lr_results))\n",
        "lr_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "148a74c4"
      },
      "outputs": [],
      "source": [
        "# train logistic regression with fold=3\n",
        "lr = create_model('lr', fold=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5a8906a8"
      },
      "outputs": [],
      "source": [
        "# train logistic regression with specific model parameters\n",
        "create_model('lr', C = 0.5, l1_ratio = 0.15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b85af29b"
      },
      "outputs": [],
      "source": [
        "# train lr and return train score as well alongwith CV\n",
        "create_model('lr', return_train_score=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3b9d9ac6"
      },
      "outputs": [],
      "source": [
        "# change the probability threshold of classifier from 0.5 to 0.66\n",
        "create_model('lr', probability_threshold = 0.66)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08634e9e"
      },
      "source": [
        "Some other parameters that you might find very useful in `create_model` are:\n",
        "\n",
        "- cross_validation\n",
        "- engine\n",
        "- fit_kwargs\n",
        "- groups\n",
        "\n",
        "You can check the docstring of the function for more info."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3fb32c74"
      },
      "outputs": [],
      "source": [
        "# help(create_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5378836"
      },
      "source": [
        "- Tune Model\n",
        "\n",
        "This function tunes the hyperparameters of the model. The output of this function is a scoring grid with cross-validated scores by fold. The best model is selected based on the metric defined in optimize parameter. Metrics evaluated during cross-validation can be accessed using the `get_metrics` function. Custom metrics can be added or removed using `add_metric` and `remove_metric` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "402597f2"
      },
      "outputs": [],
      "source": [
        "# train a dt model with default params\n",
        "dt = create_model('dt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90dc451d"
      },
      "outputs": [],
      "source": [
        "# tune hyperparameters of dt\n",
        "tuned_dt = tune_model(dt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "805b66b9"
      },
      "source": [
        "Metric to optimize can be defined in `optimize` parameter (default = 'Accuracy'). Also, a custom tuned grid can be passed with `custom_grid` parameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cca8e640"
      },
      "outputs": [],
      "source": [
        "dt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31e050ff"
      },
      "outputs": [],
      "source": [
        "# define tuning grid\n",
        "dt_grid = {'max_depth' : [None, 2, 4, 6, 8, 10, 12]}\n",
        "\n",
        "# tune model with custom grid and metric = F1\n",
        "tuned_dt = tune_model(dt, custom_grid = dt_grid, optimize = 'F1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6cca514b"
      },
      "outputs": [],
      "source": [
        "# to access the tuner object you can set return_tuner = True\n",
        "tuned_dt, tuner = tune_model(dt, return_tuner=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80cbb4a6"
      },
      "outputs": [],
      "source": [
        "# model object\n",
        "tuned_dt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7d5e49ca"
      },
      "outputs": [],
      "source": [
        "# tuner object\n",
        "tuner"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a33c70b"
      },
      "source": [
        "The default search algorithm is `RandomizedSearchCV` from `sklearn`. This can be changed by using `search_library` and `search_algorithm` parameter."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "id": "rhnUdWTqtz5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna-integration"
      ],
      "metadata": {
        "id": "NOy3UBqz_Yvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31e33547"
      },
      "outputs": [],
      "source": [
        "# tune dt using optuna\n",
        "tuned_dt = tune_model(dt, search_library = 'optuna')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1e2875b"
      },
      "source": [
        "For more details on all available `search_library` and `search_algorithm` please check the docstring. Some other parameters that you might find very useful in `tune_model` are:\n",
        "\n",
        "- choose_better\n",
        "- n_iter\n",
        "- early_stopping\n",
        "- groups\n",
        "\n",
        "You can check the docstring of the function for more info."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94f9c86e"
      },
      "outputs": [],
      "source": [
        "# help(tune_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ca8ee07"
      },
      "source": [
        "-  Ensemble Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce321d4c"
      },
      "source": [
        "This function ensembles a given estimator. The output of this function is a scoring grid with CV scores by fold. Metrics evaluated during CV can be accessed using the `get_metrics` function. Custom metrics can be added or removed using `add_metric` and `remove_metric` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8ee936d"
      },
      "outputs": [],
      "source": [
        "# ensemble with bagging\n",
        "ensemble_model(dt, method = 'Bagging')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79279394"
      },
      "outputs": [],
      "source": [
        "# ensemble with boosting\n",
        "ensemble_model(dt, method = 'Boosting')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0fa1ce2"
      },
      "source": [
        "Some other parameters that you might find very useful in `ensemble_model` are:\n",
        "\n",
        "- choose_better\n",
        "- n_estimators\n",
        "- groups\n",
        "- fit_kwargs\n",
        "- probability_threshold\n",
        "- return_train_score\n",
        "\n",
        "You can check the docstring of the function for more info."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78130ed1"
      },
      "outputs": [],
      "source": [
        "# help(ensemble_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea8a9a4e"
      },
      "source": [
        "- Blend Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ede29c4"
      },
      "source": [
        "This function trains a Soft Voting / Majority Rule classifier for select models passed in the estimator_list parameter. The output of this function is a scoring grid with CV scores by fold. Metrics evaluated during CV can be accessed using the `get_metrics` function. Custom metrics can be added or removed using `add_metric` and `remove_metric` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "61a7a1c5"
      },
      "outputs": [],
      "source": [
        "# top 3 models based on recall\n",
        "best_recall_models_top3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04f65f2f"
      },
      "outputs": [],
      "source": [
        "# blend top 3 models\n",
        "blend_models(best_recall_models_top3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e788c9c"
      },
      "source": [
        "Some other parameters that you might find very useful in `blend_models` are:\n",
        "\n",
        "- choose_better\n",
        "- method\n",
        "- weights\n",
        "- fit_kwargs\n",
        "- probability_threshold\n",
        "- return_train_score\n",
        "\n",
        "You can check the docstring of the function for more info."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99b549a6"
      },
      "outputs": [],
      "source": [
        "# help(blend_models)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e76969b0"
      },
      "source": [
        "- Stack Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55909804"
      },
      "source": [
        "This function trains a meta-model over select estimators passed in the estimator_list parameter. The output of this function is a scoring grid with CV scores by fold. Metrics evaluated during CV can be accessed using the `get_metrics` function. Custom metrics can be added or removed using `add_metric` and `remove_metric` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "201c681e"
      },
      "outputs": [],
      "source": [
        "# stack models\n",
        "stack_models(best_recall_models_top3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af78cda8"
      },
      "source": [
        "Some other parameters that you might find very useful in `stack_models` are:\n",
        "\n",
        "- choose_better\n",
        "- meta_model\n",
        "- method\n",
        "- restack\n",
        "- probability_threshold\n",
        "- return_train_score\n",
        "\n",
        "You can check the docstring of the function for more info."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3305e597"
      },
      "outputs": [],
      "source": [
        "# help(stack_models)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "279a3127"
      },
      "source": [
        "- Plot Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "862bd3e9"
      },
      "source": [
        "This function analyzes the performance of a trained model on the hold-out set. It may require re-training the model in certain cases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9c8da9b4"
      },
      "outputs": [],
      "source": [
        "# plot class report\n",
        "plot_model(best, plot = 'class_report')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "952b6f24"
      },
      "outputs": [],
      "source": [
        "# to control the scale of plot\n",
        "plot_model(best, plot = 'class_report', scale = 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54389270"
      },
      "outputs": [],
      "source": [
        "# to save the plot\n",
        "plot_model(best, plot = 'class_report', save=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fef279d"
      },
      "source": [
        "Some other parameters that you might find very useful in `plot_model` are:\n",
        "\n",
        "- fit_kwargs\n",
        "- plot_kwargs\n",
        "- groups\n",
        "- display_format\n",
        "\n",
        "You can check the docstring of the function for more info."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54b09b8e"
      },
      "outputs": [],
      "source": [
        "# help(plot_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b724ca46"
      },
      "source": [
        "- Interpret Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52f8fb33"
      },
      "source": [
        "This function analyzes the predictions generated from a trained model. Most plots in this function are implemented based on the SHAP (Shapley Additive exPlanations). For more info on this, please see https://shap.readthedocs.io/en/latest/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6b6891b7"
      },
      "outputs": [],
      "source": [
        "# train lightgbm model\n",
        "lightgbm = create_model('lightgbm')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pycaret[full]"
      ],
      "metadata": {
        "id": "dnKpb9P3uI0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3198b571"
      },
      "outputs": [],
      "source": [
        "# interpret summary model\n",
        "interpret_model(lightgbm, plot = 'summary')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "824bafdc"
      },
      "outputs": [],
      "source": [
        "interpret_model(lightgbm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ca7ce2b4"
      },
      "source": [
        "Some other parameters that you might find very useful in `interpret_model` are:\n",
        "\n",
        "- plot\n",
        "- feature\n",
        "- use_train_data\n",
        "- X_new_sample\n",
        "- y_new_sample\n",
        "- save\n",
        "\n",
        "You can check the docstring of the function for more info."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42595030"
      },
      "outputs": [],
      "source": [
        "# help(interpret_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6704f489"
      },
      "source": [
        "- Calibrate Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9713f0d"
      },
      "source": [
        "This function calibrates the probability of a given model using isotonic or logistic regression. The output of this function is a scoring grid with CV scores by fold. Metrics evaluated during CV can be accessed using the `get_metrics` function. Custom metrics can be added or removed using `add_metric` and `remove_metric` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74d47dde"
      },
      "outputs": [],
      "source": [
        "# check calbiration of default dt\n",
        "plot_model(dt, plot = 'calibration')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3d5bb544"
      },
      "outputs": [],
      "source": [
        "# calibrate default dt\n",
        "calibrated_dt = calibrate_model(dt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4f40500f"
      },
      "outputs": [],
      "source": [
        "# check calbiration of calibrated dt\n",
        "plot_model(calibrated_dt, plot = 'calibration')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fee2a997"
      },
      "source": [
        "Some other parameters that you might find very useful in `calibrate_model` are:\n",
        "\n",
        "- calibrate_fold\n",
        "- fit_kwargs\n",
        "- method\n",
        "- return_train_score\n",
        "- groups\n",
        "\n",
        "You can check the docstring of the function for more info."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30a5cb89"
      },
      "outputs": [],
      "source": [
        "# help(calibrate_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f57d0c8"
      },
      "source": [
        "- Get Leaderboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec63b67a"
      },
      "source": [
        "This function returns the leaderboard of all models trained in the current setup."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "307a6e3c"
      },
      "outputs": [],
      "source": [
        "# get leaderboard\n",
        "lb = get_leaderboard()\n",
        "lb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8a8b060"
      },
      "outputs": [],
      "source": [
        "# select the best model based on F1\n",
        "lb.sort_values(by='F1', ascending=False)['Model'].iloc[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ecf0bfa"
      },
      "source": [
        "Some other parameters that you might find very useful in `get_leaderboard` are:\n",
        "\n",
        "- finalize_models\n",
        "- fit_kwargs\n",
        "- model_only\n",
        "- groups\n",
        "\n",
        "You can check the docstring of the function for more info."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dc76f0a5"
      },
      "outputs": [],
      "source": [
        "# help(get_leaderboard)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94669c72"
      },
      "source": [
        "- AutoML\n",
        "This function returns the best model out of all trained models in the current setup based on the optimize parameter. Metrics evaluated can be accessed using the `get_metrics` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01532054"
      },
      "outputs": [],
      "source": [
        "automl()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "248dcc7c"
      },
      "source": [
        "-  Check Fairness"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93813447"
      },
      "source": [
        "There are many approaches to conceptualizing fairness. The check_fairness function follows the approach known as group fairness, which asks: which groups of individuals are at risk for experiencing harm. `check_fairness` provides fairness-related metrics between different groups (also called sub-population)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "316e23cb"
      },
      "outputs": [],
      "source": [
        "# check fairness\n",
        "check_fairness(best, sensitive_features = ['Number of times pregnant'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "726b2986"
      },
      "source": [
        "- Dashboard\n",
        "The dashboard function generates the interactive dashboard for a trained model. The dashboard is implemented using `ExplainerDashboard`. For more information check out [Explainer Dashboard.](explainerdashboard.readthedocs.io)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ca75507d"
      },
      "outputs": [],
      "source": [
        "# dashboard function\n",
        "dashboard(dt, display_format ='inline')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e27c212b"
      },
      "source": [
        "-  Finalize Model\n",
        "This function trains a given model on the entire dataset including the hold-out set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65225684"
      },
      "outputs": [],
      "source": [
        "final_best = finalize_model(best)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80d17fec"
      },
      "outputs": [],
      "source": [
        "final_best"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4693f88"
      },
      "source": [
        "-  Convert Model\n",
        "This function transpiles the trained machine learning model's decision function in different programming languages such as Python, C, Java, Go, C#, etc. It is very useful if you want to deploy models into environments where you can't install your normal Python stack to support model inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbe0e9fe"
      },
      "outputs": [],
      "source": [
        "# transpiles learned function to java\n",
        "print(convert_model(best, language = 'java'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed00202c"
      },
      "source": [
        "-  Deploy Model\n",
        "This function deploys the entire ML pipeline on the cloud.\n",
        "\n",
        "**AWS:**  When deploying model on AWS S3, environment variables must be configured using the command-line interface. To configure AWS environment variables, type `aws configure` in terminal. The following information is required which can be generated using the Identity and Access Management (IAM) portal of your amazon console account:\n",
        "\n",
        "- AWS Access Key ID\n",
        "- AWS Secret Key Access\n",
        "- Default Region Name (can be seen under Global settings on your AWS console)\n",
        "- Default output format (must be left blank)\n",
        "\n",
        "**GCP:** To deploy a model on Google Cloud Platform ('gcp'), the project must be created using the command-line or GCP console. Once the project is created, you must create a service account and download the service account key as a JSON file to set environment variables in your local environment. Learn more about it: https://cloud.google.com/docs/authentication/production\n",
        "\n",
        "**Azure:** To deploy a model on Microsoft Azure ('azure'), environment variables for the connection string must be set in your local environment. Go to settings of storage account on Azure portal to access the connection string required.\n",
        "AZURE_STORAGE_CONNECTION_STRING (required as environment variable)\n",
        "Learn more about it: https://docs.microsoft.com/en-us/azure/storage/blobs/storage-quickstart-blobs-python?toc=%2Fpython%2Fazure%2FTOC.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40b20a18"
      },
      "outputs": [],
      "source": [
        "# deploy model on aws s3\n",
        "# deploy_model(best, model_name = 'my_first_platform_on_aws',\n",
        "#             platform = 'aws', authentication = {'bucket' : 'pycaret-test'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9e236516"
      },
      "outputs": [],
      "source": [
        "# load model from aws s3\n",
        "# loaded_from_aws = load_model(model_name = 'my_first_platform_on_aws', platform = 'aws',\n",
        "#                              authentication = {'bucket' : 'pycaret-test'})\n",
        "\n",
        "# loaded_from_aws"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e169ae86"
      },
      "source": [
        "- Save / Load Model\n",
        "This function saves the transformation pipeline and a trained model object into the current working directory as a pickle file for later use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bc5cf24a"
      },
      "outputs": [],
      "source": [
        "# save model\n",
        "save_model(best, 'my_first_model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8478d34"
      },
      "outputs": [],
      "source": [
        "# load model\n",
        "loaded_from_disk = load_model('my_first_model')\n",
        "loaded_from_disk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de5eee8c"
      },
      "source": [
        "- Save / Load Experiment\n",
        "This function saves all the experiment variables on disk, allowing to later resume without rerunning the setup function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6a3c61b6"
      },
      "outputs": [],
      "source": [
        "# save experiment\n",
        "save_experiment('my_experiment')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83252c09"
      },
      "outputs": [],
      "source": [
        "# load experiment from disk\n",
        "exp_from_disk = load_experiment('my_experiment', data=data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### auto-sklearn(https://automl.github.io/auto-sklearn/master/index.html)\n",
        "\n",
        "- scikit-learning은 다양한 ML estimator를 제공하고 있음.\n",
        "    - ML 모델을 만들기 위해 scikit-learn을 사용할 때 흔히 고민에 빠지는 포인트가 있습니다.\n",
        "        - 어떤 estimator를 골라야 하나?>\n",
        "        - estimator의 파라미터는 뭘로 세팅을 해야 하나? 값은 어떻게 튜닝을 할까?\n",
        "        - 이런 결정은 다른 사람들은 어떻게 할까?\n",
        "- 소개 글\n",
        "    - auto-sklearn is an automated machine learning toolkit and a drop-in replacement for a scikit-learn estimator.\n",
        "\n",
        "- auto-sklearn의 가장 큰 장점 중 하나는 바로 scikit-learn과의 api 유사성으로 인해 scikit-learn 사용자에게 친숙한 인터페이스를 제공합니다. 이를 통해 classifier, regressor를 구현할 때 별도의 튜닝 포인트 없이 편하게 사용할 수 있습니다."
      ],
      "metadata": {
        "id": "UkraQBrGqMWq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 작동원리\n",
        "\n",
        "<img src='https://github.com/automl/auto-sklearn/raw/master/doc/images/askl_pipeline.png'>\n",
        "\n",
        "\n",
        "- auto-sklearn은 AutoML을 CASH 문제를 푸는 것으로 다루고 있습니다.\n",
        "\n",
        "- CASH는 Combined Algorithm Selection and Hyperparameter optimization 의 약자로 한글로 풀면 알고리즘 선택과 하이퍼파라미터 튜닝을 의미합니다.\n",
        "\n",
        "- 위 그림에서 auto-sklearn에서 추가한 단계는 meta-learning 과 ensemble 입니다\n",
        "\n",
        "    - 1) meta-learning 단계는 Bayesian optimizer를 warm start 해주는 역할을 합니다.\n",
        "\n",
        "    - 이 단계를 통해 Bayesian optimizer를 좀더 효율적으로 돌릴 수 있게 됩니다.\n",
        "\n",
        "    - 2) Bayesian optimizer를 통해 하이퍼파라미터 튜닝을 진행합니다\n",
        "\n",
        "    - 3) 지금까지 테스트한 모델과 파라미터 조합을 Ensemble하여 최적의 조합을 찾습니다."
      ],
      "metadata": {
        "id": "IEfVkF3dqtN2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##7. TPOT(https://epistasislab.github.io/tpot/)\n",
        "\n",
        "- Python에서 AutoML을 수행하기위한 오픈 소스 라이브러리\n",
        "\n",
        "- 데이터 변환 및 기계 학습 알고리즘에 인기있는 Scikit-Learn 기계 학습 라이브러리를 사용\n",
        "\n",
        "- 유전 프로그래밍 확률 적 글로벌 검색 절차를 사용\n",
        "\n",
        "- 주어진 데이터 세트에 대한 최고 성능의 모델 파이프 라인을 효율적으로 검색\n",
        "\n",
        "<img src='https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fn5XGo%2Fbtq3yKfzKjO%2FY7C6WkOIfdnYqTAv0ckgxk%2Fimg.png'>"
      ],
      "metadata": {
        "id": "iWnQdVixtXJm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- TPOT tutorial on the Titanic dataset\n",
        "    - The Titanic machine learning competition on Kaggle is one of the most popular beginner's competitions on the platform. We will use that competition here to demonstrate the implementation of TPOT."
      ],
      "metadata": {
        "id": "6N9s-PB2u-lk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "U7uDd1hR7uVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "telescope_data=pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/magic/magic04.data',header=None)"
      ],
      "metadata": {
        "id": "iJA8x-9N7YKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "telescope_data.head()"
      ],
      "metadata": {
        "id": "kMN6Qxe_7wPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "telescope_data.columns = ['fLength', 'fWidth','fSize','fConc','fConcl','fAsym','fM3Long','fM3Trans','fAlpha','fDist','class']"
      ],
      "metadata": {
        "id": "UsyS5MVB7wxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "telescope_data.head()"
      ],
      "metadata": {
        "id": "ertytRIp7wz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "telescope_data.info()"
      ],
      "metadata": {
        "id": "HRpODo407w1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "telescope_data.describe()"
      ],
      "metadata": {
        "id": "iDSPlL977w4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "telescope_data['class'].value_counts()"
      ],
      "metadata": {
        "id": "kO0mWaLP70Zn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "telescope_shuffle=telescope_data.iloc[np.random.permutation(len(telescope_data))]\n",
        "tele=telescope_shuffle.reset_index(drop=True)\n",
        "tele.head()"
      ],
      "metadata": {
        "id": "4B6ae_ZI70b-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tele['class']=tele['class'].map({'g':0,'h':1})\n",
        "\n",
        "tele.head()"
      ],
      "metadata": {
        "id": "pS_9S3yA70eQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tele_class = tele['class'].values"
      ],
      "metadata": {
        "id": "LbmXmHJe7w6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.isnull(tele).any()"
      ],
      "metadata": {
        "id": "x7rey5gk8DeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tele = tele.fillna(-999)"
      ],
      "metadata": {
        "id": "4I7KDt_88DgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "training_indices, validation_indices = training_indices, testing_indices = train_test_split(tele.index,\n",
        "                                                                                            stratify = tele_class,\n",
        "                                                                                            train_size=0.75, test_size=0.25)"
      ],
      "metadata": {
        "id": "9N2b3AUi8Dio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tpot"
      ],
      "metadata": {
        "id": "RKsTlNRfFUsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tpot import TPOTClassifier\n",
        "from tpot import TPOTRegressor\n",
        "\n",
        "tpot = TPOTClassifier(generations=5,verbosity=2)\n",
        "\n",
        "tpot.fit(tele.drop('class',axis=1).loc[training_indices].values,\n",
        "         tele.loc[training_indices,'class'].values)"
      ],
      "metadata": {
        "id": "_AkwM6478Dkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tpot.score(tele.drop('class',axis=1).loc[validation_indices].values,\n",
        "           tele.loc[validation_indices, 'class'].values)"
      ],
      "metadata": {
        "id": "kV3SKp7l8bWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tpot.export('tpot_MAGIC_Gamma_Telescope_pipeline.py')"
      ],
      "metadata": {
        "id": "-OzjvlIC8cvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. H2O\n",
        "\n",
        "- h2o는 java 기반의 머신러닝/AI 플랫폼인데요.\n",
        "\n",
        "- 드라이버가 필요없이 간단한 설정만으로 인공지능을 학습하고 예측할 수 있는 소프트웨어\n",
        "\n",
        "- 인공지능에 대한 지식이 부족하여도 좋은 모델을 누구나 이용할 수 있는 소프트웨어가 개발되었음.\n"
      ],
      "metadata": {
        "id": "Wf1hYALW8xxP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install h2o\n",
        "import h2o"
      ],
      "metadata": {
        "id": "JxtiOWZx848d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 다음으로, H2O 클러스터를 시작합니다."
      ],
      "metadata": {
        "id": "RJ1l0xjS88Nw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 이제 타이타닉 데이터셋을 로드하고, H2O 데이터 프레임으로 변환합니다. 그리고 outcome variable을 지정합니다."
      ],
      "metadata": {
        "id": "Zibmi8AY8-h8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "h2o.init()"
      ],
      "metadata": {
        "id": "rtxlMHIO84-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "titanic_df = h2o.import_file(\"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\")\n",
        "titanic_df[\"Survived\"] = titanic_df[\"Survived\"].asfactor()"
      ],
      "metadata": {
        "id": "zyPp-VeI9Bgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train, test = titanic_df.split_frame(ratios=[0.8], seed=1)"
      ],
      "metadata": {
        "id": "dffjKcjm9Bit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 이제 H2O AutoML을 실행하여 최상의 모델을 생성함."
      ],
      "metadata": {
        "id": "iWIWTRuG9Fve"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from h2o.automl import H2OAutoML\n",
        "\n",
        "aml = H2OAutoML(max_runtime_secs=30, seed=1)\n",
        "aml.train(y=\"Survived\", training_frame=train)"
      ],
      "metadata": {
        "id": "RJi1ZM4D9Bk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 모델링이 완료되면, AutoML 리더보드를 확인하여 모델의 성능을 비교할 수 있습니다."
      ],
      "metadata": {
        "id": "xsaSag8I9kdP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "aml.leaderboard.head()"
      ],
      "metadata": {
        "id": "6sx9I8Bb9ldc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 가장 성능이 좋은 모델을 선택하고 test 데이터셋에서 모델의 성능을 평가합니다."
      ],
      "metadata": {
        "id": "fN2HtrMm9mtl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = aml.leader\n",
        "predictions = best_model.predict(test)"
      ],
      "metadata": {
        "id": "RHR_5Bif9oqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이 밖에 AutoML을 할 수 있는 라이브러리들은 다양하게 존재합니다:\n",
        "\n",
        "- TPOT\n",
        "- FLAML\n",
        "- EvalML\n",
        "- AutoKeras\n",
        "- Auto-ViML\n",
        "- AutoGluon\n",
        "- MLBox"
      ],
      "metadata": {
        "id": "Km96J6YZ9uQV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 참고 : 나는 모델 고민할 시간에 Autogluon을 써(https://dacon.io/codeshare/7764)"
      ],
      "metadata": {
        "id": "s4P_P69hMr7H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Hyperparam Tuning & AutoML](https://www.kaggle.com/code/sudalairajkumar/hyperparam-tuning-automl)"
      ],
      "metadata": {
        "id": "cjJOfaXb-YY6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "참고)\n",
        "\n",
        "### Numba\n",
        " - Python 및 Numpy 코드의 하위 집합을 빠른 기계 코드로 변환하는 오픈소스 Jit 컴파일러.\n",
        "\n",
        " - Just In Time 컴파일러를 사용해 파이썬 코드 내에서 일반 코드 및 Numpy를 아주 빠른 속도로 처리 가능한 기능을 제공."
      ],
      "metadata": {
        "id": "LXMue7hEXHpA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 공식문서에 설명\n",
        "    - Numba makes Python code fast.\n",
        "    - Numba is an open source JIT compiler that translates a subset of Python and NumPy code into fast machine code."
      ],
      "metadata": {
        "id": "9kIyoPXvn0if"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Numbda 작동원리\n",
        "    - 데커레이팅된 함수에 대한 파이썬 bytecode를 읽고 이를 함수의 입력 인수 유형에 대한 정보와 결합한다,.\n",
        "    - 코드를 분석하고 최적화한 후, LLVM compiler library를 사용하여 함수의 machine code 버전을 만들고, 이를 사용자의 CPU 능력에 맞춤.\n",
        "    - 이 complied된 버전이 앞으로 그 함수를 호출할 때마다 사용된다.\n",
        "\n",
        "- Numbda 모듈이 모든 파이썬 코드를 최적화 해주는 것은 아니다. 일부 파이썬 코드와 Numpy 대해서만 작동하며 다른 모듈을 이용한 코드를 최적화 시켜주지는 못한다. ex)Numbda는 Pandas를 이해하지 못함. 특정 목적에 따라 충분히 활용할 수 있는 가치가 있는 모듈."
      ],
      "metadata": {
        "id": "d1t_QtqdoIQC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from time import perf_counter\n",
        "from numba import jit\n",
        "\n",
        "# 일반적인 loop\n",
        "def pure_sum(n):\n",
        "    result = 0\n",
        "    for i in range(0, n+1):\n",
        "        result += i\n",
        "    return result\n",
        "\n",
        "# Numba 모듈 사용\n",
        "@jit(nopython=True, cache=True)\n",
        "def numba_sum(n):\n",
        "    result = 0\n",
        "    for i in range(0, n+1):\n",
        "        result += i\n",
        "    return result\n",
        "\n",
        "# 시간 재기: 일반\n",
        "start = perf_counter()\n",
        "pure_sum(100000000)\n",
        "print(perf_counter() - start)\n",
        "\n",
        "# 시간 재기에 앞서 먼저 Compile을 해준다.\n",
        "numba_sum(1)\n",
        "\n",
        "# 시간 재기: Numba\n",
        "start = perf_counter()\n",
        "numba_sum(100000000)\n",
        "print(perf_counter() - start)"
      ],
      "metadata": {
        "id": "bfiOI_ASn2v4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- @jit 데커레이터의 모드\n",
        "    - @jit 데커레이터는 nopython과 object라는 2가지 compilation 모드로 작동한다.\n",
        "    - 위 예제에서 nopython=True를 통해 Numba에게 nopython 모드로 작동하라고 지시한 셈인데, 이 모드는 decorate된 function을 근본적으로 compile하여 Python Interpreter의 개입 없이 전체가 작동하도록 한다.\n",
        "    - 만약 nopython 모드가 잘 작동하지 않을 경우, Numba은 object 모드를 통해 compile 할 수 있다. @jit(nopython=True)가 아닌 @jit이라고만 데커레이팅하면 이 모드가 작동하게 된다.\n",
        "    - 본 모드에서는 Numba은 loop를 식별하여 machine code에서 compile하며 나머지는 Intereter code에서 compile하게 된다. 더 나은 성능을 기대한다면 이 모드가 아닌 nopython 모드를 사용해야 한다.\n",
        "    "
      ],
      "metadata": {
        "id": "hTIBibfDourG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 참고 : Polars(https://realpython.com/polars-python/)"
      ],
      "metadata": {
        "id": "gbn8soQg-8Gf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "esSOr4ayaIMX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}