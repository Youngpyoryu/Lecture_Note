{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "텍스트 분석 실습.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "nZZROgtvYflZ",
        "s2qwITdVZkmr",
        "0zhjvocNaH9a",
        "ULouWP6wcF-U",
        "FUbEH33nceCK",
        "GK1CabKXdBuu",
        "TgbWWvnueAGY"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLukoUw-WOPG"
      },
      "source": [
        "# 호텔 리뷰 데이터 : 감성 분류& 긍정/부정 키워드 분석"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_YmcZ0LWUcx"
      },
      "source": [
        "##개요\n",
        "\n",
        "\n",
        "제주 호텔의 리뷰 데이터(평가 점수 + 평가 내용)을 활용해 다음 2가지 분석을 진행합니다:\n",
        "\n",
        "리뷰속에 담긴 사람의 긍정 / 부정 감성을 파악하여 분류할 수 있는 감성 분류 예측 모델을 만든다\n",
        "\n",
        "만든 모델을 활용해 긍정 / 부정 키워드를 출력해, 이용객들이 느낀 제주 호텔의 장,단점을 파악한다\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McOuoKB8WYS6"
      },
      "source": [
        "1. Library & Data import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6wCBA1wWFrK"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFqLTkCRWkK0"
      },
      "source": [
        "사용할 데이터셋\n",
        "\n",
        "Tripadvisor 여행사이트에서 \"제주 호텔\"로 검색해서 나온 리뷰들을 활용합니다. (평점 & 평가 내용 포함)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WM0cFTNaWb6W"
      },
      "source": [
        "df = pd.read_csv(\"https://raw.githubusercontent.com/yoonkt200/FastCampusDataset/master/tripadviser_review.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPt6K9O-Wjqo"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SySJcekXW0kn"
      },
      "source": [
        "### Feature Description\n",
        "\n",
        "rating: 이용자 리뷰의 평가 점수 (1~5)\n",
        "\n",
        "text: 이용자 리뷰 평가 내용\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVj7oCouWzA4"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOlMMOG8W7fx"
      },
      "source": [
        "#missing value\n",
        "df.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36pgQU_0XAa7"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsNQfaj6XB2u"
      },
      "source": [
        "df['text'][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-H7lRhjXD47"
      },
      "source": [
        "df['text'][100]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkrlZTsoXH0_"
      },
      "source": [
        "내용을 확인해보니 소량의 특수문자와 모음이 존재하는 경우가 있으니 정규표현식으로 제거해보자"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lRTb3umXGBd"
      },
      "source": [
        "# 정규 표현식 함수 정의\n",
        "\n",
        "import re\n",
        "\n",
        "def apply_regular_expression(text):\n",
        "    hangul = re.compile('[^ ㄱ-ㅣ 가-힣]')  # 한글 추출 규칙: 띄어 쓰기(1 개)를 포함한 한글\n",
        "    result = hangul.sub('', text)  # 위에 설정한 \"hangul\"규칙을 \"text\"에 적용(.sub)시킴\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRbMzcraXWUv"
      },
      "source": [
        "df['text'][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TP10nlrSXRFW"
      },
      "source": [
        "apply_regular_expression(df['text'][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZZROgtvYflZ"
      },
      "source": [
        "### 한국어 형태소 분석- 명사 단위"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ischs0SnYrB9"
      },
      "source": [
        "!pip install konlpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqrC_jtuXVwJ"
      },
      "source": [
        "from konlpy.tag import Okt\n",
        "from collections import Counter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrF41M4UYoqg"
      },
      "source": [
        "apply_regular_expression(df['text'][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPwOvtOJYxcF"
      },
      "source": [
        "okt = Okt() #명사 형태소 추출 함수\n",
        "nouns = okt.nouns(apply_regular_expression(df['text'][0]))\n",
        "nouns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7O-5E4T4Y7he"
      },
      "source": [
        "전체 말뭉치(Corpus)에 적용해서 명사 형태소를 추출"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3GCjUrBY5xE"
      },
      "source": [
        "#말물치 생성\n",
        "corpus = \"\".join(df['text'].tolist())\n",
        "# tolist() 함수를 사용하여 같은 레벨(위치)에 있는 데이터 끼리 묶어준다\n",
        "corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGwXxCwSZHdk"
      },
      "source": [
        "#정규표현식 적용\n",
        "apply_regular_expression(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-yHIYmBZYSf"
      },
      "source": [
        "#전체 말뭉치(Corpus)에서 명하 형태소 추출\n",
        "nouns = okt.nouns(apply_regular_expression(corpus))\n",
        "nouns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzjAMPyWZeRu"
      },
      "source": [
        "#빈도 탐색\n",
        "counter = Counter(nouns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enJZXg-hZiHW"
      },
      "source": [
        "counter.most_common(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2qwITdVZkmr"
      },
      "source": [
        "#### 한글자 명사 제거\n",
        "위 결과에서 보이듯이, 두 글자 키워드가 대부분 의미 있는 단어지만, ‘수’, ‘것’, '곳’과 같은 한 글자 키워드는 분석에 딱히 좋은 영향을 미치지 않은 것으로 보입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3WsG59DZjcN"
      },
      "source": [
        "available_counter = Counter({x: counter[x] for x in counter if len(x) > 1})\n",
        "available_counter.most_common(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78fkGZRQZtJG"
      },
      "source": [
        "이제 한글자 키워드 모두 제거됐습니다. 하지만 “우리”, “매우” 와 같은 실질적인 의미가 없고 꾸민 역할을 하는 불용어들 아직 존재합니다. 한국어 불용어 사전을 정의하여 불용어도 제거해줄게요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAav7h-hZwdx"
      },
      "source": [
        "[RANKS NL](https://www.ranks.nl/)에 제공해주는 [한국어 불용어 사전](https://www.ranks.nl/stopwords/korean)을 활용하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHacbhQZZq5z"
      },
      "source": [
        "stopwords = pd.read_csv(\"https://raw.githubusercontent.com/yoonkt200/FastCampusDataset/master/korean_stopwords.txt\").values.tolist()\n",
        "stopwords[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2X0YWw9XaDog"
      },
      "source": [
        "이 외에도 우리가 분석하고자 하는 데이터셋에 특화된 불용어들이 있습니다. 예를 들면: “제주”, “호텔”, “숙소” 등. 이런 단어들도 불용어 사전에 추가해보도록 할게요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6R_eJnMZaBiy"
      },
      "source": [
        "jeju_hotel_stopwords = ['제주', '제주도', '호텔', '리뷰', '숙소', '여행', '트립']\n",
        "for word in jeju_hotel_stopwords:\n",
        "    stopwords.append(word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zhjvocNaH9a"
      },
      "source": [
        "## word Count\n",
        "\n",
        "> BOW 벡터 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Na7nnf1HaG8F"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "def text_cleaning(text):\n",
        "    hangul = re.compile('[^ ㄱ-ㅣ 가-힣]')  # 정규 표현식 처리\n",
        "    result = hangul.sub('', text)\n",
        "    okt = Okt()  # 형태소 추출\n",
        "    nouns = okt.nouns(result)\n",
        "    nouns = [x for x in nouns if len(x) > 1]  # 한글자 키워드 제거\n",
        "    nouns = [x for x in nouns if x not in stopwords]  # 불용어 제거\n",
        "    return nouns\n",
        "\n",
        "vect = CountVectorizer(tokenizer = lambda x: text_cleaning(x))\n",
        "bow_vect = vect.fit_transform(df['text'].tolist())\n",
        "word_list = vect.get_feature_names()\n",
        "count_list = bow_vect.toarray().sum(axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAFXwGqnaMsT"
      },
      "source": [
        "#단어 리스트\n",
        "word_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5wDZpgdadGX"
      },
      "source": [
        "#각 단어가 전체 리뷰중에 등장한 총 횟수\n",
        "count_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kg84rrJUb8fe"
      },
      "source": [
        "#각 단어의 리뷰별 등장 횟수\n",
        "bow_vect.toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tz234P4fcAIV"
      },
      "source": [
        "bow_vect.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLQKh8uocCze"
      },
      "source": [
        "# \"단어\" - \"총 등장 횟수\" Matching\n",
        "\n",
        "word_count_dict = dict(zip(word_list, count_list))\n",
        "word_count_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULouWP6wcF-U"
      },
      "source": [
        "## TF-IDF 적용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wWOgCJScE-0"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "tfidf_vectorizer = TfidfTransformer()\n",
        "tf_idf_vect = tfidf_vectorizer.fit_transform(bow_vect)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypHY6tV3cKE5"
      },
      "source": [
        "print(tf_idf_vect.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UEFPh9DcNxM"
      },
      "source": [
        "변환 후 1001*3599 matrix가 출력됩니다. 여기서\n",
        "\n",
        "한 행(row)은 한 리뷰를 의미하고\n",
        "\n",
        "한 열(column)은 한 단어를 의미합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpQasMfCcMFL"
      },
      "source": [
        "# 첫 번째 리뷰에서의 단어 중요도(TF-IDF 값) -- 0이 아닌 것만 출력\n",
        "print(tf_idf_vect[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKKeO5qEcRB-"
      },
      "source": [
        "# 첫 번째 리뷰에서 모든 단어의 중요도 -- 0인 값까지 포함\n",
        "print(tf_idf_vect[0].toarray().shape)\n",
        "print(tf_idf_vect[0].toarray())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDhhxR-fcUIa"
      },
      "source": [
        "- \"벡터\"-\"단어\" mapping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3W1dJtMJcS6L"
      },
      "source": [
        "vect.vocabulary_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eM8nk2FjcZtW"
      },
      "source": [
        "invert_index_vectorizer = {v: k for k, v in vect.vocabulary_.items()}\n",
        "print(str(invert_index_vectorizer)[:100]+'...')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUbEH33nceCK"
      },
      "source": [
        "## 감성 분류 – Logistic Regression\n",
        "\n",
        "이제 전처리된 리뷰 데이터를 활용하여 감성 분류 예측 모델을 만들겠습니다.\n",
        "\n",
        "감성 분류 예측 모델이란, 이용자 리뷰의 평가 내용을 통해 이 리뷰가 긍정적인지, 부정적인지를 예측하여, 이용자의 감성을 파악하는 겁니다.\n",
        "\n",
        "따라서, 모델의 X 값(즉, feature 값)은 이용자 리뷰의 평가 내용이 되겠고, 모델의 Y 값(즉, label 값)은 이용자의 긍/부정 감성이 되겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGpUWKeGciHX"
      },
      "source": [
        "데이터셋 생성\n",
        "\n",
        "Label\n",
        "\n",
        "우리는 이용자의 리뷰를 “긍정” / “부정” 두가지 부류로 나누고자 합니다. 하지만 이러한 이용자의 감성을 대표할 수 있는 “평가 점수” 변수는 1 ~ 5의 value를 가지고 있습니다. 따라서 \"평가 점수\"변수 (rating: 1 ~ 5)를 이진 변수 (긍정: 1, 부정:0)으로 변환해야 합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3xkvoRwcct9"
      },
      "source": [
        "df.sample(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgvE1guucppz"
      },
      "source": [
        "리뷰 내용와 평점을 살펴보면, 4 ~ 5점 리뷰는 대부분 긍정적이었지만, 1 ~ 3점 리뷰에서는 부정적인 평가가 좀 많이 보였습니다.\n",
        "그래서 4점, 5점인 리뷰는 \"긍정적인 리뷰\"로 분류하여 1를 부여하고, 1 ~ 3점 리뷰는 \"부정적인 리뷰\"로 분류하여 0을 부여하도록 할게요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YU3KFKgGcnTT"
      },
      "source": [
        "df['rating'].hist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oP_UqIOOcsKf"
      },
      "source": [
        "def rating_to_label(rating):\n",
        "    if rating > 3:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "    \n",
        "df['y'] = df['rating'].apply(lambda x: rating_to_label(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-Iq2S9QcuxL"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCTLDfg1cx-1"
      },
      "source": [
        "df['y'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZZP1DtZc2CW"
      },
      "source": [
        "- Train,test Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plVyd9HRc02m"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x = tf_idf_vect\n",
        "y = df['y']\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoS_E7kgc6qh"
      },
      "source": [
        "x_train.shape, y_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0GTQXOKc9Bj"
      },
      "source": [
        "x_test.shape, y_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlzfcR_Vc-rt"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# fit in training set\n",
        "lr = LogisticRegression(random_state = 0)\n",
        "lr.fit(x_train, y_train)\n",
        "\n",
        "# predict in test set\n",
        "y_pred = lr.predict(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GK1CabKXdBuu"
      },
      "source": [
        "## 분류 결과 평가"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HQEiMHbdA9l"
      },
      "source": [
        "# classification result for test set\n",
        "\n",
        "print('accuracy: %.2f' % accuracy_score(y_test, y_pred))\n",
        "print('precision: %.2f' % precision_score(y_test, y_pred))\n",
        "print('recall: %.2f' % recall_score(y_test, y_pred))\n",
        "print('F1: %.2f' % f1_score(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUmozpIOdFSv"
      },
      "source": [
        "# confusion matrix\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "confu = confusion_matrix(y_true = y_test, y_pred = y_pred)\n",
        "\n",
        "plt.figure(figsize=(4, 3))\n",
        "sns.heatmap(confu, annot=True, annot_kws={'size':15}, cmap='OrRd', fmt='.10g')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUOUywJMdO2Q"
      },
      "source": [
        "모델 평가결과를 살펴보면, 모델이 지나치게 긍정(“1”)으로만 예측하는 경향이 있습니다. 따라서 긍정 리뷰를 잘 예측하지만, 부정 리뷰에 대한 예측 정확도가 매우 낮습니다. 이는 샘플데이터의 클래스 불균형으로 인한 문제로 보입니다.\n",
        "따라서, 클래스 불균형 조정을 진행하겠습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDgr1b3hdNER"
      },
      "source": [
        "df['y'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "352nlgygdS0u"
      },
      "source": [
        "positive_random_idx = df[df['y']==1].sample(275, random_state=12).index.tolist()\n",
        "negative_random_idx = df[df['y']==0].sample(275, random_state=12).index.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7NHrqgCduk_"
      },
      "source": [
        "random_idx = positive_random_idx + negative_random_idx\n",
        "x = tf_idf_vect[random_idx]\n",
        "y = df['y'][random_idx]\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjOSVMXKduxB"
      },
      "source": [
        "x_train.shape,y_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtAaYGeZdyCR"
      },
      "source": [
        "x_test.shape,y_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccRuc7CDd1Nx"
      },
      "source": [
        "모델 재학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tdXLKRed0BF"
      },
      "source": [
        "lr2 = LogisticRegression(random_state = 0)\n",
        "lr2.fit(x_train, y_train)\n",
        "y_pred = lr2.predict(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmWLy52xd459"
      },
      "source": [
        "분류 결과 평가"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GanFfONVd4KW"
      },
      "source": [
        "# classification result for test set\n",
        "\n",
        "print('accuracy: %.2f' % accuracy_score(y_test, y_pred))\n",
        "print('precision: %.2f' % precision_score(y_test, y_pred))\n",
        "print('recall: %.2f' % recall_score(y_test, y_pred))\n",
        "print('F1: %.2f' % f1_score(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8WXxMGbd73e"
      },
      "source": [
        "# confusion matrix\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "confu = confusion_matrix(y_true = y_test, y_pred = y_pred)\n",
        "\n",
        "plt.figure(figsize=(4, 3))\n",
        "sns.heatmap(confu, annot=True, annot_kws={'size':15}, cmap='OrRd', fmt='.10g')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgbWWvnueAGY"
      },
      "source": [
        "#### 긍정 / 부정 키워드 분석\n",
        "\n",
        "\n",
        "기계는 이처럼 리뷰 내용에 나타나는 사람의 감성을 구별할 수 있을 뿐만 아니라, 학습된 Logistic Regression 모델을 이용하여 긍/부정 키워드도 추출해낼 수 있습니다.\n",
        "\n",
        "추출된 키워드를 통해서 이용자가 느끼는 제주호델의 장,단점을 파악할 수 있고, 이를 기반으로 앞으로 유지해야 할 좋은 서비스와 개선이 필요한 아쉬운 서비스에 대해서도 어느정도 판단할 수 있습니다.\n",
        "\n",
        "\n",
        "긍 / 부정 키워드를 추출하기 위해 먼저 Logistic Regression 모델에 각 단어의 coeficient를 시각화해보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYA9YMUSd-QE"
      },
      "source": [
        "lr2.coef_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2rDK9zXeE5_"
      },
      "source": [
        "# print logistic regression's coef\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.bar(range(len(lr2.coef_[0])), lr2.coef_[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3J3PBO-KeIYB"
      },
      "source": [
        "여기서 계수가 양인 경우는 단어가 긍정적인 영향을 미쳤다고 볼 수 있고, 반면에, 음인 경우는 부정적인 영향을 미쳤다고 볼 수 있습니다.\n",
        "\n",
        "이 계수들을 크기순으로 정렬하면, 긍정 / 부정 키워드를 출력하는 지표가 되겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIBZ1tLveGlI"
      },
      "source": [
        "#긍정 키워드와 부정 키워드의 Top5를 각각 출력\n",
        "print(sorted(((value, index) for index, value in enumerate(lr2.coef_[0])), reverse = True)[:5])\n",
        "print(sorted(((value, index) for index, value in enumerate(lr2.coef_[0])), reverse = True)[-5:])\n",
        "# enumerate: 인덱스 번호와 컬렉션의 원소를 tuple형태로 반환함"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nCdgBL9eR5y"
      },
      "source": [
        "이처럼 단어의 coeficient와 index가 출력이 됩니다.\n",
        "\n",
        "\n",
        "이제 전체 단어가 포함한 \"긍정 키워드 리스트\"와 \"부정 키워드 리스트\"를 정의하고 출력해볼게요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJb3a2OSeP9b"
      },
      "source": [
        "coef_pos_index = sorted(((value, index) for index, value in enumerate(lr2.coef_[0])), reverse = True)\n",
        "coef_neg_index = sorted(((value, index) for index, value in enumerate(lr2.coef_[0])), reverse = False)\n",
        "coef_pos_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hJSIFO9eU2s"
      },
      "source": [
        "#index를 변환하여 '긍정 키워드 리스트'와 '부정 키워드 리스트'의 Top20단어를 출력\n",
        "invert_index_vectorizer = {v: k for k, v in vect.vocabulary_.items()}\n",
        "invert_index_vectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6OjNHVNedgo"
      },
      "source": [
        "for coef in coef_pos_index[:20]:\n",
        "    print(invert_index_vectorizer[coef[1]], coef[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiU6iUMsegBc"
      },
      "source": [
        "for coef in coef_neg_index[:20]:\n",
        "    print(invert_index_vectorizer[coef[1]], coef[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UyzolQcelc0"
      },
      "source": [
        "키워드를 살펴보면:\n",
        "\n",
        "이용객들이 보통 제주 호텔의 바다뷰 혹은 바다 접근성, 주변 맛집 그리고 인테리어 등에 만족하는 것으로 보입니다.\n",
        "하지만 숙소의 냄새 그리고 침대, 에어컨 등 시설의 상태가 많이 아쉬워 보이고 개선이 필요해 보임."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_FFw_ize8Uu"
      },
      "source": [
        "# 영화 시나리오 : word cloud & 단어 중요도(TF-IDF)분석"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wOHfjQNfDAE"
      },
      "source": [
        "## 1. Libray & data import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwygIi3peiHI"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGq0H6TSsLN9"
      },
      "source": [
        "## 사용할 데이터셋\n",
        "\n",
        "영화 'The Bourne Supermacy'의 시나리오를 활용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClPR_UiKfH9i"
      },
      "source": [
        "df = pd.read_csv(\"https://raw.githubusercontent.com/yoonkt200/FastCampusDataset/master/bourne_scenario.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9-xm0DisaQC"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M546lnVIHhK9"
      },
      "source": [
        "* Feature Description\n",
        " - page_no : 데이터가 위치한 pdf파일의 페이지 수\n",
        " - scence-title :씬 제목\n",
        " - text : 씬에 해당하는 지문/대본 텍스트 정보\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ud1mlvFhHf82"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtkegLk4HrAZ"
      },
      "source": [
        "#결측치\n",
        "df.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLk_b-fgHtBT"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Aj6zt3LHuLc"
      },
      "source": [
        "# text변수 확인\n",
        "df['text'][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpgUU1tEHz92"
      },
      "source": [
        "우리가 필요없는 내용들이 포함되어 있다. 맨 앞에 있는 씬 번호, 공백,특수문자 등 이들을 제거하는 전처리 과정이 필요함\n",
        "또한, Text mining을 진행할때, 대소문자의 구분이 의미가 없다. 따라서 대문자를 소문자로 변환하는 작업도 진행"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBIIIEUpH_bX"
      },
      "source": [
        "- 텍스트 데이터 전처리\n",
        "  - 정규 표현식 적용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-O8VozRHyuR"
      },
      "source": [
        "df['text'][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTFc3fLlIEMB"
      },
      "source": [
        "# 정규 표현식 함수 정의\n",
        "\n",
        "import re\n",
        "\n",
        "def apply_regular_expression(text):\n",
        "    text = text.lower()  # 대문자 -> 소문자 변환\n",
        "    english = re.compile('[^ a-z]')  # 영어 추출 규칙: 띄어 쓰기를 포함한 알파벳\n",
        "    result = english.sub('', text)  # 위에 설정한 \"english\"규칙을 \"text\"에 적용(.sub)시킴\n",
        "    result = re.sub(' +', ' ', result) # 2개 이상의 공백을(' +') 하나의 공백(' ')으로 바꿈\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dhI580AQ9Dl"
      },
      "source": [
        "apply_regular_expression(df['text'][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zefu--X0RBgV"
      },
      "source": [
        "소문자만 존재하고, 공백과 특수문자가 모두 제거"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6_qs6-sRAje"
      },
      "source": [
        "#정규 표현식 적용\n",
        "df['processed_text'] = df['text'].apply(lambda x:apply_regular_expression(x))\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ra_vZ8_mRVOw"
      },
      "source": [
        "## Word Count\n",
        "- 말뭉치(코퍼스) 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVFZAEZMRLw3"
      },
      "source": [
        "corpus =df['processed_text'].tolist()\n",
        "corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QhoXyfGRdBP"
      },
      "source": [
        "- BOW(bag of words) 벡터 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnGtNSyhRb39"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# filter stop words\n",
        "vect = CountVectorizer(tokenizer=None, stop_words='english', analyzer='word').fit(corpus)\n",
        "# tokenize: 문장을 단어로 나누는 기준; stop_words: 불용어 설정\n",
        "\n",
        "bow_vect = vect.fit_transform(corpus) # BoW 벡터 생성\n",
        "word_list = vect.get_feature_names()\n",
        "count_list = bow_vect.toarray().sum(axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6jaN95eRhTW"
      },
      "source": [
        "#등장한 단어 list\n",
        "word_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-RhTj6ERjAG"
      },
      "source": [
        "#각 단어의 씬별 등장 횟수\n",
        "\n",
        "bow_vect.toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ePiyrTxRnQe"
      },
      "source": [
        "bow_vect.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkmluPbIRogO"
      },
      "source": [
        "#각 단어의 총 등장 횟수(모든 씬에서의 등장 횟수의 합)\n",
        "\n",
        "count_list #BoW array의 각 Column에 대해서 모든 row의 합을 구하기"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZMidXp8RwhA"
      },
      "source": [
        "#'단어' - '총 등장 횟수' matching\n",
        "\n",
        "word_count_dict = dict(zip(word_list, count_list))\n",
        "word_count_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llUU8K7QR3or"
      },
      "source": [
        "import operator\n",
        "\n",
        "\n",
        "sorted(word_count_dict.items(), key = operator.itemgetter(1), reverse = True)[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrW6BVX6R_aa"
      },
      "source": [
        "### 단어 분포 탐색"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpMPY_tHR-u5"
      },
      "source": [
        "plt.hist(list(word_count_dict.values()),bins = 150)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22ZcIz2fSJYw"
      },
      "source": [
        "대부분의 단어가 0번~50번 사이에 등장했고, 일부 소수의 단어들이 100번 이상 등장한 것을 확인"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TP0Sl86eSOU1"
      },
      "source": [
        "# 텍스트 마이닝\n",
        "\n",
        "## 단어별 빈도 분석\n",
        " - 상위 빈도수 단어 출력"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyOInKVfSIMg"
      },
      "source": [
        "# word_count_dict중 상위 25 tags 확인해보기\n",
        "\n",
        "ranked_tags = Counter(word_count_dict).most_common(25)\n",
        "ranked_tags"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iUlSqF4SU8F"
      },
      "source": [
        " - word cloud 시각화\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sH5NzVvSTjO"
      },
      "source": [
        "!pip install pytagcloud pygame simplejson"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0HUNi0QSajx"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "import random\n",
        "import pytagcloud\n",
        "import webbrowser"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTB1QaLTShtS"
      },
      "source": [
        "# Top 40 단어로 word cloud 생성하기\n",
        "taglist = pytagcloud.make_tags(sorted(word_count_dict.items(), key = operator.itemgetter(1), reverse=True)[:40], maxsize=60)  # 빈도수(itemgetter(1)) 내림차순(reverse=True)으로 정렬, maxsize: 글자 크기\n",
        "# taglist = pytagcloud.make_tages(Counter(word_count_dict).most_common(40), maxsize=60)\n",
        "\n",
        "pytagcloud.create_tag_image(taglist, 'movie_wordcloud.jpg', rectangular=False)\n",
        "from IPython.display import Image\n",
        "Image(filename='movie_wordcloud.jpg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMq-ExrNSkri"
      },
      "source": [
        "### 장면별 중요 단어 시각화(TF-IDF)\n",
        " - BOW에 대해서 TF-IDF변환 진행\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sB2-V56pSjf8"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "tfidf_vectorizer = TfidfTransformer()\n",
        "tf_idf_vect = tfidf_vectorizer.fit_transform(bow_vect)\n",
        "\n",
        "print(tf_idf_vect.shape)  # 320*2850 vector: 320 scenes, 2850 sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUwdN3IeSrzX"
      },
      "source": [
        "# 첫번째 행 출력 (0이 아닌것 만) -- 즉 첫 씬에서 모든 단어의 TF-IDF 값\n",
        "print(tf_idf_vect[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8mCAgssSuSF"
      },
      "source": [
        "# (0을 포함한) 실제 vector의 모습 출력해보기\n",
        "print(tf_idf_vect[0].toarray().shape)\n",
        "print(tf_idf_vect[0].toarray())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "To96fF6uSxbc"
      },
      "source": [
        " - “벡터” - “단어” mapping\n",
        "\n",
        "\n",
        "길이가 2850인 단어 벡터의 각 위치가 어떤 단어를 상징하는지를 알아내기 위해 단어 벡터에 대해서 “단어” - “index No.” Mapping 을 진행합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Lqb4P4KSwAP"
      },
      "source": [
        "vect.vocabulary_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwG4drKSS1Xk"
      },
      "source": [
        "# Mapping: 단어 <-> 벡터안의 index no. \n",
        "invert_index_vectorizer = {v: k for k, v in vect.vocabulary_.items()}  # value : key\n",
        "print(str(invert_index_vectorizer)[:100]+'...')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eB5BW9qkS5wu"
      },
      "source": [
        "- 중요 단어 추출 - Top3 TF-IDF\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwY1HWfvS3BS"
      },
      "source": [
        "np.argsort(tf_idf_vect[0].toarray())[0][-3:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnAQ0LTGS-BU"
      },
      "source": [
        "#TF-IDF 적용\n",
        "\n",
        "np.argsort(tf_idf_vect.toarray())[:, -3:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XC5UBH43TBr1"
      },
      "source": [
        "top_3_words = np.argsort(tf_idf_vect.toarray())[:, -3:]\n",
        "df['important_word_index'] = pd.Series(top_3_words.tolist())\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2KmysRJTGHS"
      },
      "source": [
        "하지만 지금 중요한 단어의 index만 표시 되고, 과연 어떤 단어인지를 모릅니다. 그래서 우리는 방금 추출한 “벡터”-“단어” Mapping 결과를 이용해 index에 해당하는 단어들을 추출하여 데이터셋에 저장하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91CzAu99TDgC"
      },
      "source": [
        "# index -> word 변환함수 만들기\n",
        "\n",
        "def convert_to_word(x):\n",
        "    word_list = []\n",
        "    for index in x:\n",
        "        word_list.append(invert_index_vectorizer[index])\n",
        "    return word_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIiE-aklTIP-"
      },
      "source": [
        "df['important_words'] = df['important_word_index'].apply(lambda x: convert_to_word(x))\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yq8ju-gKTKPy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}