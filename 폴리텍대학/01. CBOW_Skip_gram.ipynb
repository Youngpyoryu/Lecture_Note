{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"SQHHITwovsZV"},"source":["# 자연어처리의 기본\n","\n","- 자연어 처리(NLP)는 컴퓨터와 인간 언어 간의 상호 작용과 관련된 언어학, 컴퓨터 과학 및 인공 지능 하위 분야로, 특히 대량의 자연어 데이터를 처리하고 분석할 수 있도록 컴퓨터를 프로그래밍하는 방법을 말한다.(Wikipedia)\n","\n","- 자연어 처리(natural language)란 사람들이 일상생활에서 자연스럽게 사용하는 언어를 말함.\n","\n","- 사람들 사이에서 이루어지는 대화는 일반적으로 자연어가 사용되지만, 컴퓨터는 그렇지 않다. 사람이 컴퓨터와 대화해주기 위해서는 정제작업이 필요\n","\n","- 컴퓨터에게 원하는 특정한 일을 시키고 싶을 때에는 컴퓨터에게 정해진 프로그래밍 언어로 작성해줘야 한다. 프로그램을 작성할 때 언어의 규칙이 정확히 지켜지지 않으면 오류가 발생하거나 원치 않는 결과는 받게 된다.\n","\n","- NLP분야는 컴퓨터가 문장어를 이해하거나 생성할 수 있도록 하는 학문.\n","\n","<img src = 'https://blog.kakaocdn.net/dn/P2jwU/btqQsucF3f4/e4JN9e8aIs0kPkmnzkB6Ek/img.png'>\n","\n","- 자연어 이해(NLU, Natural language undestanding), 자연어 생성(NLG, natural language generation) 분야가 있음.\n","\n","- NLU와 NLG는 입출력이 서로 반대기 때문에 서로 처리 방법이 다름. 연구도 다르게 진행이 됨.\n","- 음성인식과 같이 시너지 효과를 냄."]},{"cell_type":"markdown","source":["<img src='https://blog.kakaocdn.net/dn/bfMwVH/btq9I3z48QL/p6rigTHn2iq54674lRw7J1/img.png'>\n","- 데이터 정제에는 노이즈를 식별하고 제거, 문자 정규화, 데이터 마스킹, 토큰화, 품사 태깅, 개체명 인식 등이 포함되어 있다.\n","\n","- 출처 : https://yuna96.tistory.com/86"],"metadata":{"id":"xGkXrvbyBsUX"}},{"cell_type":"markdown","source":["## Word2vec\n","\n","- 원 핫 인코딩을 사용하면서도 단어 간 유사도를 반영할 수 있도록 단어의 의미를 벡터화하는 방법\n","    - 비슷한 위치에서 등장하는 단어들은 비슷한 의미를 가진다 는 분포 가설을 따르는 분산 표현 방법을 사용\n"],"metadata":{"id":"iN5SghR4Fgw0"}},{"cell_type":"markdown","source":["### CBOW(Continuous Bag Of Word)\n","\n","- 주변에 있는 단어들을 입력으로 중간에 있는 단어들을 예측하는 방법\n","    - 윈도우를 두고, 윈도우 내의 주변 단어의 벡터로 중심 단어의 벡터를 예측한다.\n","    - Skip-Gram에 비해 몇 배 빠른 훈련이 가능하며, 빈번한 단어를 예측하는 데 더 나은 정확도를 가진다.\n","<img src='https://images.velog.io/images/aqaqsubin/post/5ae18e85-f0ba-4246-9430-4ef9b5e5744a/cbow_sliding_window.jpeg'>\n","\n","- 출처 : https://velog.io/@aqaqsubin/Word2Vec-Word-to-Vector"],"metadata":{"id":"nXKdPTWAEBpT"}},{"cell_type":"markdown","source":["- Word2Vec은 입력층과 출력층 사이에 하나의 은닉층이 존재하며 이는 투사층(Projection Layer)라고 불린다.\n","\n","<img src='https://images.velog.io/images/aqaqsubin/post/4953a96e-f6af-4396-9478-67668a57899b/cbow.jpeg'>"],"metadata":{"id":"vivMWxZsPTyk"}},{"cell_type":"markdown","source":["- 출처 : https://velog.io/@aqaqsubin/Word2Vec-Word-to-Vector\n","    - 스코어를 계산하는 방법은 블로그를 참고해보자"],"metadata":{"id":"jeq___olPYdD"}},{"cell_type":"markdown","source":["### Skip-Gram\n","\n","- 중심 단어를 통해 주변에 있는 단어들을 예측하는 방법\n","    - 소량의 학습 데이터에서도 잘 동작하며, 자주 사용하지 않는 희귀한 단어를 예측할 수 있다. (하지만 계산 비용이 크다는 문제점이 있다.)\n","- 마찬가지로 중심 단어에 윈도우를 두고, 윈도우 내의 주변 단어의 임베딩 벡터를 예측\n","\n","<img src='https://images.velog.io/images/aqaqsubin/post/fd2fdb22-d883-446f-82d9-5c4568719a0c/skip_gram.jpeg'>\n","\n","- 출처 ;https://velog.io/@aqaqsubin/Word2Vec-Word-to-Vector(계산하는 방법은 블로그를 참고하자)"],"metadata":{"id":"TZvQTQYyZQij"}},{"cell_type":"markdown","source":["##### **영어 Word2Vec 만들기**\n","\n","-  Gensim은 Python 기반의 Text mining library이며, 토픽 모델링, word2vec를 지원한다.\n","- Gensim의 목적은 Topic modelling for humans로 자연어 처리에 특화"],"metadata":{"id":"dCA_EgQE6Syq"}},{"cell_type":"code","metadata":{"id":"JFTCpsw6vpoD"},"source":["import re\n","import urllib.request\n","import zipfile\n","from lxml import etree\n","from nltk.tokenize import word_tokenize, sent_tokenize\n","import nltk\n","nltk.download('punkt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 훈련 데이터 다운로드\n","# 데이터 다운로드\n","urllib.request.urlretrieve(\"https://raw.githubusercontent.com/ukairia777/tensorflow-nlp-tutorial/main/09.%20Word%20Embedding/dataset/ted_en-20160408.xml\", filename=\"ted_en-20160408.xml\")"],"metadata":{"id":"A09F_2HL87Z6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["훈련 데이터 파일은 xml 문법으로 작성되어 있어 자연어를 얻기 위해서는 전처리가 필요합니다. 얻고자 하는 실질적 데이터는 영어문장으로만 구성된 내용을 담고 있는 <content>와 </content> 사이의 내용입니다. 전처리 작업을 통해 xml 문법들은 제거하고, 해당 데이터만 가져와야 합니다. 뿐만 아니라, <content>와 </content> 사이의 내용 중에는 (Laughter)나 (Applause)와 같은 배경음을 나타내는 단어도 등장하는데 이 또한 제거해야 합니다."],"metadata":{"id":"fz6UZ0Oh9CZc"}},{"cell_type":"code","source":["#훈련 데이터 전처리\n","targetXML = open('ted_en-20160408.xml', 'r', encoding='UTF8')\n","target_text = etree.parse(targetXML)\n","\n","# xml 파일로부터 <content>와 </content> 사이의 내용만 가져온다.\n","parse_text = '\\n'.join(target_text.xpath('//content/text()'))\n","\n","# 정규 표현식의 sub 모듈을 통해 content 중간에 등장하는 (Audio), (Laughter) 등의 배경음 부분을 제거.\n","# 해당 코드는 괄호로 구성된 내용을 제거.\n","content_text = re.sub(r'\\([^)]*\\)', '', parse_text)\n","\n","# 입력 코퍼스에 대해서 NLTK를 이용하여 문장 토큰화를 수행.\n","sent_text = sent_tokenize(content_text)\n","normalized_text = []\n","\n","# 각 문장에 대해서 구두점을 제거하고, 대문자를 소문자로 변환.\n","normalized_text = []\n","for string in sent_text:\n","     tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\n","     normalized_text.append(tokens)\n","\n","# 각 문장에 대해서 NLTK를 이용하여 단어 토큰화를 수행.\n","result = [word_tokenize(sentence) for sentence in normalized_text]"],"metadata":{"id":"sT0SydGf9B1z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('총 샘플의 개수 : {}'.format(len(result)))"],"metadata":{"id":"pcwkIYhp89_A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 샘플 3개만 출력\n","for line in result[:3]:\n","    print(line)"],"metadata":{"id":"rreVzUoi9loR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Word2Vec 훈련시키기**"],"metadata":{"id":"a6zRN3Eq9pcA"}},{"cell_type":"markdown","source":["\n","\n","```\n","model = Word2Vec(sentences=food_combinations,\n","                 vector_size=args.emb_dim,\n","                 window=7,\n","                 min_count=0,\n","                 workers=4,\n","                 sg=0,\n","                 epochs=5000)\n","```\n","- 파라미터\n","1. size = 워드 벡터의 특징 값. 즉, 임베딩 된 벡터의 차원.\n","2. window = 컨텍스트 윈도우 크기\n","3. min_count = 단어 최소 빈도 수 제한 (빈도가 적은 단어들은 학습하지 않는다.)\n","4. workers = 학습을 위한 프로세스 수\n","5. sg = 0은 CBOW, 1은 Skip-gram.\n","\n"],"metadata":{"id":"-4VP5NSbAZn5"}},{"cell_type":"code","source":["from gensim.models import Word2Vec\n","from gensim.models import KeyedVectors\n","\n","model = Word2Vec(sentences=result, vector_size=100, window=5, min_count=5, workers=4, sg=0)"],"metadata":{"id":"js5iYAtz9ns8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- Word2Vec는 입력한 단어에 대해서 가장 유사한 단어들을 출력하는 model.wv.most_similar을 지원"],"metadata":{"id":"FTsa4dG0KX-x"}},{"cell_type":"code","source":["model_result = model.wv.most_similar(\"man\")\n","print(model_result)"],"metadata":{"id":"e9gxDglb9tAt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- man과 유사한 단어로 woman, guy, boy, lady, girl, gentleman, soldier, kid 등을 출력하는 것을 볼 수 있습니다. Word2Vec를 통해 단어의 유사도를 계산할 수 있게 되었습니다."],"metadata":{"id":"j3HqQSwVKxgb"}},{"cell_type":"markdown","source":["**Word2Vec 모델 저장하고 로드하기**"],"metadata":{"id":"-3URr2IFKzT9"}},{"cell_type":"code","source":["model.wv.save_word2vec_format('eng_w2v') # 모델 저장\n","loaded_model = KeyedVectors.load_word2vec_format(\"eng_w2v\") # 모델 로드"],"metadata":{"id":"NVLLkMlyJ-tI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_result = loaded_model.most_similar(\"man\")\n","print(model_result)"],"metadata":{"id":"ygC3uy2IK1y-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["한국어 Word2Vec 만들기(네이버 영화 리뷰)"],"metadata":{"id":"lh9jJSQoK4pE"}},{"cell_type":"code","source":["!pip install konlpy"],"metadata":{"id":"i8VtN32FK9N1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","import urllib.request\n","from gensim.models.word2vec import Word2Vec\n","from konlpy.tag import Okt"],"metadata":{"id":"Vm_PlfxoK3MH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings.txt\", filename=\"ratings.txt\")"],"metadata":{"id":"Xy2CPFk3K7iC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#상위 5개 출력\n","train_data = pd.read_table('ratings.txt')"],"metadata":{"id":"CAzDLIR4K8qC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data[:5] # 상위 5개 출력"],"metadata":{"id":"VFpnCYe_LDd0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#총 리뷰 개수를 확인\n","print(len(train_data)) # 리뷰 개수 출력"],"metadata":{"id":"HhXv-vAXLEiw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# NULL 값 존재 유무\n","print(train_data.isnull().values.any())"],"metadata":{"id":"cPYlQxPrLH2u"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- 결측값이 존재하므로 결측값이 존재하는 행을 제거합니다."],"metadata":{"id":"HGR5Bo6TLX0Z"}},{"cell_type":"code","source":["train_data = train_data.dropna(how = 'any') # Null 값이 존재하는 행 제거\n","print(train_data.isnull().values.any()) # Null 값이 존재하는지 확인"],"metadata":{"id":"7dpXY6OcLVtJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#결측값이 삭제된 후의 리뷰 개수를 확인\n","print(len(train_data)) # 리뷰 개수 출력"],"metadata":{"id":"DSnVypp5LZaT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 정규 표현식을 통한 한글 외 문자 제거\n","train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")"],"metadata":{"id":"AEKj2IUqOVki"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data[:5] # 상위 5개 출력"],"metadata":{"id":"keK06eGaOZ8W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tqdm import tqdm\n","# 불용어 정의\n","stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n","\n","# 형태소 분석기 OKT를 사용한 토큰화 작업 (다소 시간 소요)\n","okt = Okt()\n","\n","tokenized_data = []\n","for sentence in tqdm(train_data['document']):\n","    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n","    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n","    tokenized_data.append(stopwords_removed_sentence)"],"metadata":{"id":"H8cBA_40ObtW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 리뷰 길이 분포 확인\n","print('리뷰의 최대 길이 :',max(len(review) for review in tokenized_data))\n","print('리뷰의 평균 길이 :',sum(map(len, tokenized_data))/len(tokenized_data))\n","plt.hist([len(review) for review in tokenized_data], bins=50)\n","plt.xlabel('length of samples')\n","plt.ylabel('number of samples')\n","plt.show()"],"metadata":{"id":"DDUH8tsQOd8x"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- Word2Vec으로 토큰화 된 네이버 영화 리뷰 데이터를 학습"],"metadata":{"id":"evgpfJHDYkSH"}},{"cell_type":"code","source":["from gensim.models import Word2Vec\n","\n","model = Word2Vec(sentences = tokenized_data, vector_size = 100, window = 5, min_count = 5, workers = 4, sg = 0)"],"metadata":{"id":"J_54jJgFYhx_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 완성된 임베딩 매트릭스의 크기 확인\n","model.wv.vectors.shape"],"metadata":{"id":"CoHmuSqIYliE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(model.wv.most_similar(\"최민식\"))"],"metadata":{"id":"ZgcTzvn1Ymn6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(model.wv.most_similar(\"히어로\"))"],"metadata":{"id":"GI6inw7zYnsX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Skip-gram\n","\n","- 중심 단어(center word)로부터 주변 단어(context word)를 예측하는 방법\n","\n","<img src='https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FdkXNMB%2FbtrziwEVjNX%2FoGN75xeTOQcUMPSsmPrxzK%2Fimg.png'>"],"metadata":{"id":"ACnfZV0alv-i"}},{"cell_type":"markdown","source":["### Negative Samling\n","\n","word2vec의 CBOW와 Skip-gram 모두 단어 개수가 많아질수록 계산 복잡도가 증가하여 연산 속도가 저하된다는 한계점을 보완하기 위해 제안되었습니다.\n","\n","$L_{CBOW} = - \\sum_{j=1}^{|V|} y_j log(\\hat{y})$\n","\n","$L_{Skip-gram} = - \\sum_{j=0 j\\neq m }^{2m} \\sum_{k=1}^{|V|} y_k^{(c-j)} log(\\hat{y}_k)^{(c-j)}$\n","\n","- 수식에서 알 수 있듯이, CBOW와 Skip-gram\\는 역전파 과정에서 단어 집합의 크기(V)만큼 연산이 필요함.\n","    - 단어 개수가 많아질수록 계산 복잡도 역시 높아지고, 이는 모델 학습 속도 저하를 유발함.\n","\n","#### 개념\n","\n","- Word2Vec이 학습 과정에서 전체 단어 집합이 아니라 전체 단어 집합이 아니라 일부 단어 집합에만 집중할 수 있도록 하는 방법\n","    - 주변 단어들이 아닌 단어의 집합을 만들어 부정(Negative)으로 레이블링 하고 주변 단어들은 긍정(Postive)으로 레이블링 한 후 두 집합간 이진 분류 문제로 변환\n","\n","<img src='https://wikidocs.net/images/page/69141/%EA%B7%B8%EB%A6%BC3.PNG'>\n","\n","<img src='https://wikidocs.net/images/page/69141/%EA%B7%B8%EB%A6%BC4.PNG'>\n"],"metadata":{"id":"fkU9jjPSoK6W"}},{"cell_type":"markdown","source":["- 20뉴스그룹 데이터 전처리하기"],"metadata":{"id":"-SylQXf_PZLP"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import nltk\n","from nltk.corpus import stopwords\n","from sklearn.datasets import fetch_20newsgroups\n","from tensorflow.keras.preprocessing.text import Tokenizer"],"metadata":{"id":"2vFR84tkZB1_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["20뉴스그룹 데이터를 사용합니다. 이번 실습에서는 하나의 샘플에 최소 단어 2개는 있어야 합니다. 그래야만 중심 단어, 주변 단어의 관계가 성립하며 그렇지 않으면 샘플을 구성할 수 없어 에러가 발생합니다. 전처리 과정에서 지속적으로 이를 만족하지 않는 샘플들을 제거"],"metadata":{"id":"Ai4kGhOPaXYR"}},{"cell_type":"code","source":["dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n","documents = dataset.data\n","print('총 샘플 수 :',len(documents))"],"metadata":{"id":"4bxGJ4JfPbVy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["-  불필요한 토큰을 제거하고, 소문자화를 통해 정규화를 진행"],"metadata":{"id":"NOSomcglah8J"}},{"cell_type":"code","source":["news_df = pd.DataFrame({'document':documents})\n","# 특수 문자 제거\n","news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z]\", \" \")\n","# 길이가 3이하인 단어는 제거 (길이가 짧은 단어 제거)\n","news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n","# 전체 단어에 대한 소문자 변환\n","news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower())"],"metadata":{"id":"09DOXfaHaaRG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#결측치 확인\n","news_df.isnull().values.any()"],"metadata":{"id":"xUh_D8heajTP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#빈 값(empy) 유무도 확인\n","news_df.replace(\"\", float(\"NaN\"), inplace=True)\n","news_df.isnull().values.any()"],"metadata":{"id":"Jn3aH8njalZk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Null 값이 있음을 확인했습니다. Null 값을 제거\n","news_df.dropna(inplace=True)\n","print('총 샘플 수 :',len(news_df))"],"metadata":{"id":"eVGmBxvranzH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import nltk\n","nltk.download('stopwords')\n","stop_words = stopwords.words('english')\n","tokenized_doc = news_df['clean_doc'].apply(lambda x: x.split())\n","tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])\n","tokenized_doc = tokenized_doc.to_list()"],"metadata":{"id":"UFIF1JKGaqIC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- 불용어를 제거하였으므로 단어의 수가 줄어들었습니다. 모든 샘플 중 단어가 1개 이하인 경우를 모두 찾아 제거"],"metadata":{"id":"gjrrV2doayh4"}},{"cell_type":"code","source":["# 단어가 1개 이하인 샘플의 인덱스를 찾아서 저장하고, 해당 샘플들은 제거.\n","drop_train = [index for index, sentence in enumerate(tokenized_doc) if len(sentence) <= 1]\n","tokenized_doc = np.delete(np.array(tokenized_doc, dtype=object), drop_train, axis=0)\n","print('총 샘플 수 :',len(tokenized_doc))"],"metadata":{"id":"q76ObDngasWG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- numpy 에러(2024.04.17) : https://velog.io/@setxty/numpy-array-%EB%B3%80%ED%99%98-%EC%8B%9C-Error"],"metadata":{"id":"YtaBHChwaVxq"}},{"cell_type":"markdown","source":["`[index for index, sentence in enumerate(tokenized_doc) if len(sentence) >= 2] ` 이렇게 진행하여도 됨, array로 바꾸시면 됩니다."],"metadata":{"id":"hn2369WucbPr"}},{"cell_type":"markdown","source":["- 단어 집합을 생성하고, 정수 인코딩을 진행"],"metadata":{"id":"Szv40c2Xa1wG"}},{"cell_type":"code","source":["tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(tokenized_doc)\n","\n","word2idx = tokenizer.word_index\n","idx2word = {value : key for key, value in word2idx.items()}\n","encoded = tokenizer.texts_to_sequences(tokenized_doc)"],"metadata":{"id":"aI-qSpSkazyG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#상위 2개의 샘플을 출력\n","print(encoded[:2])"],"metadata":{"id":"q6aXRGHxa2xk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#단어 집합의 크기 확인\n","vocab_size = len(word2idx) + 1\n","print('단어 집합의 크기 :', vocab_size)"],"metadata":{"id":"_4YynhCXa5C7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### negative sampling"],"metadata":{"id":"xm0zhuq-a7rM"}},{"cell_type":"code","source":["from tensorflow.keras.preprocessing.sequence import skipgrams\n","# 네거티브 샘플링\n","skip_grams = [skipgrams(sample, vocabulary_size=vocab_size, window_size=10) for sample in encoded[:10]]"],"metadata":{"id":"ZMOsnDdja7KA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- 10개의 뉴스그룹 샘플에 대해서 모두 수행되었지만, 첫번째 뉴스그룹 샘플에 대해서만 확인"],"metadata":{"id":"x26r8ad9bFol"}},{"cell_type":"code","source":["# 첫번째 샘플인 skip_grams[0] 내 skipgrams로 형성된 데이터셋 확인\n","pairs, labels = skip_grams[0][0], skip_grams[0][1]\n","for i in range(5):\n","    print(\"({:s} ({:d}), {:s} ({:d})) -> {:d}\".format(\n","          idx2word[pairs[i][0]], pairs[i][0],\n","          idx2word[pairs[i][1]], pairs[i][1],\n","          labels[i]))"],"metadata":{"id":"Y7BUpnhpa_ic"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- 윈도우 크기 내에서 중심 단어, 주변 단어의 관계를 가지는 경우에는 1의 레이블을 갖도록 하고, 그렇지 않은 경우는 0의 레이블을 가지도록 하여 데이터셋을 구성"],"metadata":{"id":"lsfjtp3xbNnc"}},{"cell_type":"code","source":["print('전체 샘플 수 :',len(skip_grams))"],"metadata":{"id":"QyC2lE-XbG7U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- encoded 중 상위 10개의 뉴스그룹 샘플에 대해서만 수행하였으므로 10이 출력됩니다. 그리고 10개의 뉴스그룹 샘플 각각은 수많은 중심 단어, 주변 단어의 쌍으로 된 샘플들을 갖고 있음"],"metadata":{"id":"kAcZdXozbT6X"}},{"cell_type":"code","source":["# 첫번째 뉴스그룹 샘플에 대해서 생긴 pairs와 labels의 개수\n","print(len(pairs))\n","print(len(labels))"],"metadata":{"id":"UV9RqmGzbOjb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#모든 뉴스그룹 샘플에 대해서 수행\n","skip_grams = [skipgrams(sample, vocabulary_size=vocab_size, window_size=10,seed=42) for sample in encoded]"],"metadata":{"id":"gMhaX5zebc-x"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- Skip-Gram with Negative Sampling(SGNS) 구현"],"metadata":{"id":"lchbSBl0bgnl"}},{"cell_type":"code","source":["from tensorflow.keras.models import Sequential, Model\n","from tensorflow.keras.layers import Embedding, Reshape, Activation, Input\n","from tensorflow.keras.layers import Dot\n","from tensorflow.keras.utils import plot_model\n","from IPython.display import SVG"],"metadata":{"id":"dy7NCv3HbfsS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- 하이퍼파라미터인 임베딩 벡터의 차원은 100으로 정하고, 두 개의 임베딩 층을 추가"],"metadata":{"id":"1Pv5iGzVbnPN"}},{"cell_type":"code","source":["embedding_dim = 100\n","\n","# 중심 단어를 위한 임베딩 테이블\n","w_inputs = Input(shape=(1, ), dtype='int32')\n","word_embedding = Embedding(vocab_size, embedding_dim)(w_inputs)\n","\n","# 주변 단어를 위한 임베딩 테이블\n","c_inputs = Input(shape=(1, ), dtype='int32')\n","context_embedding  = Embedding(vocab_size, embedding_dim)(c_inputs)"],"metadata":{"id":"jjYchkJublue"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- 각 임베딩 테이블은 중심 단어와 주변 단어 각각을 위한 임베딩 테이블이며 각 단어는 임베딩 테이블을 거쳐서 내적을 수행하고, 내적의 결과는 1 또는 0을 예측하기 위해서 시그모이드 함수를 활성화 함수로 거쳐 최종 예측값을 얻습니다."],"metadata":{"id":"gyG3LElHbtFr"}},{"cell_type":"code","source":["dot_product = Dot(axes=2)([word_embedding, context_embedding])\n","dot_product = Reshape((1,), input_shape=(1, 1))(dot_product)\n","output = Activation('sigmoid')(dot_product)\n","\n","model = Model(inputs=[w_inputs, c_inputs], outputs=output)\n","model.summary()\n","model.compile(loss='binary_crossentropy', optimizer='adam')\n","plot_model(model, to_file='model3.png', show_shapes=True, show_layer_names=True, rankdir='TB')"],"metadata":{"id":"L3oOaVhGbry3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tqdm import tqdm\n","for epoch in tqdm(range(1, 6)):\n","    loss = 0\n","    for _, elem in enumerate(skip_grams):\n","        first_elem = np.array(list(zip(*elem[0]))[0], dtype='int32')\n","        second_elem = np.array(list(zip(*elem[0]))[1], dtype='int32')\n","        labels = np.array(elem[1], dtype='int32')\n","        X = [first_elem, second_elem]\n","        Y = labels\n","        loss += model.train_on_batch(X,Y)\n","    print('Epoch :',epoch, 'Loss :',loss)"],"metadata":{"id":"dZEPFsz7buXW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- 결과 확인하기"],"metadata":{"id":"pVM61CYSbvze"}},{"cell_type":"code","source":["import gensim\n","\n","f = open('vectors.txt' ,'w')\n","f.write('{} {}\\n'.format(vocab_size-1, embedding_dim))\n","vectors = model.get_weights()[0]\n","for word, i in tokenizer.word_index.items():\n","    f.write('{} {}\\n'.format(word, ' '.join(map(str, list(vectors[i, :])))))\n","f.close()\n","\n","# 모델 로드\n","w2v = gensim.models.KeyedVectors.load_word2vec_format('./vectors.txt', binary=False)"],"metadata":{"id":"vFOZbRINbvez"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["w2v.most_similar(positive=['soldiers'])"],"metadata":{"id":"OQxsqUzBb0Fs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["w2v.most_similar(positive=['doctor'])"],"metadata":{"id":"ta4bhl6cb2kZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["w2v.most_similar(positive=['police'])"],"metadata":{"id":"MZSTG8Hrb45U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["w2v.most_similar(positive=['knife'])"],"metadata":{"id":"hyUQkXblcABk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["w2v.most_similar(positive=['engine'])"],"metadata":{"id":"ya94n7orcBlp"},"execution_count":null,"outputs":[]}]}