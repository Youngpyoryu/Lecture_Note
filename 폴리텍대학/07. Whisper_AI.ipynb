{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0dda54d0e7374b73920e3b89057c4c2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c6f2203831f44a92b825ba62164bf0e4",
              "IPY_MODEL_435a230064914b9294a736063f3c1301",
              "IPY_MODEL_c267c5526ff24650a23df32ab5b034e0",
              "IPY_MODEL_d284dcb5fb904dd999f90cac16d036a2",
              "IPY_MODEL_392a8ff8fd564bdc9b5863e06756a896"
            ],
            "layout": "IPY_MODEL_90f65b7913324eb79cac50a4fd354b04"
          }
        },
        "c6f2203831f44a92b825ba62164bf0e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6bd0f7e63dd446f9dfac604cd5f9b26",
            "placeholder": "​",
            "style": "IPY_MODEL_f67d8f33aaf44608abeeafae916ecdb4",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "435a230064914b9294a736063f3c1301": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_9d9dfc8172534740a578c1002e1da738",
            "placeholder": "​",
            "style": "IPY_MODEL_86c01dbc856748d196554b78430a3307",
            "value": ""
          }
        },
        "c267c5526ff24650a23df32ab5b034e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_b96eefa0f3b248a2948a6187ea51d104",
            "style": "IPY_MODEL_ba916ede89e24313810e66a31b30af66",
            "value": true
          }
        },
        "d284dcb5fb904dd999f90cac16d036a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_8dd4f7519e5346fcb071e56a497db15d",
            "style": "IPY_MODEL_ad100018de34410aa458d936eb21fa59",
            "tooltip": ""
          }
        },
        "392a8ff8fd564bdc9b5863e06756a896": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cfaf5ab0cdd846f6b378f3cd86b51135",
            "placeholder": "​",
            "style": "IPY_MODEL_d629024b16ca40658d0380fd0b59437e",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "90f65b7913324eb79cac50a4fd354b04": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "e6bd0f7e63dd446f9dfac604cd5f9b26": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f67d8f33aaf44608abeeafae916ecdb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9d9dfc8172534740a578c1002e1da738": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86c01dbc856748d196554b78430a3307": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b96eefa0f3b248a2948a6187ea51d104": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba916ede89e24313810e66a31b30af66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8dd4f7519e5346fcb071e56a497db15d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad100018de34410aa458d936eb21fa59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "cfaf5ab0cdd846f6b378f3cd86b51135": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d629024b16ca40658d0380fd0b59437e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Whisper란?\n",
        "\n",
        "**openai에서 공개한 인공지능 모델로 음성을 텍스트로 변환할 수 있는 기술이다. Speech to Text (STT)를 인공지능으로 가능하게 합니다. 무료로 공개했으며 github에 코드가 올라와 있어 누구나 사용할 수 있다.**\n",
        "\n",
        "**자동 음성 인식 Whisper는 딥 러닝을 기반으로 개발된 음성 인식 AI입니다. Whisper는 녹음된 음성 파일을 인식하고 텍스트로 변환할 수 있습니다. Whisper는 배경 소음이 많은 환경에서도 정확하게 음성을 인식할 수 있습니다. Whisper 또한 GPT 기반으로 개발된 음성 인식 모델로 음성을 텍스트로 변환할 수 있는 소프트웨어입니다.\n",
        "특히 놀라운 점은 정말 다양한 국적의 언어를 지원한다는 점인데요, 음성을 통해 텍스트를 생성해야 하는 작업이 필요하다면 무료이면서 효과적으로 이 소프트웨어를 활용할 수 있을 것으로 생각됩니다.**\n",
        "\n",
        "\n",
        "- STT : Speech To Text / 음성 데이터를 텍스트 데이터로 변환하는 기술\n",
        "- TTS : Text TO Speech / 텍스트 형태의 데이터를 기반으로 음성 데이터로 변환하는 기술\n",
        "\n",
        "\n",
        "참고 : https://github.com/openai/whisper\n",
        "\n",
        "\n",
        "<img src='https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbINoTH%2FbtsijgFAgik%2FOKAzIBZafUk3mkrPUW4jok%2Fimg.jpg'>\n",
        "\n",
        "출처 : https://jpjp9292.tistory.com/12\n",
        "\n",
        "Whisper는 2022년 9월 Open AI에서 개발한 자동음성인식(ASR: Automatic Speech Recognition) 모델입니다. 음성데이터를 인식 후 문자로 변환해 주는 메커니즘을 갖추었습니다. 구체적으로 Whisper 모델의 특징을 아키텍처와 코드적용을 통해 확인해 보겠습니다.\n",
        "\n",
        " 참고 : https://jpjp9292.tistory.com/12"
      ],
      "metadata": {
        "id": "RDfky-Nrb4M1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "학습데이터를 무려 680,000시간을 사용하여 학습한 모델을 공개하였습니다.\n",
        "\n",
        "사용된 데이터의 양을 보면 아래 그림과 같이 다양한 언어를 학습을 시켰는데요. 그중에 특히 한국어가 약 8,000시간, 영어 제외 언어로는 7번째 많은 양의 데이터를 학습을 시켰습니다.\n",
        "\n",
        "<img src='https://devocean.sk.com/editorImg/2023/2/17/71d6978f83abdf5df8c189ee117e3d4a741004f07af23fbb253d402550b6a1d4'>\n",
        "\n",
        "<img src='https://devocean.sk.com/editorImg/2023/2/17/e9e4e55a2bd1b992445f560ff6dd927940e941994e0c047a411226ef1a62b1e6'>"
      ],
      "metadata": {
        "id": "c4wVlRSGjQhW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://devocean.sk.com/editorImg/2023/2/17/1b4cf5796a32d8caa00faf0bf4cd6a470955d491795060fa70dfedc3170a0b2d'>"
      ],
      "metadata": {
        "id": "slm_4o6zk5H7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "참고 : https://sooeun67.github.io/data%20science/openai-api-on-colab/"
      ],
      "metadata": {
        "id": "xIyzCRfymJ-B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "코드 출처 : https://velog.io/@mino0121/NLP-OpenAI-Whisper-Fine-tuning-for-Korean-ASR-with-HuggingFace-Transformers"
      ],
      "metadata": {
        "id": "n_T0yGQ39iew"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "참고 : https://github.com/openai/whisper/blob/main/notebooks/Multilingual_ASR.ipynb?ref=assemblyai.com"
      ],
      "metadata": {
        "id": "P2ENAw-tpLwm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OpenAI Whisper Fine-tuning for Korean ASR with HuggingFace Transformers"
      ],
      "metadata": {
        "id": "M-63RTbwl8PR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "본격적인 작업에 앞서, Colab을 사용하여 본 작업을 수행한다면 데이터의 전처리 작업까지는 CPU를 활용하는 것을 권장한다. 데이터 전처리 후 Huggingface에 전처리된 데이터셋을 업로드하고, 그 후에 런타임 유형을 GPU로 변환하여 전처리된 데이터셋을 로드, 모델의 학습을 수행한다면 Colab 컴퓨팅 자원의 소모를 방지할 수 있을 것이다."
      ],
      "metadata": {
        "id": "h-HN1eiHllNl"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxQB4IEFz5qo"
      },
      "source": [
        "### Prepare Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGDyAhq7zoc0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bce5f5ae-5bf0-47d4-d3b1-6cbc21d54db2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/huggingface/transformers\n",
            "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-vtkuh4kg\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-vtkuh4kg\n",
            "  Resolved https://github.com/huggingface/transformers to commit 8c12690cecbb97e187861e386f7a0ac790e4236c\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.41.0.dev0) (3.13.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers==4.41.0.dev0) (0.22.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.41.0.dev0) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.41.0.dev0) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.41.0.dev0) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.41.0.dev0) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.41.0.dev0) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers==4.41.0.dev0) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.41.0.dev0) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.41.0.dev0) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.41.0.dev0) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.41.0.dev0) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.41.0.dev0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.41.0.dev0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.41.0.dev0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.41.0.dev0) (2024.2.2)\n",
            "Requirement already satisfied: jiwer in /usr/local/lib/python3.10/dist-packages (3.0.3)\n",
            "Requirement already satisfied: click<9.0.0,>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from jiwer) (8.1.7)\n",
            "Requirement already satisfied: rapidfuzz<4,>=3 in /usr/local/lib/python3.10/dist-packages (from jiwer) (3.8.1)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.29.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.2.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.22.2)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.41.0.dev0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.13.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.22.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.2.1+cu121)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.29.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.1.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->transformers[torch]) (12.4.127)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->transformers[torch]) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->transformers[torch]) (1.3.0)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.16.6)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.45.0)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets>=2.6.1\n",
        "!pip install git+https://github.com/huggingface/transformers\n",
        "!pip install evaluate>=0.30\n",
        "!pip install jiwer\n",
        "!pip install accelerate -U\n",
        "!pip install transformers[torch]\n",
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331,
          "referenced_widgets": [
            "0dda54d0e7374b73920e3b89057c4c2c",
            "c6f2203831f44a92b825ba62164bf0e4",
            "435a230064914b9294a736063f3c1301",
            "c267c5526ff24650a23df32ab5b034e0",
            "d284dcb5fb904dd999f90cac16d036a2",
            "392a8ff8fd564bdc9b5863e06756a896",
            "90f65b7913324eb79cac50a4fd354b04",
            "e6bd0f7e63dd446f9dfac604cd5f9b26",
            "f67d8f33aaf44608abeeafae916ecdb4",
            "9d9dfc8172534740a578c1002e1da738",
            "86c01dbc856748d196554b78430a3307",
            "b96eefa0f3b248a2948a6187ea51d104",
            "ba916ede89e24313810e66a31b30af66",
            "8dd4f7519e5346fcb071e56a497db15d",
            "ad100018de34410aa458d936eb21fa59",
            "cfaf5ab0cdd846f6b378f3cd86b51135",
            "d629024b16ca40658d0380fd0b59437e"
          ]
        },
        "id": "kvDH0hS-z_Bv",
        "outputId": "3023db64-3589-4ecb-df7b-8b2581d7314e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0dda54d0e7374b73920e3b89057c4c2c"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# 훈련이 끝난 모델을 HuggingFace Hub에 업로드하기 위해 로그인\n",
        "# 비공개 혹은 제한된 공개의 데이터셋에 접근할 경우에도 로그인이 필요하다.\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pu1C3VJC0i6N"
      },
      "source": [
        "### Prepare Feature Extractor and Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldqeYHRg1Lnq"
      },
      "source": [
        "음성 인식 Pipeline은 다음과 같이 세 가지 구성으로 나눌 수 있다.\n",
        "1. 오디오 raw data에 대하여 pre-process를 진행할 feature extractor\n",
        "2. sequence-to-sequence 매핑을 수행할 model\n",
        "3. model의 output을 text 포맷으로 후처리할 tokenizer\n",
        "\n",
        "HuggingFace의 Transformers는 Whisper의 모델과 함께 이와 연관된 WhisperFeatureExtractor와 WhisperTokenizer를 함께 제공하고 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXjUUiyt2StH"
      },
      "source": [
        "#### Load WhisperFeatureExtractor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOikJDfl2dEz"
      },
      "source": [
        "발화는 시간에 따라 변화하는 1차원 배열로 표현된다. 각 시계열별로 입력된 배열의 값은 그 순간의 신호가 갖는 진폭이다. 그 진폭의 정보만으로, 우리는 음성 데이터의 주파수 스펙트럼을 재구성하고 모든 음향적 특징을 복원할 수 있다.\n",
        "\n",
        "발화는 연속적이므로, 진폭의 값은 무한하다. 그리고 이는 유한한 배열을 기대하는 컴퓨터 장치에 문제를 야기한다. 따라서, 우리는 고정된 시간 간격에 따라 신호에서 값을 샘플링하여 우리의 음성 신호를 분할한다. 오디오를 샘플링하는 간격은 sampling rate로 알려져 있으며, 이는 일반적으로 '샘플/초' 또는 헤르츠(Hz) 단위로 측정된다. 높은 sample rate로 샘플링하는 것은 연속적인 발화 신호에 대하여 더 나은 접근을 가능케 하지만, 초당 더 많은 값을 저장하도록 요구하기도 한다.\n",
        "\n",
        "우리가 가진 오디오 데이터의 sampling rate를 우리가 사용할 모델이 기대하는 sampling rate와 일치시키는 것은 매우 중요한데, 서로 다른 sampling rate의 오디오 신호는 매우 다른 분포를 보이기 때문이다. 오디오 신호는 적절한 sampling rate로 처리되어야만 한다. 그렇지 않으면 예상치 못한 결과를 야기할 수 있다. 예를 들어, 16kHz로 샘플링된 오디오 샘플을 8kHz의 sampling rate로 들으면 해당 오디오의 속도가 절반으로 들릴 것이다. 동일한 방식으로, 잘못된 sampling rate의 오디오 데이터를 ASR 모델에 넘길 경우 모델이 불안정해질 수 있다. Whisper Feature Extractor는 16kHz의 오디오 데이터를 요구하며, 우리는 우리가 가진 오디오 데이터를 이에 맞추어야 한다.\n",
        "\n",
        "Whisper Feature Extractor는 두 가지 동작을 수행한다. 먼저 오디오 샘플에 대하여 padding 및 trucating을 진행하여 모든 샘플들의 길이를 30초로 맞춘다. 30초보다 짧은 샘플은 0으로 채워 30초로 맞추고(음성 신호에서 0은 신호 없음 혹은 침묵을 의미한다), 30초보다 긴 샘플들은 30초로 잘라낸다. 이처럼 인풋 단계에서 각 배치의 모든 요소들이 최대 길이로 채워지거나 잘라지기 때문에, 우리는 Whisper 모델에 데이터를 전달할 때 Attention mask를 필요로 하지 않는다. Whisper 모델은 이 부분에서 독특한데, 대부분의 오디오 모델은 sequence가 패딩된 위치와 self-attention 메커니즘이 어느 부분을 무시해야 하는지에 대한 상세 정보를 제공하는 attention mask를 필요로 한다. 그러나 Whisper는 이러한 attention mask 없이 발화 신호로부터 무시해야 할 인풋을 직접 추론하도록 학습되었다.\n",
        "\n",
        "Whisper Feature Extractor의 두 번째 동작은 피댕된 오디오 배열을 log-Mel 스펙트럼으로 변환하는 것이다. 이 스펙트럼은 푸리에 변환과 같이 주파수 신호를 시각적으로 나타낸 것이다.Whisper 모델의 인풋으로 주어져야 하는 것이 바로 이 log-Mel 스펙트럼이다.\n",
        "\n",
        "Transformer의 Whisper Feature Extractor는 패딩과 스펙트로그램 변환을 단 한 줄의 코드로 수행할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MOWGniR-eIl",
        "outputId": "67083bb0-4130-4881-ff53-8f1134330206"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import WhisperFeatureExtractor\n",
        "# 파인튜닝을 진행하고자 하는 모델의 feature extractor를 로드\n",
        "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-base\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7q8ht02_frC"
      },
      "source": [
        "#### Load WhisperTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LIO59vx_iby"
      },
      "source": [
        "Whisper 모델의 아웃풋은 vocabulary item들의 단어 중 예측된 텍스트를 나타내는 index값이다. Tokenizer는 이러한 텍스트 토큰 시퀀스와 실제 텍스트 문자열을 매핑해준다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r75NnnX8AlmR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a27e39d-6e6e-4a4a-eae3-38acb9726083"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "from transformers import WhisperTokenizer\n",
        "# 파인튜닝을 진행하고자 하는 모델의 tokenizer를 로드\n",
        "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-base\", language=\"Korean\", task=\"transcribe\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ngr5ehEEE7Xr"
      },
      "source": [
        "WhisperTokenizer는 인코딩 과정에서 'Special token'들을 부여한다. 여기에는 문장의 시작과 끝을 나타내는 토큰(전사의 시작과 끝을 나타내는 토큰을 포함한다), 언어를 나타내는 토큰, 태스크(전사, 번역 등)를 나타내는 토큰 등이 있다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrDi4NAgE7dK",
        "outputId": "65d2414b-4aef-4bbe-f4f2-d33906557e10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:                 저는 서울중앙지검 지능범죄수사팀 최인호 검사입니다.\n",
            "Decoded w/ special:    <|startoftranscript|><|ko|><|transcribe|><|notimestamps|>저는 서울중앙지검 지능범죄수사팀 최인호 검사입니다.<|endoftext|>\n",
            "Decoded w/out special: 저는 서울중앙지검 지능범죄수사팀 최인호 검사입니다.\n",
            "Are equal:             True\n"
          ]
        }
      ],
      "source": [
        "input_str = \"저는 서울중앙지검 지능범죄수사팀 최인호 검사입니다.\"\n",
        "labels = tokenizer(input_str).input_ids\n",
        "decoded_with_special = tokenizer.decode(labels, skip_special_tokens=False)\n",
        "decoded_str = tokenizer.decode(labels, skip_special_tokens=True)\n",
        "\n",
        "\n",
        "print(f\"Input:                 {input_str}\")\n",
        "print(f\"Decoded w/ special:    {decoded_with_special}\")\n",
        "print(f\"Decoded w/out special: {decoded_str}\")\n",
        "print(f\"Are equal:             {input_str == decoded_str}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzE75HKSF8Fj"
      },
      "source": [
        "#### Combine To Create a WhisperProcessor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3ohliyiGHAa"
      },
      "source": [
        "Feature Extactor와 Tokenizer를 간편히 사용하기 위해, 두 모듈을 WhisperProcessor 클래스 하나로 묶을 수 있다. Processor는 Feature Extactor와 Tokenizer를 상속하며, 오디오 인풋의 입력과 모델의 예측에 사용할 수 있다. 이로써 우리는 훈련 과정에서 Processor와 Model이라는 두 개의 객체만 추적하면 된다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wT3p5GO3G14L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc28f47d-daf7-4a52-ef57-d36540594818"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "from transformers import WhisperProcessor\n",
        "\n",
        "processor = WhisperProcessor.from_pretrained(\"openai/whisper-base\", language=\"Korean\", task=\"transcribe\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYvRvgvxG6Sx"
      },
      "source": [
        "### Prepare Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVvcgSQCI8qW"
      },
      "source": [
        "보유한 음성 데이터와 이에 매치되는 텍스트 전사 데이터를 모델에 입력할 수 있는 형태로 변환해야 한다. 앞서 언급했듯 WhisperFeatureExtractor는 16kHz의 오디오 데이터를 입력으로 받으므로 이에 맞추어 오디오 데이터를 리샘플링해야 한다.\n",
        "\n",
        "본 프로젝트에서는 AI-HUB의 '[저음질 전화 음성인식 데이터](https://www.aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&dataSetSn=571)'의 일부를 활용한다. 이 데이터셋은 각 문장별로 음성 데이터(wav 파일)와 해당 음성 데이터의 전사 데이터(text 파일)이 같은 파일명으로 매치되어 있으며, labeled data의 경우 해당 통화 내용의 전체 텍스트가 정리된 json 파일이 1개씩 포함되어 있다. 자세한 사항은 링크를 통해 확인하자.\n",
        "\n",
        "이 데이터에 대하여 수행해야 할 전처리를 단계별로 구분하면 다음과 같다.\n",
        "\n",
        "1. 'idx, 오디오 파일 경로, transcription' 순으로 하나의 인스턴스가 만들어지도록 drive 내 파일들을 취합하여 DataFrame으로 만든다.\n",
        "2. 오디오 파일 경로를 이용해 sampling rate를 확인하고, 필요할 경우 16kHz로 리샘플링한다.\n",
        "3. 상기의 DataFrame을 input(오디오 경로)과 label(transcription)로 분류하여 x, y 데이터로 분할한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqmQTFsuV8qA"
      },
      "source": [
        "#### 파일 경로 취합"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izh5If2Xq875"
      },
      "outputs": [],
      "source": [
        "# 1. 오디오 파일 경로 취합\n",
        "import glob\n",
        "\n",
        "path = \"오디오 파일들이 포함된 경로를 입력한다. - 예) /content/drive/MyDrive/NLP_Project_data/raw_data/*\"\n",
        "raw_data_list = glob.glob(path)\n",
        "raw_data_list = sorted(raw_data_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8hl9gyquKvi"
      },
      "outputs": [],
      "source": [
        "print(f\"file_list : {raw_data_list[:10]}\")\n",
        "print(len(raw_data_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iN8xV58rv8Gt"
      },
      "outputs": [],
      "source": [
        "# 2. 텍스트 파일 경로 취합\n",
        "path = \"텍스트 파일들이 포함된 경로를 입력한다. - 예) /content/drive/MyDrive/NLP_Project_data/labeled_data/*\"\n",
        "labeled_data_list = glob.glob(path)\n",
        "# 레이블 데이터에는 json 데이터가 폴더별로 하나씩 있으므로 txt 파일만을 골라낸다.\n",
        "labeled_data_list = sorted([file for file in labeled_data_list if file.endswith(\".txt\")])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xVm31tbEwCkQ"
      },
      "outputs": [],
      "source": [
        "print(f\"file_list : {labeled_data_list[:10]}\")\n",
        "print(len(labeled_data_list))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yzExkfr1m3K"
      },
      "source": [
        "> 텍스트 데이터는 파일의 내용을 추출하여 데이터프레임으로 만든다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCClTGkqIJcR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f217505-2162-4eb0-e8ed-cd5869b5f412"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0it [00:00, ?it/s]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "transcript_list = []\n",
        "for labeled_data in tqdm(labeled_data_list):\n",
        "    with open(labeled_data, 'r', encoding='UTF8') as f:\n",
        "        line = f.readline()\n",
        "        transcript_list.append(line)\n",
        "\n",
        "df = pd.DataFrame(data=transcript_list, columns = [\"transcript\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMWg4EJnRSGh"
      },
      "outputs": [],
      "source": [
        "# 텍스트 데이터로 만든 데이터프레임에 음성 파일 경로 컬럼을 추가\n",
        "df[\"raw_data\"] = raw_data_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vB-id8psrzC6"
      },
      "outputs": [],
      "source": [
        "# 필요에 따라 데이터프레임을 csv 파일로 저장한다.\n",
        "df.to_csv(\"./path_and_transcript.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8ZA01YIW-2a"
      },
      "outputs": [],
      "source": [
        "# csv 파일을 로드하여 df로 변환\n",
        "df = pd.read_csv(\"./path_and_transcript.csv\", index_col=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "nW_PyfNqXT1k",
        "outputId": "d3888c77-952d-4c3c-9a38-c568ff1716ad"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [transcript, raw_data]\n",
              "Index: []"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-51978a0d-94a7-4162-9c70-bb9e712ab636\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>transcript</th>\n",
              "      <th>raw_data</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-51978a0d-94a7-4162-9c70-bb9e712ab636')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-51978a0d-94a7-4162-9c70-bb9e712ab636 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-51978a0d-94a7-4162-9c70-bb9e712ab636');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_1a0b36d5-0c42-4fb5-94d1-976727b0656e\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_1a0b36d5-0c42-4fb5-94d1-976727b0656e button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "repr_error": "Out of range float values are not JSON compliant: nan"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zXGWsIiXjG7",
        "outputId": "4203bab3-c5bf-4f84-d20b-c9dacdc48755"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "# Null data 유무 확인\n",
        "df.isnull().values.sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hTUwjCDAw6K"
      },
      "source": [
        "#### Transformer Dataset 만들기"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "취합한 데이터프레임을 Transformers의 DatasetDict로 만들어 두면 해당 데이터를 언제 어디서든 자유롭게 활용할 수 있다. 물론 컴퓨팅 환경이 적합하다면 오프라인이 훨씬 빠르겠지만, 그렇지 않다면 고려해볼 수 있는 사항이다."
      ],
      "metadata": {
        "id": "2vCVgBuQj7QB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UWz8v1DRAxA1"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset, DatasetDict\n",
        "from datasets import Audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "reyQMzWDAxFy"
      },
      "outputs": [],
      "source": [
        "# 오디오 파일 경로를 dict의 \"audio\" 키의 value로 넣고 이를 데이터셋으로 변환\n",
        "# 이때, Whisper가 요구하는 사양대로 Sampling rate는 16,000으로 설정한다.\n",
        "ds = Dataset.from_dict({\"audio\": [path for path in df[\"raw_data\"]],\n",
        "                       \"transcripts\": [transcript for transcript in df[\"transcript\"]]}).cast_column(\"audio\", Audio(sampling_rate=16000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UM22hDAuBEBe"
      },
      "outputs": [],
      "source": [
        "# 데이터셋을 훈련 데이터와 테스트 데이터, 밸리데이션 데이터로 분할\n",
        "train_testvalid = ds.train_test_split(test_size=0.2)\n",
        "test_valid = train_testvalid[\"test\"].train_test_split(test_size=0.5)\n",
        "datasets = DatasetDict({\n",
        "    \"train\": train_testvalid[\"train\"],\n",
        "    \"test\": test_valid[\"test\"],\n",
        "    \"valid\": test_valid[\"train\"]})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTSv8hVyuPXg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "outputId": "685369a7-e5a2-4d3b-998d-4728ea83cd33"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "HFValidationError",
          "evalue": "Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: '업로드할 허깅페이스 주소 입력'.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-9bfd46d03736>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 작성한 데이터셋을 허깅페이스에 업로드\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush_to_hub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"업로드할 허깅페이스 주소 입력\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/dataset_dict.py\u001b[0m in \u001b[0;36mpush_to_hub\u001b[0;34m(self, repo_id, config_name, set_default, data_dir, commit_message, commit_description, private, token, revision, branch, create_pr, max_shard_size, num_shards, embed_external_files)\u001b[0m\n\u001b[1;32m   1701\u001b[0m         \u001b[0mapi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHfApi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHF_ENDPOINT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1703\u001b[0;31m         repo_url = api.create_repo(\n\u001b[0m\u001b[1;32m   1704\u001b[0m             \u001b[0mrepo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1705\u001b[0m             \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m         ):\n\u001b[1;32m    110\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"repo_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"from_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"to_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m                 \u001b[0mvalidate_repo_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"token\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marg_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mREPO_ID_REGEX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         raise HFValidationError(\n\u001b[0m\u001b[1;32m    166\u001b[0m             \u001b[0;34m\"Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0;34m\" forbidden, '-' and '.' cannot start or end the name, max length is 96:\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHFValidationError\u001b[0m: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: '업로드할 허깅페이스 주소 입력'."
          ]
        }
      ],
      "source": [
        "# 작성한 데이터셋을 허깅페이스에 업로드\n",
        "datasets.push_to_hub(\"업로드할 허깅페이스 주소 입력\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Preprocessing"
      ],
      "metadata": {
        "id": "Ex6n0rsO8H9K"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaFdLNCpLTZi"
      },
      "source": [
        "> 업로드된 데이터셋을 다운로드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEFp456gM0La"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "id": "xzqA1-lMuP_I",
        "outputId": "fb7d3082-3c45-49c7-ad3f-f210e06154d6"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "HFValidationError",
          "evalue": "Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: '다운받을 데이터셋의 주소 입력'.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-3b63d187f252>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 데이터셋을 업로드할 때 접근을 제한하거나 비공개로 설정한 경우 허깅페이스 로그인이 필요하다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlow_call_voices\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"다운받을 데이터셋의 주소 입력\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[1;32m   2585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2586\u001b[0m     \u001b[0;31m# Create a dataset builder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2587\u001b[0;31m     builder_instance = load_dataset_builder(\n\u001b[0m\u001b[1;32m   2588\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2589\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, use_auth_token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001b[0m\n\u001b[1;32m   2257\u001b[0m         \u001b[0mdownload_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdownload_config\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mDownloadConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2258\u001b[0m         \u001b[0mdownload_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2259\u001b[0;31m     dataset_module = dataset_module_factory(\n\u001b[0m\u001b[1;32m   2260\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2261\u001b[0m         \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[1;32m   1908\u001b[0m                         \u001b[0;34mf\"Couldn't find '{path}' on the Hugging Face Hub either: {type(e1).__name__}: {e1}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1909\u001b[0m                     ) from None\n\u001b[0;32m-> 1910\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1911\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1912\u001b[0m         raise FileNotFoundError(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[1;32m   1853\u001b[0m                     )\n\u001b[1;32m   1854\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1855\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1856\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msibling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrfilename\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msibling\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msiblings\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# contains a dataset script\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1857\u001b[0m                 \u001b[0mfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHfFileSystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHF_ENDPOINT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[1;32m   1826\u001b[0m             \u001b[0mhf_api\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHfApi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHF_ENDPOINT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1827\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1828\u001b[0;31m                 dataset_info = hf_api.dataset_info(\n\u001b[0m\u001b[1;32m   1829\u001b[0m                     \u001b[0mrepo_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1830\u001b[0m                     \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m         ):\n\u001b[1;32m    110\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"repo_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"from_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"to_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m                 \u001b[0mvalidate_repo_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"token\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marg_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mREPO_ID_REGEX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         raise HFValidationError(\n\u001b[0m\u001b[1;32m    166\u001b[0m             \u001b[0;34m\"Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0;34m\" forbidden, '-' and '.' cannot start or end the name, max length is 96:\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHFValidationError\u001b[0m: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: '다운받을 데이터셋의 주소 입력'."
          ]
        }
      ],
      "source": [
        "# 데이터셋을 업로드할 때 접근을 제한하거나 비공개로 설정한 경우 허깅페이스 로그인이 필요하다.\n",
        "low_call_voices  = load_dataset(\"다운받을 데이터셋의 주소 입력\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lE5ggVuWx0qp"
      },
      "source": [
        "데이터셋에 대하여 다음의 작업을 수행할 함수를 선언한다.\n",
        "1. 오디오 데이터를 로드하고 리샘플링을 실시\n",
        "2. feature extractor를 통해 1차원 오디오 배열을 log-Mel spectrogram으로 변환\n",
        "3. tokenizer를 이용해 전사 데이터를 label ids로 변환"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8sx6sS-vRPY"
      },
      "outputs": [],
      "source": [
        "def prepare_dataset(batch):\n",
        "    # 오디오 파일을 16kHz로 로드\n",
        "    audio = batch[\"audio\"]\n",
        "\n",
        "    # input audio array로부터 log-Mel spectrogram 변환\n",
        "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
        "\n",
        "    # target text를 label ids로 변환\n",
        "    batch[\"labels\"] = tokenizer(batch[\"transcripts\"]).input_ids\n",
        "    return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQYcqdDqvRT4"
      },
      "outputs": [],
      "source": [
        "# 데이터 전처리 함수를 데이터셋 전체에 적용\n",
        "low_call_voices = datasets.map(prepare_dataset, remove_columns=datasets.column_names[\"train\"], num_proc=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72zMpmv2Moge"
      },
      "outputs": [],
      "source": [
        "# 전처리 작업이 오래 걸릴 수 있으므로, colab을 사용하여 파인튜닝을 진행한다면 전처리가 완료된 데이터셋을 Hub에 저장하는 것을 추천한다.\n",
        "low_call_voices.push_to_hub(\"업로드할 허깅페이스 주소 입력\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 이 부분부터 런타임 유형을 GPU로 설정하는 것을 추천"
      ],
      "metadata": {
        "id": "pDxREtX-mFnx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download preprocessed dataset"
      ],
      "metadata": {
        "id": "kxfcR80n8SwM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fWMSgKLSJhGi"
      },
      "outputs": [],
      "source": [
        "# Hub로부터 전처리가 완료된 데이터셋을 로드\n",
        "from datasets import load_dataset\n",
        "low_call_voices_prepreocessed = load_dataset(\"다운받을 데이터셋의 주소 입력\")\n",
        "low_call_voices_prepreocessed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5FwPUUaO350"
      },
      "source": [
        "### Prepare Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pP0Te5S4O39c"
      },
      "source": [
        "이제 데이터가 준비되었으므로 training pipeline을 수행할 준비가 되었다. Hugging face의 'Trainer'를 사용하면 다음의 단계에 따라 간단히 수행할 수 있다.\n",
        "\n",
        "1. data collator 선언 : data collator는 우리가 전처리한 데이터를 받아 모델에 입력할 수 있는 PyTorch Tensor 형태로 변환해준다.\n",
        "2. 평가 지표 : 모델을 평가하기 위한 지표를 설정한다. 영어의 경우 Word Error Rate(WER)를 주로 사용하지만, 교착어인 한국어는 CER(Charactor Error Rate)를 사용하는 편이 더 효과적이다.\n",
        "3. pre-trained model checkpoint 로드 : pre-trained model checkpoint를 로드하고 학습을 위한 설정을 진행한다.\n",
        "4. Training argument의 정의 : Trainer가 활용할 수 있도록 argument를 설정한다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JkuNu5dO4A8"
      },
      "source": [
        "#### Define a Data Collator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paa5zHbWO4EU"
      },
      "source": [
        "Sequence-to-sequence 발화 모델을 위한 Data collator는 input_feature와 label을 독립적으로 다룬다는 점에서 독특한 성격을 보인다. input_feature는 feature extractor로, label은 tokenizer로 다루어야 한다.\n",
        "\n",
        "input_feature는 30초 길이로 패딩되고 고정된 차원의 log_mel spectrogram으로 변환되었으므로, 우리가 할 일은 이를 PyTorch tensor로 변환하는 것뿐이다. 이를 위해 .pad 메서드의 return_tensor=pt 인자를 사용한다. 이때 이미 패딩이 완료되었으므로 여기서 추가적인 패딩 작업이 이루어지지는 않으며, 그저 input_feature를 PyTorch tensor로 변환하기만 할 것이다.\n",
        "\n",
        "반면, label은 아직 패딩 작업이 이루어지지 않았다. 따라서 먼저 tokenizer의 .pad 메서드를 이용해 패딩 작업을 실시할 것이다. 패딩 토큰들은 -100으로 치환되며, 따라서 이 토큰들은 모델이 loss를 계산할 때는 이용되지 않을 것이다. 그리고 이후 training 작업 동안 우리는 label sequence의 시작 부분에 있는 transcript 토큰을 잘라낼 것이다.\n",
        "\n",
        "앞서 선언했던 WhisperPrecessor를 이용해 feature extractor와 tokenizer 모두를 사용할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "htpGKDwKO4IH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Union"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ET9AfiDXSby3"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class DataCollatorSpeechSeq2SeqWithPadding:\n",
        "    processor: Any\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        # 인풋 데이터와 라벨 데이터의 길이가 다르며, 따라서 서로 다른 패딩 방법이 적용되어야 한다. 그러므로 두 데이터를 분리해야 한다.\n",
        "        # 먼저 오디오 인풋 데이터를 간단히 토치 텐서로 반환하는 작업을 수행한다.\n",
        "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
        "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
        "\n",
        "        # Tokenize된 레이블 시퀀스를 가져온다.\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "        # 레이블 시퀀스에 대해 최대 길이만큼 패딩 작업을 실시한다.\n",
        "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
        "\n",
        "        # 패딩 토큰을 -100으로 치환하여 loss 계산 과정에서 무시되도록 한다.\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "\n",
        "        # 이전 토크나이즈 과정에서 bos 토큰이 추가되었다면 bos 토큰을 잘라낸다.\n",
        "        # 해당 토큰은 이후 언제든 추가할 수 있다.\n",
        "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
        "            labels = labels[:, 1:]\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 훈련시킬 모델의 processor, tokenizer, feature extractor 로드\n",
        "from transformers import WhisperTokenizer,  WhisperFeatureExtractor\n",
        "from transformers import WhisperProcessor\n",
        "\n",
        "processor = WhisperProcessor.from_pretrained(\"openai/whisper-base\", language=\"Korean\", task=\"transcribe\")\n",
        "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-base\", language=\"Korean\", task=\"transcribe\")\n",
        "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-base\")"
      ],
      "metadata": {
        "id": "6WMQm08gvPn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVL8hh4ASb3A"
      },
      "outputs": [],
      "source": [
        "# 데이터 콜레이터 초기화\n",
        "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bB2QxtWRSb6w"
      },
      "source": [
        "#### Evaluation Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOWdI09I4DDx"
      },
      "source": [
        "검증 데이터셋에 사용할 evaluation metrics를 정의한다. 영어의 경우 WER을 사용하지만, 한국어 데이터이므로 CER을 사용하는 것이 더 적절할 것이다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3mm-1MU4YU5"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "\n",
        "metric = evaluate.load('cer')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkCZBFpJ4dcU"
      },
      "source": [
        "이제 모델의 예측에 대해 CER metric을 반환할 함수를 선언한다.\n",
        "\n",
        "compute_metrics라는 이름의 이 함수는 가장 먼저 label_ids에 있는 -100을 pad_token으로 치환한다(data collator가 비용함수를 계산할 때 pad token을 무시하도록 하기 위해 설정한). 그 뒤 예측치와 label ids를 문자열로 변환하고 예측치와 정답을 비교하여 CER을 계산해낸다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5cOUbjCBr-U"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(pred):\n",
        "    pred_ids = pred.predictions\n",
        "    label_ids = pred.label_ids\n",
        "\n",
        "    # pad_token을 -100으로 치환\n",
        "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
        "\n",
        "    # metrics 계산 시 special token들을 빼고 계산하도록 설정\n",
        "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
        "\n",
        "    cer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
        "\n",
        "    return {\"cer\": cer}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHke2aFwDjW4"
      },
      "source": [
        "### Load a Pre-Trained Checkpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZK9NMvaDxJ3"
      },
      "source": [
        "pre-trained Whisper base 모델의 checkpoint를 로드한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZCB8FICoDkX4"
      },
      "outputs": [],
      "source": [
        "from transformers import WhisperForConditionalGeneration\n",
        "\n",
        "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-base\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExZZQm-KD2xH"
      },
      "source": [
        "Whisper 모델은 문장의 자동 생성이 시작되기 전 모델의 출력으로 강제되는 토큰(forced_decoder_ids)이 있다. 이 token ids는 전사 언어와 zero-shot ASR 작업에 영향을 미친다. 파인 튜닝을 위해 우리는 이 ids를 None으로 바꿔주어야 한다. 우리는 모델이 정확한 언어(한국어)로 예측하고 전사하도록 훈련할 것이기 때문이다.\n",
        "\n",
        "또한 문장의 생성 중 완전하게 억제되는 토큰들도 있다(suppress_tokens). 이 토큰들은 로그 확률을 -inf로 설정하며, 따라서 샘플링되지 않는다. 우리는 이 토큰들을 비어 있는 리스트로 치환할 것이다. 즉, 어떤 토큰도 억제되지 않는다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prZpvNaoHxQy"
      },
      "outputs": [],
      "source": [
        "model.config.forced_decoder_ids = None\n",
        "model.config.suppress_tokens = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksdUBfGzHxvp"
      },
      "source": [
        "### Define the Training Arguments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PO9WXLB3H0RT"
      },
      "source": [
        "마지막 단계로서, 트레이닝을 위한 모든 파라미터들을 정의해야 한다. 각각의 파라미터들은 다음과 같은 의미를 갖는다.\n",
        "\n",
        "- output_dir : 모델의 가중치를 저장하기 위한 경로를 설정한다. 이 경로는 허깅 페이스 허브의 리포지토리 이름으로도 설정 가능하다.\n",
        "- generation_max_length : 평가 작업 동안 자기회귀적으로 생성되는 토큰들의 최대 길이를 설정한다.\n",
        "- save_steps : 훈련 동안, 이 파라미터에 설정한 step마다 중간 체크포인트가 비동기적으로 저장 및 업로드될 것이다.\n",
        "- eval_steps : 훈련 동안, 이 파라미터에 설정한 step마다 체크포인트에 대한 평가가 이루어질 것이다.\n",
        "- report_to : 훈련 로그를 어디에 저장할지를 설정한다. 'azure_ml', 'comet_ml', 'mlflow', 'neptune', 'tensorboard' 그리고 'wandb'를 지원하며, 기본값은 'tensorboard'이다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qeFBglE4JUyP"
      },
      "outputs": [],
      "source": [
        "from transformers import Seq2SeqTrainingArguments\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"repo_name\",  # 원하는 리포지토리 이름을 임력한다.\n",
        "    per_device_train_batch_size=16,\n",
        "    gradient_accumulation_steps=1,  # 배치 크기가 2배 감소할 때마다 2배씩 증가\n",
        "    learning_rate=1e-5,\n",
        "    warmup_steps=500,\n",
        "    max_steps=4000,  # epoch 대신 설정\n",
        "    gradient_checkpointing=True,\n",
        "    fp16=True,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    per_device_eval_batch_size=8,\n",
        "    predict_with_generate=True,\n",
        "    generation_max_length=225,\n",
        "    save_steps=1000,\n",
        "    eval_steps=1000,\n",
        "    logging_steps=25,\n",
        "    report_to=[\"tensorboard\"],\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"cer\",  # 한국어의 경우 'wer'보다는 'cer'이 더 적합할 것\n",
        "    greater_is_better=False,\n",
        "    push_to_hub=True,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHxTOlpdJ6UG"
      },
      "source": [
        "트레이닝 파라미터들의 설정이 끝났다면 트레이너를 설정한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jikyDBQyMAvw"
      },
      "outputs": [],
      "source": [
        "from transformers import Seq2SeqTrainer\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    args=training_args,\n",
        "    model=model,\n",
        "    train_dataset=low_call_voices_prepreocessed[\"train\"],\n",
        "    eval_dataset=low_call_voices_prepreocessed[\"valid\"],  # or \"test\"\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=processor.feature_extractor,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iw-Cg2DlTngN"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxUVJ24kTpQ6"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uk_zS3GlMYM"
      },
      "source": [
        "#### Model uploading to HuggingFace hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAqfkEqqnd0o"
      },
      "outputs": [],
      "source": [
        "kwargs = {\n",
        "    \"dataset_tags\": \"사용한 데이터셋의 주소 입력\",\n",
        "    \"dataset\": \"사용한 데이터셋의 이름 입력\",  # a 'pretty' name for the training dataset\n",
        "    \"dataset_args\": \"config: ko, split: valid\",\n",
        "    \"language\": \"ko\",\n",
        "    \"model_name\": \"모델 이름 입력\",  # a 'pretty' name for your model\n",
        "    \"finetuned_from\": \"openai/whisper-base\",\n",
        "    \"tasks\": \"automatic-speech-recognition\",\n",
        "    \"tags\": \"hf-asr-leaderboard\",\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plO3hxasvgN0"
      },
      "outputs": [],
      "source": [
        "trainer.push_to_hub(**kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "processor.push_to_hub(\"repo_name\")\n",
        "tokenizer.push_to_hub(\"repo_name\")"
      ],
      "metadata": {
        "id": "pXeaATSouf7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation"
      ],
      "metadata": {
        "id": "zXLPziQSCrkL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model Selection"
      ],
      "metadata": {
        "id": "x2lMqjoGCu6q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 파인 튜닝한 모델을 로드\n",
        "from transformers import WhisperForConditionalGeneration, WhisperProcessor, WhisperFeatureExtractor, WhisperTokenizer\n",
        "\n",
        "model = WhisperForConditionalGeneration.from_pretrained(\"허브에 업로드한 모델 주소 입력\")\n",
        "\n",
        "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"허브에 업로드한 모델 주소 입력\")\n",
        "tokenizer = WhisperTokenizer.from_pretrained(\"허브에 업로드한 모델 주소 입력\", language=\"Korean\", task=\"transcribe\")\n",
        "processor = WhisperProcessor.from_pretrained(\"허브에 업로드한 모델 주소 입력\")"
      ],
      "metadata": {
        "id": "TtFj5whGCrrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training argument 설정"
      ],
      "metadata": {
        "id": "iZCIf8fJCyPq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainingArguments\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"repo_name\",  # 원하는 리포지토리 이름을 임력한다.\n",
        "    per_device_train_batch_size=16,\n",
        "    gradient_accumulation_steps=1,  # 배치 크기가 2배 감소할 때마다 2배씩 증가\n",
        "    learning_rate=1e-5,\n",
        "    warmup_steps=500,\n",
        "    max_steps=4000,\n",
        "    gradient_checkpointing=True,\n",
        "    fp16=True,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    per_device_eval_batch_size=8,\n",
        "    predict_with_generate=True,\n",
        "    generation_max_length=225,\n",
        "    save_steps=1000,\n",
        "    eval_steps=1000,\n",
        "    logging_steps=25,\n",
        "    report_to=[\"tensorboard\"],\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"cer\",  # 한국어의 경우 'wer'보다는 'cer'이 더 적합할 것\n",
        "    greater_is_better=False,\n",
        "    push_to_hub=False,\n",
        ")\n"
      ],
      "metadata": {
        "id": "jLSCkblzDArz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainer\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    args=training_args,\n",
        "    model=model,\n",
        "    train_dataset=low_call_voices_prepreocessed[\"train\"],\n",
        "    eval_dataset=low_call_voices_prepreocessed[\"test\"],  # for evaluation(not validation)\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=processor.feature_extractor,\n",
        ")\n"
      ],
      "metadata": {
        "id": "F_YHiWArDAut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Evaluation 진행"
      ],
      "metadata": {
        "id": "VY5Myn9vDLwn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "lUp3xrdNDAxq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}